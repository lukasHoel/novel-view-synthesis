{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Point_Manipulation.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "L9ZkStRMnqbX"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L9ZkStRMnqbX",
        "colab_type": "text"
      },
      "source": [
        "#1. Install PyTorch3D Libraries\n",
        "PyTorch3D is required for Pointcloud computations.\n",
        "\n",
        " Do NOT install via conda.\n",
        "\n",
        " Not Necessary for running only the Point_Manipulation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T3Uj33D1VGA3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install 'git+https://github.com/facebookresearch/pytorch3d.git'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7KJHZCoXoFBN",
        "colab_type": "text"
      },
      "source": [
        "#2. Further Setup and Imports\n",
        "Get Project Repository and ICL-NUIM dataset from google drive."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o-fmOLpZltMu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156
        },
        "outputId": "e7ee0247-6974-4182-b253-99cefa23aab2"
      },
      "source": [
        "is_on_colab = True\n",
        "if is_on_colab:\n",
        "    # Google Colab setup\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "\n",
        "    from getpass import getpass\n",
        "    import urllib\n",
        "    import os\n",
        "    user = input('Github user name: ')\n",
        "    password = getpass('Github password: ')\n",
        "    password = urllib.parse.quote(password) # your password is converted into url format\n",
        "    cmd_string = 'git clone https://{0}:{1}@github.com/lukasHoel/novel-view-synthesis.git'.format(user, password)\n",
        "    os.system(cmd_string)\n",
        "    os.chdir(\"novel-view-synthesis\")"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n",
            "Github user name: Tristram-TUM\n",
            "Github password: ··········\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bNpisVFWl7DV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os \n",
        "os.chdir(\"novel-view-synthesis\")\n",
        "\n",
        "from models.dummy_model import DummyModel\n",
        "from util.solver import Solver\n",
        "from data.nuim_dataloader import ICLNUIMDataset\n",
        "from util.camera_transformations import *\n",
        "from torch.utils import data\n",
        "from torch.utils.data.sampler import SubsetRandomSampler\n",
        "import torchvision.transforms\n",
        "import torch\n",
        "import numpy as np"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aqfNCgOjxLuq",
        "colab_type": "text"
      },
      "source": [
        "##3. Load Dataset from drive\n",
        "First transforms to PIL Image to allow for resizing to 256x256. At the moment all depth values over 10 are clipped to 0.01. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "obZj4oU0mBGP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 394
        },
        "outputId": "c1713ba1-3d1e-4083-8b86-a6750fa5dd0f"
      },
      "source": [
        "# Load Dataset from drive or local\n",
        "is_on_colab = True\n",
        "if is_on_colab:\n",
        "    path = \"/content/drive/My Drive/NVS_Small/ICL_NUIM\"\n",
        "else:\n",
        "    path = \"/home/lukas/ICL-NUIM/prerendered_data/living_room_traj0_loop\"\n",
        "\n",
        "class Clip_Depth(object):\n",
        "    '''Normalize depth'''\n",
        "\n",
        "    def __call__(self, sample):\n",
        "        sample[sample>10] = 0.01\n",
        "        return sample\n",
        "\n",
        "#transform data to PILImage to allow for resizing (shouldnt change depth data according to docu)\n",
        "transform = torchvision.transforms.Compose([\n",
        "    torchvision.transforms.ToPILImage(),\n",
        "    torchvision.transforms.Resize((256,256)),\n",
        "    torchvision.transforms.ToTensor(), \n",
        "    Clip_Depth()   \n",
        "])   \n",
        "data_dict = {\n",
        "    \"path\": path,\n",
        "}\n",
        "    \n",
        "dataset = ICLNUIMDataset(path, transform=transform, cam_transforms=True)\n",
        "\n",
        "print(\"Loaded following data: {} (samples: {})\".format(data_dict[\"path\"], len(dataset)))\n",
        "\n",
        "item = dataset.__getitem__(0)\n",
        "print(item['cam'])"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loaded following data: /content/drive/My Drive/NVS_Small/ICL_NUIM (samples: 108)\n",
            "{'RT1': tensor([[-0.9998,  0.0000, -0.0218,  0.7909],\n",
            "        [ 0.0000,  1.0000,  0.0000,  1.3000],\n",
            "        [ 0.0218,  0.0000, -0.9998,  1.4623],\n",
            "        [ 0.0000,  0.0000,  0.0000,  1.0000]]), 'K': tensor([[ 481.2000,    0.0000,  319.5000,    0.0000],\n",
            "        [   0.0000, -480.0000,  239.5000,    0.0000],\n",
            "        [   0.0000,    0.0000,    1.0000,    0.0000],\n",
            "        [   0.0000,    0.0000,    0.0000,    1.0000]]), 'RT2': tensor([[-9.9984e-01, -6.3104e-04, -1.7768e-02,  7.9998e-01],\n",
            "        [-5.9969e-04,  1.0000e+00, -1.7698e-03,  1.2992e+00],\n",
            "        [ 1.7769e-02, -1.7589e-03, -9.9984e-01,  1.4622e+00],\n",
            "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  1.0000e+00]]), 'Kinv': tensor([[ 0.0021,  0.0000, -0.6640,  0.0000],\n",
            "        [ 0.0000, -0.0021,  0.4990,  0.0000],\n",
            "        [ 0.0000,  0.0000,  1.0000,  0.0000],\n",
            "        [ 0.0000,  0.0000,  0.0000,  1.0000]]), 'RT1inv': tensor([[-0.9998,  0.0000,  0.0218,  0.7589],\n",
            "        [ 0.0000,  1.0000,  0.0000, -1.3000],\n",
            "        [-0.0218,  0.0000, -0.9998,  1.4792],\n",
            "        [ 0.0000,  0.0000,  0.0000,  1.0000]]), 'RT2inv': tensor([[-9.9984e-01, -5.9969e-04,  1.7769e-02,  7.7465e-01],\n",
            "        [-6.3104e-04,  1.0000e+00, -1.7589e-03, -1.2961e+00],\n",
            "        [-1.7768e-02, -1.7698e-03, -9.9984e-01,  1.4785e+00],\n",
            "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  1.0000e+00]])}\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/pytorch/torch/csrc/utils/tensor_numpy.cpp:141: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M1_sTa5dxr2c",
        "colab_type": "text"
      },
      "source": [
        "##4. Create Batches\n",
        "Currently there is a bug where any Batch_size larger than 1 throws an index error."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j52afHoZri8f",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "4596a722-ed43-418d-8483-4cbf98b82d24"
      },
      "source": [
        "# Create Train and Val dataset with 80% train and 20% val.\n",
        "# from: https://stackoverflow.com/questions/50544730/how-do-i-split-a-custom-dataset-into-training-and-test-datasets\n",
        "\n",
        "dataset_args = {\n",
        "    \"batch_size\": 1,\n",
        "    \"validation_percentage\": 0.2,\n",
        "    \"shuffle_dataset\": True,\n",
        "    \"depth_to_image_plane\": False,\n",
        "    **data_dict\n",
        "}\n",
        "\n",
        "num_workers = 4\n",
        "random_seed= 42\n",
        "\n",
        "# Creating data indices for training and validation splits:\n",
        "dataset_size = len(dataset)\n",
        "indices = list(range(dataset_size))\n",
        "split = int(np.floor(dataset_args[\"validation_percentage\"] * dataset_size))\n",
        "if dataset_args[\"shuffle_dataset\"]:\n",
        "    np.random.seed(random_seed)\n",
        "    np.random.shuffle(indices)\n",
        "train_indices, val_indices = indices[split:], indices[:split]\n",
        "\n",
        "# Creating PT data samplers and loaders:\n",
        "train_sampler = SubsetRandomSampler(train_indices)\n",
        "valid_sampler = SubsetRandomSampler(val_indices)\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(dataset, batch_size=dataset_args[\"batch_size\"], \n",
        "                                           sampler=train_sampler, num_workers=num_workers)\n",
        "validation_loader = torch.utils.data.DataLoader(dataset, batch_size=dataset_args[\"batch_size\"],\n",
        "                                                sampler=valid_sampler, num_workers=num_workers)\n",
        "\n",
        "dataset_args[\"train_len\"] = len(train_loader)\n",
        "dataset_args[\"val_len\"] = len(validation_loader)\n",
        "\n",
        "print(\"Dataset parameters: {}\".format(dataset_args))\n",
        "\n",
        "for i, sample in enumerate(train_loader):\n",
        "    print(sample['cam']['RT2'].shape)\n",
        "    break"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Dataset parameters: {'batch_size': 1, 'validation_percentage': 0.2, 'shuffle_dataset': True, 'depth_to_image_plane': False, 'path': '/content/drive/My Drive/NVS_Small/ICL_NUIM', 'train_len': 87, 'val_len': 21}\n",
            "torch.Size([1, 4, 4])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vGMz32ifxIbP",
        "colab_type": "text"
      },
      "source": [
        "##5. Point Manipulator\n",
        "This class gets as input the depth values and Camera Parameters and calculates the point cloud from them. Currently splatting is commented out. \n",
        "\n",
        "\n",
        "Project_pts function: \n",
        "\n",
        "\n",
        ">My current understanding is that they first create a homogeneous coordinate system and multiply with Z to get the projected coordinates. Then they apply the necessary transformations to reproject the points. \n",
        "\n",
        ">Some signs of Y or Z might not be correct right now, need further tests. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GOs6YMXLUbjr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "#from pytorch3d.structures import Pointclouds\n",
        "\n",
        "EPS = 1e-2\n",
        "\n",
        "class PtsManipulator(nn.Module):\n",
        "    def __init__(self, W, C=3):\n",
        "        super().__init__()\n",
        "\n",
        "        #self.splatter = get_splatter(\n",
        "        #   opt.splatter, None, opt, size=W, C=C, points_per_pixel=opt.pp_pixel\n",
        "        #)\n",
        "\n",
        "        # create coordinate system for x and y\n",
        "        xs = torch.linspace(0, W - 1, W) / float(W - 1) * 2 - 1\n",
        "        ys = torch.linspace(0, W - 1, W) / float(W - 1) * 2 - 1\n",
        "        \n",
        "        xs = xs.view(1, 1, 1, W).repeat(1, 1, W, 1)\n",
        "        ys = ys.view(1, 1, W, 1).repeat(1, 1, 1, W)\n",
        "\n",
        "        # build homogeneous coordinate system with [X, Y, 1, 1] to prepare for depth\n",
        "        xyzs = torch.cat(\n",
        "            (xs, -ys, -torch.ones(xs.size()), torch.ones(xs.size())), 1\n",
        "        ).view(1, 4, -1)\n",
        "\n",
        "        self.register_buffer(\"xyzs\", xyzs)\n",
        "\n",
        "    def project_pts(\n",
        "        self, pts3D, K, K_inv, RT_cam1, RTinv_cam1, RT_cam2, RTinv_cam2\n",
        "    ):\n",
        "        # add Zs to the coordinate system        \n",
        "        # projected_coors is then [X*Z, -Y*Z, -Z, 1] with Z being the depth of the image (should be inverted?)\n",
        "        projected_coors = self.xyzs * pts3D\n",
        "        projected_coors[:, -1, :] = 1\n",
        "\n",
        "        # Transform into camera coordinate of the first view\n",
        "        cam1_X = K_inv.bmm(projected_coors)\n",
        "\n",
        "        # Transform to World Coordinates and apply transformation to second view\n",
        "        RT = RT_cam2.bmm(RTinv_cam1)\n",
        "        wrld_X = RT.bmm(cam1_X)\n",
        "\n",
        "        # Apply intrinsics\n",
        "        xy_proj = K.bmm(wrld_X)\n",
        "\n",
        "        # remove invalid zs that cause nans\n",
        "        mask = (xy_proj[:, 2:3, :].abs() < EPS).detach()\n",
        "        zs = xy_proj[:, 2:3, :]\n",
        "        zs[mask] = EPS\n",
        "\n",
        "        sampler = torch.cat((xy_proj[:, 0:2, :] / -zs, xy_proj[:, 2:3, :]), 1)\n",
        "        sampler[mask.repeat(1, 3, 1)] = -10\n",
        "        # Flip the ys\n",
        "        sampler = sampler * torch.Tensor([1, -1, -1]).unsqueeze(0).unsqueeze(2).to(sampler.device)\n",
        "\n",
        "        return sampler\n",
        "\n",
        "    def forward_justpts(\n",
        "        self, src, pred_pts, K, K_inv, RT_cam1, RTinv_cam1, RT_cam2, RTinv_cam2\n",
        "    ):\n",
        "        # Now project these points into a new view\n",
        "        bs, c, w, h = src.size()\n",
        "\n",
        "        if len(pred_pts.size()) > 3:\n",
        "            # reshape into the right positioning\n",
        "            pred_pts = pred_pts.view(bs, 1, -1)\n",
        "            src = src.view(bs, c, -1)\n",
        "\n",
        "        pts3D = self.project_pts(\n",
        "            pred_pts, K, K_inv, RT_cam1, RTinv_cam1, RT_cam2, RTinv_cam2\n",
        "        )\n",
        "        pointcloud = pts3D.permute(0, 2, 1).contiguous()\n",
        "        #result = self.splatter(pointcloud, src)\n",
        "\n",
        "        return pointcloud\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        alphas,\n",
        "        src,\n",
        "        pred_pts,\n",
        "        K,\n",
        "        K_inv,\n",
        "        RT_cam1,\n",
        "        RTinv_cam1,\n",
        "        RT_cam2,\n",
        "        RTinv_cam2,\n",
        "    ):\n",
        "        # Now project these points into a new view\n",
        "        bs, c, w, h = src.size()\n",
        "\n",
        "        if len(pred_pts.size()) > 3:\n",
        "            # reshape into the right positioning\n",
        "            pred_pts = pred_pts.view(bs, 1, -1)\n",
        "            src = src.view(bs, c, -1)\n",
        "            alphas = alphas.view(bs, 1, -1).permute(0, 2, 1).contiguous()\n",
        "\n",
        "        pts3D = self.project_pts(\n",
        "            pred_pts, K, K_inv, RT_cam1, RTinv_cam1, RT_cam2, RTinv_cam2\n",
        "        )\n",
        "        result = self.splatter(pts3D.permute(0, 2, 1).contiguous(), alphas, src)\n",
        "\n",
        "        return result"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jdId0hjKy2FG",
        "colab_type": "text"
      },
      "source": [
        "##6. Test the Point Manipulator Class\n",
        "Unsqueeze because we dont work with batches but with a single item from the dataset. \n",
        "\n",
        "\n",
        "The resulting Pointcloud has [X, Y, Z] coordinates for every Pixel."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R9sPr1vuABHp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "outputId": "ba33a6f1-33ee-4a80-eff0-d5eab0ac127e"
      },
      "source": [
        "K = item['cam']['K'].unsqueeze(0)\n",
        "Kinv = item['cam']['Kinv'].unsqueeze(0)\n",
        "input_RT = item['cam']['RT1'].unsqueeze(0)\n",
        "input_RTinv = item['cam']['RT1inv'].unsqueeze(0)\n",
        "output_RT = item['cam']['RT2'].unsqueeze(0)\n",
        "output_RTinv = item['cam']['RT2inv'].unsqueeze(0)\n",
        "\n",
        "mani = PtsManipulator(256)\n",
        "ptc = mani.forward_justpts(item['image'].unsqueeze(0), item['depth'].unsqueeze(0), K, Kinv, input_RT, input_RTinv, output_RT, output_RTinv)\n",
        "print(ptc)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[[ 5.0950, -3.1419,  2.5454],\n",
            "         [ 5.0835, -3.1376,  2.5628],\n",
            "         [ 5.0697, -3.1327,  2.5825],\n",
            "         ...,\n",
            "         [ 7.4415, -1.2160,  2.2649],\n",
            "         [ 7.4508, -1.2163,  2.2640],\n",
            "         [ 7.4597, -1.2165,  2.2632]]])\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}