{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/lukasHoel/novel-view-synthesis/blob/projection_fix/notebooks/Point_Manipulation_lukas_14_05.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "L9ZkStRMnqbX"
   },
   "source": [
    "# SETUP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_on_colab = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "T3Uj33D1VGA3",
    "outputId": "69ca5e4f-a26d-47ef-8e99-2740878103e0"
   },
   "outputs": [],
   "source": [
    "if is_on_colab:\n",
    "    !pip install 'git+https://github.com/facebookresearch/pytorch3d.git'\n",
    "    !pip install open3d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 165
    },
    "colab_type": "code",
    "id": "o-fmOLpZltMu",
    "outputId": "8a44c789-1eab-49cd-aa35-77846e3ee552"
   },
   "outputs": [],
   "source": [
    "if is_on_colab:\n",
    "    # Google Colab setup\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "\n",
    "    from getpass import getpass\n",
    "    import urllib\n",
    "    import os\n",
    "    user = input('Github user name: ')\n",
    "    password = getpass('Github password: ')\n",
    "    password = urllib.parse.quote(password) # your password is converted into url format\n",
    "    cmd_string = 'git clone -b projection_fix https://{0}:{1}@github.com/lukasHoel/novel-view-synthesis.git'.format(user, password)\n",
    "    os.system(cmd_string)\n",
    "    os.chdir(\"novel-view-synthesis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bNpisVFWl7DV"
   },
   "outputs": [],
   "source": [
    "import os \n",
    "if is_on_colab:\n",
    "    os.chdir(\"/content/novel-view-synthesis\")\n",
    "else:\n",
    "    os.chdir(\"/home/lukas/Desktop/git/novel-view-synthesis\")\n",
    "\n",
    "from models.dummy_model import DummyModel\n",
    "from util.solver import Solver\n",
    "from data.nuim_dataloader import ICLNUIMDataset\n",
    "from util.camera_transformations import *\n",
    "from torch.utils import data\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "import torchvision.transforms\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dpp3ef-np3zu"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "#%matplotlib notebook\n",
    "\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "def show_plot(points, color=None):\n",
    "    \n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "    ax.scatter(points[:,0], points[:,1], points[:,2], marker='o', s=0.1, c=color)\n",
    "    #ax.plot_surface(points[:,0], points[:,1], points[:,2])\n",
    "\n",
    "    ax.set_xlabel('X Label')\n",
    "    ax.set_ylabel('Y Label')\n",
    "    ax.set_zlabel('Z Label')\n",
    "\n",
    "    plt.show()\n",
    "    \n",
    "def show_image(img):\n",
    "    plt.imshow(img, aspect=\"auto\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "M1_sTa5dxr2c"
   },
   "source": [
    "# LOAD ICL DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "obZj4oU0mBGP",
    "outputId": "e1b2479f-6c07-447b-84fe-f1ab4249dbd1",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Load Dataset from drive or local\n",
    "if is_on_colab:\n",
    "    path = \"/content/drive/My Drive/Novel_View_Synthesis/ICL-NUIM/living_room_traj2_loop\"\n",
    "else:\n",
    "    path = \"/home/lukas/Desktop/datasets/ICL-NUIM/prerendered_data/living_room_traj2_loop\"\n",
    "    #path = \"/home/lukas/Desktop/git/synsin/dataset\"\n",
    "\n",
    "\n",
    "img_shape=(128, 128)\n",
    "#transform data to PILImage to allow for resizing (shouldnt change depth data according to docu)\n",
    "transform = torchvision.transforms.Compose([\n",
    "    #torchvision.transforms.ToPILImage(),\n",
    "    #torchvision.transforms.Pad(padding=(0, 80), fill=0),\n",
    "    torchvision.transforms.Resize(img_shape),\n",
    "    torchvision.transforms.ToTensor(), \n",
    "    #Clip_Depth()   \n",
    "])   \n",
    "data_dict = {\n",
    "    \"path\": path,\n",
    "}\n",
    "    \n",
    "np.random.seed(1)\n",
    "    \n",
    "dataset = ICLNUIMDataset(path, transform=transform,\n",
    "                         inverse_depth=False,\n",
    "                         sampleOutput=True,\n",
    "                         out_shape=img_shape)\n",
    "\n",
    "print(\"Loaded following data: {} (samples: {})\".format(data_dict[\"path\"], len(dataset)))\n",
    "\n",
    "item = dataset.__getitem__(400)\n",
    "print(item['cam'])\n",
    "print(item['output']['idx'])\n",
    "\n",
    "print(item[\"image\"].shape)\n",
    "print(item[\"depth\"].shape)\n",
    "print(torch.max(item[\"depth\"]))\n",
    "print(torch.min(item[\"depth\"]))\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "plt.imshow(np.moveaxis(item['image'].squeeze().cpu().detach().numpy(), 0, -1))\n",
    "plt.show()\n",
    "\n",
    "plt.imshow(item['depth'].squeeze().cpu().detach().numpy())\n",
    "plt.show()\n",
    "\n",
    "plt.imshow(np.moveaxis(item['output']['image'].squeeze().cpu().detach().numpy(), 0, -1))\n",
    "plt.show()\n",
    "\n",
    "print(item[\"output\"][\"idx\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 493
    },
    "colab_type": "code",
    "id": "lr0AIwsbpqHu",
    "outputId": "9d69e290-d3b8-48be-919b-1786fc98a2b1"
   },
   "outputs": [],
   "source": [
    "def getEulerAngles(R):\n",
    "    ry = np.arcsin(R[0,2])\n",
    "    rz = np.arccos(R[0,0] / np.cos(ry))\n",
    "    rx = np.arccos(R[2,2] / np.cos(ry))\n",
    "\n",
    "    return rx, ry, rz\n",
    "\n",
    "RT1 = item[\"cam\"][\"RT1\"].cpu().numpy()\n",
    "RT1inv = item[\"cam\"][\"RT1inv\"].cpu().numpy()\n",
    "\n",
    "RT2 = item[\"cam\"][\"RT2\"].cpu().numpy()\n",
    "RT2inv = item[\"cam\"][\"RT2inv\"].cpu().numpy()\n",
    "\n",
    "print(\"RT1: \", getEulerAngles(RT1))\n",
    "print(\"RT1inv: \", getEulerAngles(RT1inv))\n",
    "#print(\"RT1inv np: \", getEulerAngles(np.linalg.inv(RT1)))\n",
    "print()\n",
    "\n",
    "print(\"RT2: \", getEulerAngles(RT2))\n",
    "print(\"RT2inv: \", getEulerAngles(RT2inv))\n",
    "#print(\"RT2inv np: \", getEulerAngles(np.linalg.inv(RT2)))\n",
    "print()\n",
    "\n",
    "RT = item[\"cam\"][\"RT2\"].unsqueeze(0).bmm(item[\"cam\"][\"RT1inv\"].unsqueeze(0))\n",
    "print(RT)\n",
    "print(\"RT: \", getEulerAngles(RT.squeeze().cpu().numpy()))\n",
    "print()\n",
    "\n",
    "RT = item[\"cam\"][\"RT2inv\"].unsqueeze(0).bmm(item[\"cam\"][\"RT1\"].unsqueeze(0))\n",
    "print(RT)\n",
    "print(\"RT: \", getEulerAngles(RT.squeeze().cpu().numpy()))\n",
    "\n",
    "RT = item[\"cam\"][\"RT1\"].unsqueeze(0).bmm(item[\"cam\"][\"RT2inv\"].unsqueeze(0))\n",
    "print(RT)\n",
    "print(\"RT: \", getEulerAngles(RT.squeeze().cpu().numpy()))\n",
    "print()\n",
    "\n",
    "RT = item[\"cam\"][\"RT1inv\"].unsqueeze(0).bmm(item[\"cam\"][\"RT2\"].unsqueeze(0))\n",
    "print(RT)\n",
    "print(\"RT: \", getEulerAngles(RT.squeeze().cpu().numpy()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GENERATE PLY FILES FROM ICL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "GKeuTLIy06TA",
    "outputId": "337ca614-8e65-4e92-93c6-b746ce13a328"
   },
   "outputs": [],
   "source": [
    "print(item['depth'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 55
    },
    "colab_type": "code",
    "id": "EvpuyORz1c-n",
    "outputId": "2f7602ed-afbf-4f46-89fa-9ede4fb4af68"
   },
   "outputs": [],
   "source": [
    "import open3d as o3d\n",
    "\n",
    "if is_on_colab:\n",
    "    output_path = \"/content/\"\n",
    "else:\n",
    "    output_path = \"/home/lukas/Desktop/\"\n",
    "\n",
    "def render_pointclouds(H, W, depth, K, Kinv, RT1inv, RT2, colors, prefix=\"\"):\n",
    "\n",
    "  # decide if we want to normalise depth or not\n",
    "  #z = (depth - depth.min()) / (depth.max()- depth.min())\n",
    "  z = depth\n",
    "  fx = K[0,0]\n",
    "  fy = K[1,1]\n",
    "  cx = K[0,2]\n",
    "  cy = K[1,2]\n",
    "  # create coordinate system for x and y\n",
    "  '''\n",
    "  TODO: Are our coordinates correct? Or do we have to create them like Matthias does it? \n",
    "  (https://github.com/niessner/VoxelHashing/blob/421d548cca570ec14d51148cd6e4e5aca45652ef/DepthSensingCUDA/Source/DepthCameraUtil.h#L117)\n",
    "  I think with the calculation in the toImagePlane method in the dataloader we are doing something similar, but is it exactly the same?\n",
    "\n",
    "  Comment Lukas: Source Code from Matthias is the implementation of Kinv * (xz, yz, z) which we also do.\n",
    "  The toImagePlane method takes care of normalizing depth_from_cam to global 3D z coordinate, e.g. a wall is now straight (has same depth everywhere)\n",
    "  That method is needed because ICL dataset has depth_from_cam as default value and thus, points further away from the cam have greater depth (e.g. wall does not have same depth everywhere)\n",
    "\n",
    "  TODO: maybe try out going backwards from GT image to first view with RT2inv*RT1, to see if transformation works?\n",
    "  Comment Lukas: I implemented this in the next 2 cells\n",
    "  '''\n",
    "  #xs = torch.linspace(0, W - 1, W) / float(W - 1) * 2 - 1\n",
    "  #ys = torch.linspace(0, W - 1, W) / float(W - 1) * 2 - 1\n",
    "  #xs = xs.view(1, 1, 1, W).repeat(1, 1, W, 1)\n",
    "  #ys = ys.view(1, 1, W, 1).repeat(1, 1, 1, W)\n",
    "\n",
    "  '''\n",
    "  TODO: instead of linspace from [-1,1] use the original size (640x480), also without Resize(256,256)!\n",
    "  Maybe this works better?\n",
    "  '''\n",
    "  xs = torch.linspace(0, W - 1, W)\n",
    "  ys = torch.linspace(0, H - 1, H)\n",
    "  #xs /= float(W-1) * 2 - 1\n",
    "  #ys /= float(H-1) * 2 - 1\n",
    "  xs = xs.view(1, 1, 1, W).repeat(1, 1, H, 1)\n",
    "  ys = ys.view(1, 1, H, 1).repeat(1, 1, 1, W)\n",
    "\n",
    "  xyzs = torch.cat((xs, ys, torch.ones(xs.size()), torch.ones(xs.size())), 1).view(1, 4, -1)\n",
    "\n",
    "  print(xyzs)\n",
    "\n",
    "  print(xyzs.shape)\n",
    "  print(z.shape)\n",
    "\n",
    "  pcd = o3d.geometry.PointCloud()\n",
    "  pcd.points = o3d.utility.Vector3dVector(xyzs[:,:3,:].permute(0,2,1).cpu().numpy().astype(np.float64).squeeze())\n",
    "  pcd.colors = o3d.utility.Vector3dVector(colors.view(3, -1).permute((1,0)).cpu().numpy().astype(np.float64))\n",
    "  o3d.io.write_point_cloud(output_path + prefix + \"cam1_image.ply\", pcd)\n",
    "\n",
    "  # add depth\n",
    "  cam_coor = xyzs * z\n",
    "  print(cam_coor.shape)\n",
    "  cam_coor[:, -1, :] = 1 \n",
    "\n",
    "  pcd = o3d.geometry.PointCloud()\n",
    "  pcd.points = o3d.utility.Vector3dVector(cam_coor[:,:3,:].permute(0,2,1).cpu().numpy().astype(np.float64).squeeze())\n",
    "  pcd.colors = o3d.utility.Vector3dVector(colors.view(3, -1).permute((1,0)).cpu().numpy().astype(np.float64))\n",
    "  o3d.io.write_point_cloud(output_path + prefix + \"cam1_projection.ply\", pcd)\n",
    "\n",
    "  '''\n",
    "  leave out Kinv transformation here because that leads to strange behaviour.\n",
    "\n",
    "  Comment Lukas: When using the original linspace 640x480 then we can do this\n",
    "\n",
    "  See here: https://svncvpr.in.tum.de/cvpr-ros-pkg/trunk/rgbd_benchmark/rgbd_benchmark_tools/src/rgbd_benchmark_tools/generate_pointcloud.py\n",
    "  '''\n",
    "\n",
    "  '''\n",
    "  f = 10.0\n",
    "  n = 0.01\n",
    "  perspective = torch.zeros(4,4)\n",
    "  perspective[0,0] = 2 * K[0,0] / W\n",
    "  perspective[0,2] = -(2 * (K[0,2] / W) - 1)\n",
    "  perspective[1,1] = 2 * K[1,1] / H\n",
    "  perspective[1,2] = -(2 * (K[1,2] / H) - 1)\n",
    "  perspective[2,2] = -(f+n)/(f-n)\n",
    "  perspective[2,3] = -2*f*n/(f-n)\n",
    "  perspective[3,2] = -1\n",
    "  p_inv = perspective.inverse()\n",
    "  \n",
    "  print(perspective)\n",
    "  print(p_inv)\n",
    "  '''\n",
    "\n",
    "  print(cam_coor.shape)\n",
    "  cam_coor[:, :3, :] = Kinv.matmul(cam_coor[:, :3, :])\n",
    "  #cam_coor = p_inv.matmul(cam_coor)\n",
    "  show_plot(cam_coor[:,:3], color=colors.view(3, -1).permute((1,0)).numpy())\n",
    "\n",
    "  pcd = o3d.geometry.PointCloud()\n",
    "  pcd.points = o3d.utility.Vector3dVector(cam_coor[:,:3,:].permute(0,2,1).cpu().numpy().astype(np.float64).squeeze())\n",
    "  pcd.colors = o3d.utility.Vector3dVector(colors.view(3, -1).permute((1,0)).cpu().numpy().astype(np.float64))\n",
    "  o3d.io.write_point_cloud(output_path + prefix + \"cam1.ply\", pcd)\n",
    "\n",
    "  # tranform to world coordinates \n",
    "  world_coor = RT1inv.matmul(cam_coor)\n",
    "\n",
    "  pcd = o3d.geometry.PointCloud()\n",
    "  pcd.points = o3d.utility.Vector3dVector(world_coor[:,:3,:].permute(0,2,1).cpu().numpy().astype(np.float64).squeeze())\n",
    "  pcd.colors = o3d.utility.Vector3dVector(colors.view(3, -1).permute((1,0)).cpu().numpy().astype(np.float64))\n",
    "  o3d.io.write_point_cloud(output_path + prefix + \"world.ply\", pcd)\n",
    "\n",
    "  # transform to Camera Coordinates of Viewpoint 2\n",
    "  new_coor = RT2.matmul(world_coor)\n",
    "  new_coor = new_coor[:,:3]\n",
    "\n",
    "  pcd = o3d.geometry.PointCloud()\n",
    "  pcd.points = o3d.utility.Vector3dVector(new_coor.permute(0,2,1).cpu().numpy().astype(np.float64).squeeze())\n",
    "  pcd.colors = o3d.utility.Vector3dVector(colors.view(3, -1).permute((1,0)).cpu().numpy().astype(np.float64))\n",
    "  o3d.io.write_point_cloud(output_path + prefix + \"cam2.ply\", pcd)\n",
    "\n",
    "  #go back to image coordinates\n",
    "  new_img = K.matmul(new_coor)\n",
    "\n",
    "  pcd = o3d.geometry.PointCloud()\n",
    "  pcd.points = o3d.utility.Vector3dVector(new_img.permute(0,2,1).cpu().numpy().astype(np.float64).squeeze())\n",
    "  pcd.colors = o3d.utility.Vector3dVector(colors.view(3, -1).permute((1,0)).cpu().numpy().astype(np.float64))\n",
    "  o3d.io.write_point_cloud(output_path + prefix + \"cam2_projection.ply\", pcd)\n",
    "\n",
    "  new_img[:, :2] = new_img[:, :2] / new_img[:, 2]\n",
    "  #new_img[:, 2] = 1\n",
    "  show_plot(new_img, color=colors.view(3, -1).permute((1,0)).numpy())\n",
    "\n",
    "  # initialise PointCloud for Open3D and add Points and colors to it in open3d format.\n",
    "  # open3d expects float64 numpy arrays to convert with vector3dvector function\n",
    "  pcd = o3d.geometry.PointCloud()\n",
    "  pcd.points = o3d.utility.Vector3dVector(new_img.permute(0,2,1).cpu().numpy().astype(np.float64).squeeze())\n",
    "  pcd.colors = o3d.utility.Vector3dVector(colors.view(3, -1).permute((1,0)).cpu().numpy().astype(np.float64))\n",
    "  # save pointcloud as .ply file\n",
    "  o3d.io.write_point_cloud(output_path + prefix + \"cam2_image.ply\", pcd)\n",
    "\n",
    "  if is_on_colab:\n",
    "    cmd_string = \"zip /content/\" + prefix + \"pointclouds.zip /content/\" + prefix + \"*.ply\"\n",
    "    os.system(cmd_string)\n",
    "    from google.colab import files\n",
    "    files.download(\"/content/\" + prefix + \"pointclouds.zip\")\n",
    "    !rm /content/*.ply"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 625
    },
    "colab_type": "code",
    "id": "F0mtFa8pPa3P",
    "outputId": "7c1e52e8-91ae-4a0f-cabc-e9e3ca346d6c"
   },
   "outputs": [],
   "source": [
    "# GENERATE POINT CLOUD FROM INPUT IMAGE\n",
    "\n",
    "#W = item['depth'].shape[-1]\n",
    "_, H, W = item['depth'].shape\n",
    "depth = item['depth'].view(1, -1)\n",
    "K = item['cam']['K'][:3, :3]\n",
    "Kinv = item['cam']['Kinv'][:3, :3]\n",
    "RT1inv = item['cam']['RT1inv']\n",
    "RT2 = item['cam']['RT2']\n",
    "\n",
    "# ALTERNATIVE\n",
    "RT1inv = item['cam']['RT1']\n",
    "RT2 = item['cam']['RT2inv']\n",
    "\n",
    "render_pointclouds(H, W, depth, K, Kinv, RT1inv, RT2, colors=item[\"image\"], prefix=\"from_input_\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 625
    },
    "colab_type": "code",
    "id": "4oyRSYTSOiat",
    "outputId": "e3f89ecf-1080-4107-8f04-1e0fbfce2335"
   },
   "outputs": [],
   "source": [
    "# GENERATE POINT CLOUD FROM OUTPUT IMAGE\n",
    "#W = item['depth'].shape[-1]\n",
    "_, H, W = item['output']['depth'].shape\n",
    "depth = item['output']['depth'].view(1, -1)\n",
    "K = item['cam']['K'][:3, :3]\n",
    "Kinv = item['cam']['Kinv'][:3, :3]\n",
    "#RT1inv = item['cam']['RT2inv'] # here the other way round because we go from output to input\n",
    "#RT2 = item['cam']['RT1'] # here the other way round because we go from output to input\n",
    "\n",
    "\n",
    "RT1inv = item['cam']['RT2'] # here the other way round because we go from output to input\n",
    "RT2 = item['cam']['RT1inv'] # here the other way round because we go from output to input\n",
    "\n",
    "render_pointclouds(H, W, depth, K, Kinv, RT1inv, RT2, colors=item[\"output\"][\"image\"], prefix=\"from_output_\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PROJECT ICL WITH PtsManipulator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## INPUT->OUTPUT "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from projection.z_buffer_manipulator import PtsManipulator\n",
    "\n",
    "_, H, W = item['depth'].shape\n",
    "\n",
    "K = item['cam']['K'].unsqueeze(0)\n",
    "Kinv = item['cam']['Kinv'].unsqueeze(0)\n",
    "#K=torch.eye(4).unsqueeze(0)\n",
    "#Kinv=torch.eye(4).unsqueeze(0)\n",
    "input_RT = item['cam']['RT1'].unsqueeze(0)\n",
    "input_RTinv = item['cam']['RT1inv'].unsqueeze(0)\n",
    "output_RT = item['cam']['RT2'].unsqueeze(0)\n",
    "output_RTinv = item['cam']['RT2inv'].unsqueeze(0)\n",
    "\n",
    "\n",
    "print(K)\n",
    "\n",
    "mani = PtsManipulator(mode=PtsManipulator.icl_nuim_mode, W=W, H=H)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(\"Jo\")\n",
    "    device = \"cuda:0\"\n",
    "    mani.to(device)\n",
    "    item['image'] = item['image'].to(device)\n",
    "    item['depth'] = item['depth'].to(device)\n",
    "    K = K.to(device)\n",
    "    Kinv = Kinv.to(device)\n",
    "    input_RT = input_RT.to(device)\n",
    "    input_RTinv = input_RTinv.to(device)\n",
    "    output_RT = output_RT.to(device)\n",
    "    output_RTinv = output_RTinv.to(device)\n",
    "\n",
    "img = mani.forward_justpts(item['image'].unsqueeze(0), item['depth'].unsqueeze(0),\n",
    "                           K, Kinv,\n",
    "                           input_RT, input_RTinv,\n",
    "                           output_RT, output_RTinv)\n",
    "\n",
    "print(\"FINAL\")\n",
    "print(img.shape)\n",
    "show_image(img.squeeze().permute((1,2,0)).cpu().detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(img.shape)\n",
    "print(\"RESULT\")\n",
    "show_image(img.squeeze().permute((1,2,0)).cpu().detach().numpy())\n",
    "print(\"OUTPUT\")\n",
    "show_image(item['output']['image'].squeeze().permute((1,2,0)).cpu().detach().numpy())\n",
    "print(\"INPUT\")\n",
    "show_image(item['image'].squeeze().permute((1,2,0)).cpu().detach().numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OUTPUT->INPUT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from projection.z_buffer_manipulator import PtsManipulator\n",
    "\n",
    "_, H, W = item['output']['depth'].shape\n",
    "\n",
    "K = item['cam']['K'].unsqueeze(0)\n",
    "Kinv = item['cam']['Kinv'].unsqueeze(0)\n",
    "#K=torch.eye(4).unsqueeze(0)\n",
    "#Kinv=torch.eye(4).unsqueeze(0)\n",
    "input_RT = item['cam']['RT2'].unsqueeze(0)\n",
    "input_RTinv = item['cam']['RT2inv'].unsqueeze(0)\n",
    "output_RT = item['cam']['RT1'].unsqueeze(0)\n",
    "output_RTinv = item['cam']['RT1inv'].unsqueeze(0)\n",
    "\n",
    "\n",
    "print(K)\n",
    "\n",
    "mani = PtsManipulator(mode=PtsManipulator.icl_nuim_mode, W=W, H=H)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(\"Jo\")\n",
    "    device = \"cuda:0\"\n",
    "    mani.to(device)\n",
    "    item['output']['image'] = item['output']['image'].to(device)\n",
    "    item['output']['depth'] = item['output']['depth'].to(device)\n",
    "    K = K.to(device)\n",
    "    Kinv = Kinv.to(device)\n",
    "    input_RT = input_RT.to(device)\n",
    "    input_RTinv = input_RTinv.to(device)\n",
    "    output_RT = output_RT.to(device)\n",
    "    output_RTinv = output_RTinv.to(device)\n",
    "\n",
    "img = mani.forward_justpts(item['output']['image'].unsqueeze(0), item['output']['depth'].unsqueeze(0),\n",
    "                           K, Kinv,\n",
    "                           input_RT, input_RTinv,\n",
    "                           output_RT, output_RTinv)\n",
    "\n",
    "print(\"FINAL\")\n",
    "print(img.shape)\n",
    "show_image(img.squeeze().permute((1,2,0)).cpu().detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(img.shape)\n",
    "print(\"RESULT\")\n",
    "show_image(img.squeeze().permute((1,2,0)).cpu().detach().numpy())\n",
    "print(\"OUTPUT\")\n",
    "show_image(item['image'].squeeze().permute((1,2,0)).cpu().detach().numpy())\n",
    "print(\"INPUT\")\n",
    "show_image(item['output']['image'].squeeze().permute((1,2,0)).cpu().detach().numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LOAD MATTERPORT DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data.mp3d_dataloader import MP3D_Habitat_Offline_Dataset\n",
    "\n",
    "# Load Dataset from drive or local\n",
    "if is_on_colab:\n",
    "    path = \"/content/drive/My Drive/Novel_View_Synthesis/Matterport-Demo\"\n",
    "else:\n",
    "    path = \"/home/lukas/datasets/Matterport3D/data/v1/tasks/mp3d_habitat/rendered\"\n",
    "\n",
    "\n",
    "#img_shape=(256, 256)\n",
    "#transform data to PILImage to allow for resizing (shouldnt change depth data according to docu)\n",
    "transform = torchvision.transforms.Compose([\n",
    "    #torchvision.transforms.ToPILImage(),\n",
    "    #torchvision.transforms.Pad(padding=(0, 80), fill=0),\n",
    "    #torchvision.transforms.Resize(img_shape),\n",
    "    torchvision.transforms.ToTensor(), \n",
    "    #Clip_Depth()   \n",
    "])   \n",
    "data_dict = {\n",
    "    \"path\": path,\n",
    "}\n",
    "    \n",
    "    \n",
    "dataset = MP3D_Habitat_Offline_Dataset(path, \n",
    "                                       in_size=256,\n",
    "                                       transform=transform,\n",
    "                                       inverse_depth=False,\n",
    "                                       sampleOutput=True)\n",
    "\n",
    "print(\"Loaded following data: {} (samples: {})\".format(data_dict[\"path\"], len(dataset)))\n",
    "\n",
    "item = dataset.__getitem__(2)\n",
    "print(item['cam'])\n",
    "print(item['output']['idx'])\n",
    "\n",
    "print(item[\"image\"].shape)\n",
    "print(item[\"depth\"].shape)\n",
    "print(torch.max(item[\"depth\"]))\n",
    "print(torch.min(item[\"depth\"]))\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "plt.imshow(np.moveaxis(item['image'].squeeze().cpu().detach().numpy(), 0, -1))\n",
    "plt.show()\n",
    "\n",
    "plt.imshow(item['depth'].squeeze().cpu().detach().numpy())\n",
    "plt.show()\n",
    "\n",
    "plt.imshow(np.moveaxis(item['output']['image'].squeeze().cpu().detach().numpy(), 0, -1))\n",
    "plt.show()\n",
    "\n",
    "print(item[\"output\"][\"idx\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PROJECT MATTERPORT WITH PtsManipulator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## INPUT->OUTPUT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from projection.z_buffer_manipulator import PtsManipulator\n",
    "\n",
    "_, H, W = item['depth'].shape\n",
    "\n",
    "K = item['cam']['K'].unsqueeze(0)\n",
    "Kinv = item['cam']['Kinv'].unsqueeze(0)\n",
    "#K=torch.eye(4).unsqueeze(0)\n",
    "#Kinv=torch.eye(4).unsqueeze(0)\n",
    "input_RT = item['cam']['RT1'].unsqueeze(0)\n",
    "input_RTinv = item['cam']['RT1inv'].unsqueeze(0)\n",
    "output_RT = item['cam']['RT2'].unsqueeze(0)\n",
    "output_RTinv = item['cam']['RT2inv'].unsqueeze(0)\n",
    "\n",
    "\n",
    "print(K)\n",
    "\n",
    "mani = PtsManipulator(mode=PtsManipulator.matterport_mode, W=W)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(\"Jo\")\n",
    "    device = \"cuda:0\"\n",
    "    mani.to(device)\n",
    "    item['image'] = item['image'].to(device)\n",
    "    item['depth'] = item['depth'].to(device)\n",
    "    K = K.to(device)\n",
    "    Kinv = Kinv.to(device)\n",
    "    input_RT = input_RT.to(device)\n",
    "    input_RTinv = input_RTinv.to(device)\n",
    "    output_RT = output_RT.to(device)\n",
    "    output_RTinv = output_RTinv.to(device)\n",
    "\n",
    "img = mani.forward_justpts(item['image'].unsqueeze(0), item['depth'].unsqueeze(0),\n",
    "                           K, Kinv,\n",
    "                           input_RT, input_RTinv,\n",
    "                           output_RT, output_RTinv)\n",
    "\n",
    "print(\"FINAL\")\n",
    "print(img.shape)\n",
    "show_image(img.squeeze().permute((1,2,0)).cpu().detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(img.shape)\n",
    "print(\"RESULT\")\n",
    "show_image(img.squeeze().permute((1,2,0)).cpu().detach().numpy())\n",
    "print(\"OUTPUT\")\n",
    "show_image(item['output']['image'].squeeze().permute((1,2,0)).cpu().detach().numpy())\n",
    "print(\"INPUT\")\n",
    "show_image(item['image'].squeeze().permute((1,2,0)).cpu().detach().numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OUTPUT->INPUT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from projection.z_buffer_manipulator import PtsManipulator\n",
    "\n",
    "_, H, W = item['output']['depth'].shape\n",
    "\n",
    "K = item['cam']['K'].unsqueeze(0)\n",
    "Kinv = item['cam']['Kinv'].unsqueeze(0)\n",
    "#K=torch.eye(4).unsqueeze(0)\n",
    "#Kinv=torch.eye(4).unsqueeze(0)\n",
    "input_RT = item['cam']['RT2'].unsqueeze(0)\n",
    "input_RTinv = item['cam']['RT2inv'].unsqueeze(0)\n",
    "output_RT = item['cam']['RT1'].unsqueeze(0)\n",
    "output_RTinv = item['cam']['RT1inv'].unsqueeze(0)\n",
    "\n",
    "\n",
    "print(K)\n",
    "\n",
    "mani = PtsManipulator(mode=PtsManipulator.matterport_mode, W=W)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(\"Jo\")\n",
    "    device = \"cuda:0\"\n",
    "    mani.to(device)\n",
    "    item['output']['image'] = item['output']['image'].to(device)\n",
    "    item['output']['depth'] = item['output']['depth'].to(device)\n",
    "    K = K.to(device)\n",
    "    Kinv = Kinv.to(device)\n",
    "    input_RT = input_RT.to(device)\n",
    "    input_RTinv = input_RTinv.to(device)\n",
    "    output_RT = output_RT.to(device)\n",
    "    output_RTinv = output_RTinv.to(device)\n",
    "\n",
    "img = mani.forward_justpts(item['output']['image'].unsqueeze(0), item['output']['depth'].unsqueeze(0),\n",
    "                           K, Kinv,\n",
    "                           input_RT, input_RTinv,\n",
    "                           output_RT, output_RTinv)\n",
    "\n",
    "print(\"FINAL\")\n",
    "print(img.shape)\n",
    "show_image(img.squeeze().permute((1,2,0)).cpu().detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(img.shape)\n",
    "print(\"RESULT\")\n",
    "show_image(img.squeeze().permute((1,2,0)).cpu().detach().numpy())\n",
    "print(\"OUTPUT\")\n",
    "show_image(item['image'].squeeze().permute((1,2,0)).cpu().detach().numpy())\n",
    "print(\"INPUT\")\n",
    "show_image(item['output']['image'].squeeze().permute((1,2,0)).cpu().detach().numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LOAD ICL DYNAMICS DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data.nuim_dynamics_dataloader import ICLNUIM_Dynamic_Dataset\n",
    "\n",
    "# Load Dataset from drive or local\n",
    "if is_on_colab:\n",
    "    path = \"/content/drive/My Drive/Novel_View_Synthesis/ICL-NUIM/custom/seq0001\"\n",
    "else:\n",
    "    path = \"/home/lukas/datasets/ICL-NUIM/custom/seq0001\"\n",
    "\n",
    "\n",
    "img_shape=(256, 256)\n",
    "#transform data to PILImage to allow for resizing (shouldnt change depth data according to docu)\n",
    "transform = torchvision.transforms.Compose([\n",
    "    #torchvision.transforms.ToPILImage(),\n",
    "    #torchvision.transforms.Pad(padding=(0, 80), fill=0),\n",
    "    torchvision.transforms.Resize(img_shape),\n",
    "    torchvision.transforms.ToTensor(), \n",
    "    #Clip_Depth()   \n",
    "])   \n",
    "data_dict = {\n",
    "    \"path\": path,\n",
    "}\n",
    "    \n",
    "    \n",
    "dataset = ICLNUIM_Dynamic_Dataset(path, \n",
    "                                  transform=transform,\n",
    "                                  output_from_other_view=False,\n",
    "                                  inverse_depth=False,\n",
    "                                  sampleOutput=True,\n",
    "                                  out_shape=img_shape)\n",
    "\n",
    "print(\"Loaded following data: {} (samples: {})\".format(data_dict[\"path\"], len(dataset)))\n",
    "\n",
    "item = dataset.__getitem__(0)\n",
    "print(item['cam'])\n",
    "print(item['output']['idx'])\n",
    "\n",
    "print(item[\"image\"].shape)\n",
    "print(item[\"depth\"].shape)\n",
    "print(torch.max(item[\"depth\"]))\n",
    "print(torch.min(item[\"depth\"]))\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "plt.imshow(np.moveaxis(item['image'].squeeze().cpu().detach().numpy(), 0, -1))\n",
    "plt.show()\n",
    "\n",
    "plt.imshow(item['depth'].squeeze().cpu().detach().numpy())\n",
    "plt.show()\n",
    "\n",
    "plt.imshow(np.moveaxis(item['output']['image'].squeeze().cpu().detach().numpy(), 0, -1))\n",
    "plt.show()\n",
    "\n",
    "plt.imshow(np.moveaxis(item['dynamics']['mask'].cpu().detach().numpy(), 0, -1).squeeze())\n",
    "plt.show()\n",
    "\n",
    "print(item[\"output\"][\"idx\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GENERATE PLY FILES FROM ICL DYNAMICS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import open3d as o3d\n",
    "\n",
    "if is_on_colab:\n",
    "    output_path = \"/content/\"\n",
    "else:\n",
    "    output_path = \"/home/lukas/Desktop/\"\n",
    "\n",
    "def render_pointclouds(H, W, depth, K, Kinv, RT1inv, RT2, dynamics, colors, prefix=\"\"):\n",
    "\n",
    "    # decide if we want to normalise depth or not\n",
    "    #z = (depth - depth.min()) / (depth.max()- depth.min())\n",
    "    z = depth\n",
    "    fx = K[0,0]\n",
    "    fy = K[1,1]\n",
    "    cx = K[0,2]\n",
    "    cy = K[1,2]\n",
    "    # create coordinate system for x and y\n",
    "    '''\n",
    "    TODO: Are our coordinates correct? Or do we have to create them like Matthias does it? \n",
    "    (https://github.com/niessner/VoxelHashing/blob/421d548cca570ec14d51148cd6e4e5aca45652ef/DepthSensingCUDA/Source/DepthCameraUtil.h#L117)\n",
    "    I think with the calculation in the toImagePlane method in the dataloader we are doing something similar, but is it exactly the same?\n",
    "\n",
    "    Comment Lukas: Source Code from Matthias is the implementation of Kinv * (xz, yz, z) which we also do.\n",
    "    The toImagePlane method takes care of normalizing depth_from_cam to global 3D z coordinate, e.g. a wall is now straight (has same depth everywhere)\n",
    "    That method is needed because ICL dataset has depth_from_cam as default value and thus, points further away from the cam have greater depth (e.g. wall does not have same depth everywhere)\n",
    "\n",
    "    TODO: maybe try out going backwards from GT image to first view with RT2inv*RT1, to see if transformation works?\n",
    "    Comment Lukas: I implemented this in the next 2 cells\n",
    "    '''\n",
    "    #xs = torch.linspace(0, W - 1, W) / float(W - 1) * 2 - 1\n",
    "    #ys = torch.linspace(0, W - 1, W) / float(W - 1) * 2 - 1\n",
    "    #xs = xs.view(1, 1, 1, W).repeat(1, 1, W, 1)\n",
    "    #ys = ys.view(1, 1, W, 1).repeat(1, 1, 1, W)\n",
    "\n",
    "    '''\n",
    "    TODO: instead of linspace from [-1,1] use the original size (640x480), also without Resize(256,256)!\n",
    "    Maybe this works better?\n",
    "    '''\n",
    "    xs = torch.linspace(0, W - 1, W)\n",
    "    ys = torch.linspace(0, H - 1, H)\n",
    "    #xs /= float(W-1) * 2 - 1\n",
    "    #ys /= float(H-1) * 2 - 1\n",
    "    xs = xs.view(1, 1, 1, W).repeat(1, 1, H, 1)\n",
    "    ys = ys.view(1, 1, H, 1).repeat(1, 1, 1, W)\n",
    "\n",
    "    xyzs = torch.cat((xs, ys, torch.ones(xs.size()), torch.ones(xs.size())), 1).view(1, 4, -1)\n",
    "\n",
    "    print(xyzs)\n",
    "\n",
    "    print(xyzs.shape)\n",
    "    print(z.shape)\n",
    "\n",
    "    pcd = o3d.geometry.PointCloud()\n",
    "    pcd.points = o3d.utility.Vector3dVector(xyzs[:,:3,:].permute(0,2,1).cpu().numpy().astype(np.float64).squeeze())\n",
    "    pcd.colors = o3d.utility.Vector3dVector(colors.view(3, -1).permute((1,0)).cpu().numpy().astype(np.float64))\n",
    "    o3d.io.write_point_cloud(output_path + prefix + \"cam1_image.ply\", pcd)\n",
    "\n",
    "    \n",
    "    \n",
    "    f = 10.0\n",
    "    n = 0.1\n",
    "    perspective = torch.zeros(4,4)\n",
    "    perspective[0,0] = 2 * K[0,0] / W\n",
    "    perspective[0,2] = -(2 * (K[0,2] / W) - 1)\n",
    "    perspective[1,1] = 2 * K[1,1] / H\n",
    "    perspective[1,2] = -(2 * (K[1,2] / H) - 1)\n",
    "    perspective[2,2] = -(f+n)/(f-n)\n",
    "    perspective[2,3] = -2*f*n/(f-n)\n",
    "    perspective[3,2] = -1\n",
    "    p_inv = perspective.inverse()\n",
    "\n",
    "    print(perspective)\n",
    "    print(p_inv)\n",
    "    \n",
    "    # calculate z back to NDC (because currently given as linear depth / f)\n",
    "    \n",
    "    #z_view = z\n",
    "    \n",
    "    #z_view = z * f # back to [n, f] range\n",
    "    z_view = z\n",
    "    \n",
    "    \n",
    "    #print(\"SHAPE\", z_view.shape)\n",
    "    #print(torch.min(z_view))\n",
    "    #print(torch.max(z_view))\n",
    "    #z_ndc = - perspective[2,2] - (perspective[2,3] / z_view) # this is z_view * perspective and the perspective-z-divide: https://www.derschmale.com/2014/01/26/reconstructing-positions-from-the-depth-buffer/\n",
    "    \n",
    "    #z_clip = perspective[2,2] * z_view + perspective[2,3]\n",
    "    #w_clip = perspective[3,2] * z_view\n",
    "    #print(\"SHAPE\", z_clip.shape)\n",
    "    #print(torch.min(z_clip))\n",
    "    #print(torch.max(z_clip))\n",
    "    \n",
    "    #z_ndc = z_clip / w_clip\n",
    "    \n",
    "    #print(\"SHAPE\", z_ndc.shape)\n",
    "    #print(torch.min(z_ndc))\n",
    "    #print(torch.max(z_ndc))\n",
    "    \n",
    "    # add depth\n",
    "    cam_coor = xyzs * z_view\n",
    "    #cam_coor = xyzs\n",
    "    #cam_coor[:, 2, :] = z_ndc\n",
    "    print(cam_coor.shape)\n",
    "    cam_coor[:, -1, :] = 1 \n",
    "\n",
    "    pcd = o3d.geometry.PointCloud()\n",
    "    pcd.points = o3d.utility.Vector3dVector(cam_coor[:,:3,:].permute(0,2,1).cpu().numpy().astype(np.float64).squeeze())\n",
    "    pcd.colors = o3d.utility.Vector3dVector(colors.view(3, -1).permute((1,0)).cpu().numpy().astype(np.float64))\n",
    "    o3d.io.write_point_cloud(output_path + prefix + \"cam1_projection.ply\", pcd)\n",
    "\n",
    "    '''\n",
    "    leave out Kinv transformation here because that leads to strange behaviour.\n",
    "\n",
    "    Comment Lukas: When using the original linspace 640x480 then we can do this\n",
    "\n",
    "    See here: https://svncvpr.in.tum.de/cvpr-ros-pkg/trunk/rgbd_benchmark/rgbd_benchmark_tools/src/rgbd_benchmark_tools/generate_pointcloud.py\n",
    "    '''\n",
    "\n",
    "    print(cam_coor.shape)\n",
    "    #cam_coor[:, :3, :] = Kinv.matmul(cam_coor[:, :3, :])\n",
    "    #cam_coor = p_inv.matmul(cam_coor)\n",
    "    #cam_coor[:, :3] /= cam_coor[:, 3]\n",
    "    #cam_coor[:,3] = 1\n",
    "    \n",
    "    # x\n",
    "    #cam_coor[:,0] = -(cam_coor[:,0] + perspective[0,2]) * z_view / 2 * K[0,0] / W\n",
    "    \n",
    "    # y\n",
    "    #cam_coor[:,1] = -(cam_coor[:,1] + perspective[1,2]) * z_view / 2 * K[1,1] / W\n",
    "    \n",
    "    #z\n",
    "    #cam_coor[:,2] = z_view\n",
    "    \n",
    "    #w\n",
    "    #cam_coor[:,3] = 1\n",
    "    \n",
    "    cam_coor[:, :3, :] = Kinv.matmul(cam_coor[:, :3, :])\n",
    "    \n",
    "    show_plot(cam_coor[:,:3], color=colors.view(3, -1).permute((1,0)).numpy())\n",
    "\n",
    "    pcd = o3d.geometry.PointCloud()\n",
    "    pcd.points = o3d.utility.Vector3dVector(cam_coor[:,:3,:].permute(0,2,1).cpu().numpy().astype(np.float64).squeeze())\n",
    "    pcd.colors = o3d.utility.Vector3dVector(colors.view(3, -1).permute((1,0)).cpu().numpy().astype(np.float64))\n",
    "    o3d.io.write_point_cloud(output_path + prefix + \"cam1.ply\", pcd)\n",
    "\n",
    "    # tranform to world coordinates \n",
    "    world_coor = RT1inv.matmul(cam_coor)\n",
    "\n",
    "    pcd = o3d.geometry.PointCloud()\n",
    "    pcd.points = o3d.utility.Vector3dVector(world_coor[:,:3,:].permute(0,2,1).cpu().numpy().astype(np.float64).squeeze())\n",
    "    pcd.colors = o3d.utility.Vector3dVector(colors.view(3, -1).permute((1,0)).cpu().numpy().astype(np.float64))\n",
    "    o3d.io.write_point_cloud(output_path + prefix + \"world.ply\", pcd)\n",
    "    \n",
    "    # apply dynamics\n",
    "    print(world_coor.shape)\n",
    "    transformation = dynamics[\"transformation\"]\n",
    "    \n",
    "    # change transformation to be on world coordinates - in the mesh I applied it in model coordinates!!\n",
    "    # how to do it --> Transform points in model coordinates --> apply transformation --> back to world coordinates\n",
    "    #model_to_world = torch.eye(4)\n",
    "    #model_to_world[1,1] = -1 # scale -y\n",
    "    #model_to_world[0,0] = -1 # scale -x\n",
    "    #model_to_world[2,2] = -1 # scale -z\n",
    "    \n",
    "    #dynamics_transform = transformation.matmul(model_to_world.inverse())\n",
    "    #dynamics_transform = model_to_world.matmul(dynamics_transform)\n",
    "    \n",
    "    transformation[:, 0,3] *= -1\n",
    "    #transformation[:, 2,3] *= -1\n",
    "    \n",
    "    \n",
    "    print(transformation)\n",
    "    mask = dynamics[\"mask\"].view(-1)\n",
    "    print(mask.shape)\n",
    "    world_coor[:,:,mask] = transformation.matmul(world_coor[:,:,mask])\n",
    "    \n",
    "    pcd = o3d.geometry.PointCloud()\n",
    "    pcd.points = o3d.utility.Vector3dVector(world_coor[:,:3,:].permute(0,2,1).cpu().numpy().astype(np.float64).squeeze())\n",
    "    pcd.colors = o3d.utility.Vector3dVector(colors.view(3, -1).permute((1,0)).cpu().numpy().astype(np.float64))\n",
    "    o3d.io.write_point_cloud(output_path + prefix + \"world_dyn.ply\", pcd)\n",
    "\n",
    "    # transform to Camera Coordinates of Viewpoint 2\n",
    "    new_coor = RT2.matmul(world_coor)\n",
    "    new_coor = new_coor[:,:3]\n",
    "\n",
    "    pcd = o3d.geometry.PointCloud()\n",
    "    pcd.points = o3d.utility.Vector3dVector(new_coor[:,:3].permute(0,2,1).cpu().numpy().astype(np.float64).squeeze())\n",
    "    pcd.colors = o3d.utility.Vector3dVector(colors.view(3, -1).permute((1,0)).cpu().numpy().astype(np.float64))\n",
    "    o3d.io.write_point_cloud(output_path + prefix + \"cam2.ply\", pcd)\n",
    "\n",
    "    #go back to image coordinates\n",
    "    new_img = K.matmul(new_coor)\n",
    "    #new_img = new_img[:,:3]\n",
    "    \n",
    "    #new_img = perspective.matmul(new_coor)\n",
    "    # perspective z-divide back to ndc\n",
    "    #new_img[:, :3] /= new_img[:, 3]\n",
    "    #new_img = new_img[:,:3]\n",
    "\n",
    "    pcd = o3d.geometry.PointCloud()\n",
    "    pcd.points = o3d.utility.Vector3dVector(new_img[:,:3].permute(0,2,1).cpu().numpy().astype(np.float64).squeeze())\n",
    "    pcd.colors = o3d.utility.Vector3dVector(colors.view(3, -1).permute((1,0)).cpu().numpy().astype(np.float64))\n",
    "    o3d.io.write_point_cloud(output_path + prefix + \"cam2_projection.ply\", pcd)\n",
    "    \n",
    "    new_img[:, :2] = new_img[:, :2] / new_img[:, 2]\n",
    "    #new_img[:, 2] = 1\n",
    "    show_plot(new_img, color=colors.view(3, -1).permute((1,0)).numpy())\n",
    "\n",
    "    # initialise PointCloud for Open3D and add Points and colors to it in open3d format.\n",
    "    # open3d expects float64 numpy arrays to convert with vector3dvector function\n",
    "    pcd = o3d.geometry.PointCloud()\n",
    "    pcd.points = o3d.utility.Vector3dVector(new_img.permute(0,2,1).cpu().numpy().astype(np.float64).squeeze())\n",
    "    pcd.colors = o3d.utility.Vector3dVector(colors.view(3, -1).permute((1,0)).cpu().numpy().astype(np.float64))\n",
    "    # save pointcloud as .ply file\n",
    "    o3d.io.write_point_cloud(output_path + prefix + \"cam2_image.ply\", pcd)\n",
    "\n",
    "    if is_on_colab:\n",
    "        cmd_string = \"zip /content/\" + prefix + \"pointclouds.zip /content/\" + prefix + \"*.ply\"\n",
    "        os.system(cmd_string)\n",
    "        from google.colab import files\n",
    "        files.download(\"/content/\" + prefix + \"pointclouds.zip\")\n",
    "        !rm /content/*.ply"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GENERATE POINT CLOUD FROM INPUT IMAGE\n",
    "\n",
    "#W = item['depth'].shape[-1]\n",
    "_, H, W = item['depth'].shape\n",
    "depth = item['depth'].view(1, -1)\n",
    "K = item['cam']['K'][:3, :3]\n",
    "Kinv = item['cam']['Kinv'][:3, :3]\n",
    "RT1inv = item['cam']['RT1inv']\n",
    "RT2 = item['cam']['RT2']\n",
    "dynamics = item['dynamics']\n",
    "\n",
    "# ALTERNATIVE\n",
    "RT1inv = item['cam']['RT1']\n",
    "RT2 = item['cam']['RT2inv']\n",
    "\n",
    "print(RT1inv)\n",
    "print(RT2)\n",
    "print(H, W)\n",
    "print(K)\n",
    "\n",
    "render_pointclouds(H, W, depth, K, Kinv, RT1inv, RT2, dynamics, colors=item[\"image\"], prefix=\"from_input_\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PROJECT ICL DYNAMICS WITH PtsManipulator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from projection.z_buffer_manipulator import PtsManipulator\n",
    "\n",
    "_, H, W = item['depth'].shape\n",
    "\n",
    "K = item['cam']['K'].unsqueeze(0)\n",
    "Kinv = item['cam']['Kinv'].unsqueeze(0)\n",
    "#K=torch.eye(4).unsqueeze(0)\n",
    "#Kinv=torch.eye(4).unsqueeze(0)\n",
    "input_RT = item['cam']['RT1'].unsqueeze(0)\n",
    "input_RTinv = item['cam']['RT1inv'].unsqueeze(0)\n",
    "output_RT = item['cam']['RT2'].unsqueeze(0)\n",
    "output_RTinv = item['cam']['RT2inv'].unsqueeze(0)\n",
    "\n",
    "dynamics = item['dynamics']\n",
    "\n",
    "print(dynamics)\n",
    "\n",
    "mani = PtsManipulator(mode=PtsManipulator.icl_nuim_mode, W=W, H=H)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda:0\"\n",
    "    mani.to(device)\n",
    "    item['image'] = item['image'].to(device)\n",
    "    item['depth'] = item['depth'].to(device)\n",
    "    K = K.to(device)\n",
    "    Kinv = Kinv.to(device)\n",
    "    input_RT = input_RT.to(device)\n",
    "    input_RTinv = input_RTinv.to(device)\n",
    "    output_RT = output_RT.to(device)\n",
    "    output_RTinv = output_RTinv.to(device)\n",
    "    dynamics[\"transformation\"] = dynamics[\"transformation\"].to(device)\n",
    "\n",
    "img = mani.forward_justpts(item['image'].unsqueeze(0), item['depth'].unsqueeze(0),\n",
    "                           K, Kinv,\n",
    "                           input_RT, input_RTinv,\n",
    "                           output_RT, output_RTinv,\n",
    "                           dynamics)\n",
    "\n",
    "print(\"FINAL\")\n",
    "print(img.shape)\n",
    "show_image(img.squeeze().permute((1,2,0)).cpu().detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(img.shape)\n",
    "print(\"RESULT\")\n",
    "show_image(img.squeeze().permute((1,2,0)).cpu().detach().numpy())\n",
    "print(\"OUTPUT\")\n",
    "show_image(item['output']['image'].squeeze().permute((1,2,0)).cpu().detach().numpy())\n",
    "print(\"INPUT\")\n",
    "show_image(item['image'].squeeze().permute((1,2,0)).cpu().detach().numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ICL DYNAMICS WITH BATCH-SIZE > 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_cuda(data_tuple):\n",
    "    out = ()\n",
    "    if torch.cuda.is_available():\n",
    "        for data in data_tuple:\n",
    "            if isinstance(data, dict):\n",
    "                for k, v in data.items():\n",
    "                    data[k] = data[k].to(\"cuda:0\")\n",
    "                out += (data,)\n",
    "            else:\n",
    "                out += (data.to(\"cuda:0\"),)\n",
    "    return out\n",
    "\n",
    "def default_batch_loader(batch):\n",
    "    input_img = batch['image']\n",
    "    K = batch['cam']['K']\n",
    "    K_inv = batch['cam']['Kinv']\n",
    "    input_RT = batch['cam']['RT1']\n",
    "    input_RT_inv = batch['cam']['RT1inv']\n",
    "    output_RT = batch['cam']['RT2']\n",
    "    output_RT_inv = batch['cam']['RT2inv']\n",
    "    gt_img = batch['output']['image'] if batch['output'] is not None else None\n",
    "    depth_img = batch['depth']\n",
    "    dynamics = batch['dynamics']\n",
    "\n",
    "    # modified return statement to work directly with PtsManipulator#forward_justpts method\n",
    "    return input_img, depth_img, K, K_inv, input_RT, input_RT_inv, output_RT, output_RT_inv, dynamics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data_loader = torch.utils.data.DataLoader(dataset,\n",
    "                                           batch_size=2, \n",
    "                                           num_workers=1)\n",
    "\n",
    "mani = PtsManipulator(mode=PtsManipulator.icl_nuim_mode, W=W, H=H)\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda:0\"\n",
    "    mani.to(device)\n",
    "\n",
    "for i, batch in enumerate(data_loader):\n",
    "    gt_img = batch[\"output\"][\"image\"]\n",
    "    batch = to_cuda(default_batch_loader(batch))\n",
    "    img = mani.forward_justpts(*batch)\n",
    "    print(img.shape)\n",
    "    \n",
    "    print(\"RESULT\")\n",
    "    show_image(img[0].permute((1,2,0)).cpu().detach().numpy())\n",
    "    print(\"OUTPUT\")\n",
    "    show_image(gt_img[0].permute((1,2,0)).cpu().detach().numpy())\n",
    "    print(\"INPUT\")\n",
    "    show_image(batch[0][0].permute((1,2,0)).cpu().detach().numpy())\n",
    "    \n",
    "    print(\"RESULT\")\n",
    "    show_image(img[1].permute((1,2,0)).cpu().detach().numpy())\n",
    "    print(\"OUTPUT\")\n",
    "    show_image(gt_img[1].permute((1,2,0)).cpu().detach().numpy())\n",
    "    print(\"INPUT\")\n",
    "    show_image(batch[0][1].permute((1,2,0)).cpu().detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "L9ZkStRMnqbX"
   ],
   "include_colab_link": true,
   "name": "Point_Manipulation.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
