{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4hmm4vcJti8k"
   },
   "source": [
    "# Basic Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PEG28Bp3aIAs"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is on colab:  False\n",
      "Is on zerus: True\n"
     ]
    }
   ],
   "source": [
    "# Check device running the notebook automatically\n",
    "import sys\n",
    "is_on_colab = 'google.colab' in sys.modules\n",
    "is_on_zerus = 'teampc' in sys.argv[0]\n",
    "print(\"Is on colab: \", is_on_colab)\n",
    "print(\"Is on zerus:\", is_on_zerus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yVHnBLJjvr-N"
   },
   "source": [
    "## Setup for Colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "B1BOhxh7BEYm"
   },
   "outputs": [],
   "source": [
    "if is_on_colab:\n",
    "    # Google Colab setup\n",
    "    \n",
    "    # Mount drive\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive', force_remount=True)\n",
    "\n",
    "    # Retrieve repository and cd into root folder\n",
    "    from getpass import getpass\n",
    "    import urllib\n",
    "    import os\n",
    "    os.chdir(\"/content\")\n",
    "    user = input('Github user name: ')\n",
    "    password = getpass('Github password: ')\n",
    "    password = urllib.parse.quote(password) # your password is converted into url format\n",
    "    branch = \"\" # \"-b \" + \"branch_name\"\n",
    "    cmd_string = 'git clone {0} https://{1}:{2}@github.com/lukasHoel/novel-view-synthesis.git'.format(branch, user, password)\n",
    "    os.system(cmd_string)\n",
    "    os.chdir(\"novel-view-synthesis\")\n",
    "\n",
    "    # Install PyTorch3D libraries (required for pointcloud computations.)\n",
    "    !pip install 'git+https://github.com/facebookresearch/pytorch3d.git'\n",
    "    !pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VF0QazUmv5lY"
   },
   "source": [
    "## Setup for Local Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VTsC8dapAyAD"
   },
   "outputs": [],
   "source": [
    "# ONLY NECESSARY FOR LOCAL EXECUTION (WORKS WITHOUT THIS CELL IN GOOGLE COLAB)\n",
    "# Setup that is necessary for jupyter notebook to find sibling-directories\n",
    "# see: https://stackoverflow.com/questions/34478398/import-local-function-from-a-module-housed-in-another-directory-with-relative-im\n",
    "\n",
    "\n",
    "if not is_on_colab:\n",
    "    \n",
    "    import os\n",
    "    import sys\n",
    "    module_path = os.path.abspath(os.path.join('..'))\n",
    "    if module_path not in sys.path:\n",
    "        sys.path.append(module_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Gd7dyzCGwCZo"
   },
   "source": [
    "## General Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EGfclJp_AyA7"
   },
   "outputs": [],
   "source": [
    "# Imports for this notebook\n",
    "from models.nvs_model import NovelViewSynthesisModel\n",
    "from models.synthesis.synt_loss_metric import SynthesisLoss\n",
    "from util.nvs_solver import NVS_Solver\n",
    "from util.gan_wrapper_solver import GAN_Wrapper_Solver\n",
    "from data.nuim_dataloader import ICLNUIMDataset\n",
    "from data.nuim_dynamics_dataloader import ICLNUIM_Dynamic_Dataset\n",
    "from data.mp3d_dataloader import MP3D_Habitat_Offline_Dataset\n",
    "from projection.z_buffer_manipulator import PtsManipulator\n",
    "\n",
    "from torch.utils import data\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "import torchvision.transforms\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "B7R9GHGlhsMQ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training is on GPU with CUDA: True\n",
      "Device: cuda:0\n",
      "Tue Jun 30 11:54:12 2020       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 410.104      Driver Version: 410.104      CUDA Version: 10.0     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  GeForce GTX 108...  Off  | 00000000:4C:00.0 Off |                  N/A |\n",
      "| 19%   41C    P0    57W / 250W |     10MiB / 11178MiB |      0%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  GeForce GTX 108...  Off  | 00000000:4D:00.0 Off |                  N/A |\n",
      "| 19%   40C    P0    51W / 250W |      0MiB / 11178MiB |      7%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   2  GeForce GTX 108...  Off  | 00000000:4E:00.0 Off |                  N/A |\n",
      "| 55%   85C    P2   195W / 250W |   6027MiB / 11176MiB |    100%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                       GPU Memory |\n",
      "|  GPU       PID   Type   Process name                             Usage      |\n",
      "|=============================================================================|\n",
      "|    2      6728      C   python                                      6015MiB |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "# Check training on GPU?\n",
    "\n",
    "cuda = torch.cuda.is_available()\n",
    "\n",
    "print(\"Training is on GPU with CUDA: {}\".format(cuda))\n",
    "\n",
    "device = \"cuda:0\" if cuda else \"cpu\"\n",
    "\n",
    "print(\"Device: {}\".format(device))\n",
    "\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "u7mIvRRwJGlR"
   },
   "outputs": [],
   "source": [
    "def count_parameters(model):\n",
    "    \"\"\"Given a model return total number of parameters\"\"\"\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QkO15Cr3t3pA"
   },
   "source": [
    "# Load Data\n",
    "Load ICL-NUIM dataset or Matterport3D dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lQOjeo-oTNny"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using dataset: icl\n",
      "Image Size: 128\n",
      "Using dynamics: True\n"
     ]
    }
   ],
   "source": [
    "#dataset_mode = PtsManipulator.matterport_mode\n",
    "dataset_mode = PtsManipulator.icl_nuim_mode \n",
    "\n",
    "use_dynamics = True\n",
    "\n",
    "image_size = 128\n",
    "\n",
    "print(\"Using dataset: \" + dataset_mode)\n",
    "print(\"Image Size: \" + str(image_size))\n",
    "print(\"Using dynamics: \" + str(use_dynamics))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TcKlv4vsAyBE"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded following data: /mnt/raid/teampc/ICL-NUIM/custom/seq0001 (samples: 64) with configuration: {'mode': 'icl', 'image_size': 128, 'use_dynamics': True, 'path': '/mnt/raid/teampc/ICL-NUIM/custom/seq0001', 'sampleOutput': True, 'inverse_depth': False, 'cacheItems': False, 'icl_nuim_output_size': 128, 'icl_dynamic_output_from_other_view': False}\n"
     ]
    }
   ],
   "source": [
    "# Load dataset from drive or local\n",
    "\n",
    "if is_on_colab:\n",
    "    if dataset_mode == PtsManipulator.matterport_mode:\n",
    "        path = \"/content/drive/My Drive/Novel_View_Synthesis/matterport3d\"\n",
    "    elif dataset_mode == PtsManipulator.icl_nuim_mode:\n",
    "        path = \"/content/drive/My Drive/Novel_View_Synthesis/ICL-NUIM/living_room_traj2_loop\"\n",
    "        \n",
    "elif is_on_zerus:\n",
    "    if dataset_mode == PtsManipulator.matterport_mode:\n",
    "        raise ValueError(\"Path to mp3d on zerus not specified in this notebook!\")\n",
    "    elif dataset_mode == PtsManipulator.icl_nuim_mode and not use_dynamics:\n",
    "        path = \"/mnt/raid/teampc/ICL-NUIM/living_room_traj2_loop\"\n",
    "    elif dataset_mode == PtsManipulator.icl_nuim_mode and use_dynamics:\n",
    "        path = \"/mnt/raid/teampc/ICL-NUIM/custom/seq0001\"\n",
    "        \n",
    "else:\n",
    "    if dataset_mode == PtsManipulator.matterport_mode:\n",
    "        path = \"/home/lukas/Desktop/git/synsin/dataset\"\n",
    "    elif dataset_mode == PtsManipulator.icl_nuim_mode and not use_dynamics:\n",
    "        path = \"/home/lukas/Desktop/datasets/ICL-NUIM/prerendered_data/living_room_traj2_loop\"\n",
    "    elif dataset_mode == PtsManipulator.icl_nuim_mode and use_dynamics:\n",
    "        path = \"/home/lukas/Desktop/datasets/ICL-NUIM/custom/seq0001\"\n",
    "\n",
    "transform = torchvision.transforms.Compose([\n",
    "    #torchvision.transforms.ToPILImage(), # no longer needed: new dataloader now returns PIL Images\n",
    "    torchvision.transforms.Resize((image_size, image_size)),\n",
    "    torchvision.transforms.ToTensor()\n",
    "])\n",
    "    \n",
    "data_dict = {\n",
    "    \"mode\": dataset_mode,\n",
    "    \"image_size\": image_size,\n",
    "    \"use_dynamics\": use_dynamics,\n",
    "    \"path\": path,\n",
    "    \"sampleOutput\": True,\n",
    "    \"inverse_depth\": False,\n",
    "    \"cacheItems\": False, # Caching will work only if num_workers = 0. Decide what you like more!\n",
    "}\n",
    "    \n",
    "if dataset_mode == PtsManipulator.matterport_mode:\n",
    "    \n",
    "    # THIS IS THE HARDCODED IMAGE SIZE THAT WE SET IN THE HABITAT FRAMEWORK WHEN RENDERING MP3D IMAGES\n",
    "    # THIS DOES NOT CHANGE WHEN WE USE DIFFERENT IMAGE SIZES IN A TRANSFORM OBJECT\n",
    "    # WHEN CHANGING THE IMAGE SIZE IN TRANSFORM OBJECT, THIS GETS REFLECTED IN THE image_size ATTRIBUTE\n",
    "    data_dict['mp3d_image_input_size'] = 256\n",
    "    \n",
    "    data_dict['train_path'] = path + \"/train\"\n",
    "    data_dict['val_path'] = path + \"/val\"\n",
    "\n",
    "    train_dataset = MP3D_Habitat_Offline_Dataset(data_dict['train_path'],\n",
    "                                        in_size=data_dict['mp3d_image_input_size'],\n",
    "                                        transform=transform,\n",
    "                                        sampleOutput=data_dict[\"sampleOutput\"],\n",
    "                                        inverse_depth=data_dict[\"inverse_depth\"],\n",
    "                                        cacheItems=data_dict[\"cacheItems\"])\n",
    "    \n",
    "    print(\"Loaded following data: {} (samples: {}) with configuration: {}\\n\".format(data_dict[\"train_path\"], len(train_dataset), data_dict))\n",
    "    \n",
    "    val_dataset = MP3D_Habitat_Offline_Dataset(data_dict['val_path'],\n",
    "                                        in_size=data_dict['mp3d_image_input_size'],\n",
    "                                        transform=transform,\n",
    "                                        sampleOutput=data_dict[\"sampleOutput\"],\n",
    "                                        inverse_depth=data_dict[\"inverse_depth\"],\n",
    "                                        cacheItems=data_dict[\"cacheItems\"])\n",
    "    \n",
    "    print(\"Loaded following data: {} (samples: {}) with configuration: {}\\n\".format(data_dict[\"val_path\"], len(val_dataset), data_dict))\n",
    "        \n",
    "elif dataset_mode == PtsManipulator.icl_nuim_mode and not use_dynamics:\n",
    "\n",
    "    data_dict['icl_nuim_output_size'] = image_size\n",
    "    data_dict['path'] = path\n",
    "    \n",
    "    dataset = ICLNUIMDataset(data_dict['path'],\n",
    "                             transform=transform,\n",
    "                             sampleOutput=data_dict[\"sampleOutput\"],\n",
    "                             inverse_depth=data_dict[\"inverse_depth\"],\n",
    "                             cacheItems=data_dict[\"cacheItems\"], \n",
    "                             out_shape=(image_size, image_size))\n",
    "\n",
    "    print(\"Loaded following data: {} (samples: {}) with configuration: {}\".format(data_dict[\"path\"], len(dataset), data_dict))\n",
    "    \n",
    "elif dataset_mode == PtsManipulator.icl_nuim_mode and use_dynamics:\n",
    "    \n",
    "    data_dict['icl_nuim_output_size'] = image_size\n",
    "    data_dict['path'] = path\n",
    "    data_dict['icl_dynamic_output_from_other_view'] = False\n",
    "    \n",
    "    dataset = ICLNUIM_Dynamic_Dataset(data_dict['path'],\n",
    "                             sampleOutput=True,\n",
    "                             output_from_other_view=data_dict['icl_dynamic_output_from_other_view'], \n",
    "                             inverse_depth=False,\n",
    "                             cacheItems=False,\n",
    "                             transform=transform,\n",
    "                             out_shape=(image_size, image_size))\n",
    "    \n",
    "    print(\"Loaded following data: {} (samples: {}) with configuration: {}\".format(data_dict[\"path\"], len(dataset), data_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aJLyrjl2AyBK"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset parameters: {'batch_size': 8, 'num_workers': 1, 'random_seed': 42, 'shuffle_dataset': True, 'mode': 'icl', 'image_size': 128, 'use_dynamics': True, 'path': '/mnt/raid/teampc/ICL-NUIM/custom/seq0001', 'sampleOutput': True, 'inverse_depth': False, 'cacheItems': False, 'icl_nuim_output_size': 128, 'icl_dynamic_output_from_other_view': False, 'validation_percentage': 0.2, 'train_len': 7, 'val_len': 2}\n"
     ]
    }
   ],
   "source": [
    "dataset_args = {\n",
    "    \"batch_size\": 8,\n",
    "    \"num_workers\": 1, # Dataset Caching will work only if num_workers = 0. Decide what you like more!\n",
    "    \"random_seed\": 42, # seed random generation for shuffeling indices to always get same images in train/val\n",
    "    \"shuffle_dataset\": True,\n",
    "    **data_dict\n",
    "}\n",
    "\n",
    "if dataset_mode == PtsManipulator.matterport_mode:\n",
    "    # For mp3d we have separate train/val folders so we can just create different loaders out of the different datasets\n",
    "\n",
    "    train_len = len(train_dataset)\n",
    "    train_sampler = SubsetRandomSampler(list(range(train_len)))\n",
    "\n",
    "    val_len = len(val_dataset)\n",
    "    val_sampler = SubsetRandomSampler(list(range(val_len)))\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(train_dataset,\n",
    "                                               batch_size=dataset_args[\"batch_size\"], \n",
    "                                               #shuffle=dataset_args[\"shuffle_dataset\"],\n",
    "                                               sampler=train_sampler,\n",
    "                                               num_workers=dataset_args[\"num_workers\"])\n",
    "    \n",
    "    validation_loader = torch.utils.data.DataLoader(val_dataset,\n",
    "                                            batch_size=dataset_args[\"batch_size\"], \n",
    "                                            #shuffle=dataset_args[\"shuffle_dataset\"],\n",
    "                                            sampler=val_sampler,\n",
    "                                            num_workers=dataset_args[\"num_workers\"])\n",
    "\n",
    "elif dataset_mode == PtsManipulator.icl_nuim_mode:\n",
    "    # Create Train and Val dataset with 80% train and 20% val.\n",
    "    # from: https://stackoverflow.com/questions/50544730/how-do-i-split-a-custom-dataset-into-training-and-test-datasets\n",
    "\n",
    "    # For ICL dataset we do not have train/val datasets so we split the existing dataset 80% to 20%\n",
    "    dataset_args[\"validation_percentage\"] = 0.2\n",
    "\n",
    "    # Creating data indices for training and validation splits:\n",
    "    dataset_size = len(dataset)\n",
    "    indices = list(range(dataset_size))\n",
    "    split = int(np.floor(dataset_args[\"validation_percentage\"] * dataset_size))\n",
    "    if dataset_args[\"shuffle_dataset\"]:\n",
    "        np.random.seed(dataset_args[\"random_seed\"])\n",
    "        np.random.shuffle(indices)\n",
    "    train_indices, val_indices = indices[split:], indices[:split]\n",
    "\n",
    "    # #####################\n",
    "    # ICL OVERFITTING CASE:\n",
    "    # #####################\n",
    "    '''\n",
    "    train_indices = train_indices[:4] # train_indices[0:4] # [train_indices[0]]\n",
    "    val_indices = val_indices[:2]\n",
    "\n",
    "    overfit_item = dataset.__getitem__(train_indices[0])\n",
    "    print(\"OVERFITTING Input Image: {}, Output Image: {}\".format(\n",
    "        train_indices[0],\n",
    "        overfit_item[\"output\"][\"idx\"]))\n",
    "\n",
    "    input_img = overfit_item[\"image\"].cpu().detach().numpy()\n",
    "    output_img = overfit_item[\"output\"][\"image\"].cpu().detach().numpy()\n",
    "\n",
    "    print(torch.min(overfit_item[\"output\"][\"image\"]))\n",
    "    print(torch.max(overfit_item[\"output\"][\"image\"]))\n",
    "    print(overfit_item[\"cam\"])\n",
    "\n",
    "    %matplotlib inline\n",
    "\n",
    "    import matplotlib\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    print(\"OVERFIT TRAIN INPUT IMAGE\")\n",
    "    plt.imshow(np.moveaxis(input_img, 0, -1))\n",
    "    plt.show()\n",
    "\n",
    "    print(\"OVERFIT TRAIN OUTPUT IMAGE\")\n",
    "    plt.imshow(np.moveaxis(output_img, 0, -1))\n",
    "    plt.show()\n",
    "    '''\n",
    "    # #########################\n",
    "    # END ICL OVERFITTING CASE\n",
    "    # #########################\n",
    "    \n",
    "    # Creating PT data samplers and loaders:\n",
    "    train_sampler = SubsetRandomSampler(train_indices)\n",
    "    valid_sampler = SubsetRandomSampler(val_indices)\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(dataset,\n",
    "                                               batch_size=dataset_args[\"batch_size\"], \n",
    "                                               sampler=train_sampler,\n",
    "                                               num_workers=dataset_args[\"num_workers\"])\n",
    "    validation_loader = torch.utils.data.DataLoader(dataset,\n",
    "                                                    batch_size=dataset_args[\"batch_size\"],\n",
    "                                                    sampler=valid_sampler,\n",
    "                                                    num_workers=dataset_args[\"num_workers\"])\n",
    "\n",
    "dataset_args[\"train_len\"] = len(train_loader)\n",
    "dataset_args[\"val_len\"] = len(validation_loader)\n",
    "\n",
    "print(\"Dataset parameters: {}\".format(dataset_args))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qKgCXiSPZgY1"
   },
   "source": [
    "# Model & Loss Init\n",
    "\n",
    "Instantiate and initialize NovelViewSynthesisModel and a selected flavor of SynthesisLoss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hzBJYgAlZgY2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss names: ('l1', 'content')\n",
      "Weight of each loss: ('1.0', '0.0')\n",
      "Model configuration: {'imageSize': 128, 'use_gt_depth': False, 'normalize_images': False, 'use_rgb_features': False, 'num_depth_filters': 16, 'enc_dims': [3, 8, 8, 16, 16, 32, 32, 64, 64, 64, 128, 128, 256, 256], 'enc_blk_types': ['id', 'id', 'id', 'id', 'id', 'id', 'id', 'id', 'id', 'id', 'id', 'id', 'id'], 'enc_noisy_bn': False, 'enc_spectral_norm': True, 'dec_activation_func': Sigmoid(), 'dec_dims': [256, 256, 128, 128, 64, 64, 64, 32, 32, 16, 16, 8, 8, 3], 'dec_blk_types': ['id', 'id', 'id', 'id', 'id', 'id', 'id', 'id', 'id', 'id', 'id', 'id', 'id'], 'dec_noisy_bn': False, 'dec_spectral_norm': True, 'projection_mode': 'icl', 'l1_loss': '1.0_l1', 'content_loss': '0.0_content', 'model': 'NovelViewSynthesisModel'}\n",
      "Total number of paramaters: 7796231\n",
      "Parameters ENCODER: 2967622\n",
      "Parameters DEPTH: 2451233\n",
      "Parameters DECODER: 2377120\n"
     ]
    }
   ],
   "source": [
    "# TODO: Define more parameters in the dict according to availalbe ones in the model, as soon as they are needed.\n",
    "# Right now we just use the default parameters for the rest (see outcommented list or the .py file)\n",
    "    \n",
    "model_args={\n",
    "    'imageSize': image_size, # change this now in the first dataloading cell from above!\n",
    "    \n",
    "    'use_gt_depth': False,\n",
    "    'normalize_images': False,\n",
    "    'use_rgb_features': False,\n",
    "\n",
    "    'num_depth_filters': 16,\n",
    "    \n",
    "    'enc_dims': [3, 8, 8, 16, 16, 32, 32, 64, 64, 64, 128, 128, 256, 256],\n",
    "    'enc_blk_types': [\"id\", \"id\", \"id\", \"id\", \"id\", \"id\", \"id\", \"id\", \"id\", \"id\", \"id\", \"id\", \"id\"],\n",
    "    #'enc_dims': [3, 8, 8, 16, 16, 32, 32, 64, 64, 64],\n",
    "    #'enc_blk_types': [\"id\", \"id\", \"id\", \"id\", \"id\", \"id\", \"id\", \"id\", \"id\"],\n",
    "    #'enc_dims': [3, 8, 8],\n",
    "    #'enc_blk_types': [\"id\", \"id\"],\n",
    "    'enc_noisy_bn': False,\n",
    "    'enc_spectral_norm': True,\n",
    "    \n",
    "    'dec_activation_func': nn.Sigmoid(),\n",
    "    #'dec_dims': [64, 64, 32, 32, 32, 16, 16, 8, 8, 3],\n",
    "    #'dec_blk_types': [\"id\", \"id\", \"id\", \"id\", \"id\", \"id\", \"id\", \"id\", \"id\"],\n",
    "    'dec_dims': [256, 256, 128, 128, 64, 64, 64, 32, 32, 16, 16, 8, 8, 3],\n",
    "    'dec_blk_types': [\"id\", \"id\", \"id\", \"id\", \"id\", \"id\", \"id\", \"id\", \"id\", \"id\", \"id\", \"id\", \"id\"],\n",
    "    #'dec_dims': [3, 8, 8, 16, 16, 32, 32, 64, 64, 32, 32, 16, 16, 8, 8, 3],\n",
    "    #'dec_blk_types': [\"id\", \"id\", \"id\", \"id\", \"id\", \"id\", \"id\", \"id\", \"id\", \"id\", \"id\", \"id\", \"id\", \"id\", \"id\"],\n",
    "    'dec_noisy_bn': False,\n",
    "    'dec_spectral_norm': True,\n",
    "                      \n",
    "    'projection_mode': dataset_mode,\n",
    "    \n",
    "    # from here attributes for the loss of the nvs_model\n",
    "    'l1_loss': '1.0_l1',\n",
    "    'content_loss': '0.0_content', # synsin default: 10.0\n",
    "}\n",
    "\n",
    "# keep this loss object constant and modify usage of losses by e.g. setting one coefficient to 0\n",
    "nvs_loss = SynthesisLoss(losses=[\n",
    "    model_args['l1_loss'],\n",
    "    model_args['content_loss']\n",
    "])\n",
    "\n",
    "model = NovelViewSynthesisModel(imageSize=model_args['imageSize'],\n",
    "                                \n",
    "                                max_z=10,\n",
    "                                min_z=0,\n",
    "                                num_filters=model_args['num_depth_filters'],\n",
    "                                \n",
    "                                enc_dims=model_args['enc_dims'],\n",
    "                                enc_blk_types=model_args['enc_blk_types'],\n",
    "                                enc_noisy_bn=model_args['enc_noisy_bn'],\n",
    "                                enc_spectral_norm=model_args['enc_spectral_norm'],\n",
    "                                \n",
    "                                dec_dims=model_args['dec_dims'],\n",
    "                                dec_blk_types=model_args['dec_blk_types'],\n",
    "                                dec_activation_func=model_args['dec_activation_func'],\n",
    "                                dec_noisy_bn=model_args['dec_noisy_bn'],\n",
    "                                dec_spectral_norm=model_args['dec_spectral_norm'],\n",
    "                                \n",
    "                                projection_mode=model_args['projection_mode'],\n",
    "                                #points_per_pixel=8,\n",
    "                                #learn_feature=True,\n",
    "                                #radius=3.0,\n",
    "                                #rad_pow=2,\n",
    "                                #accumulation='alphacomposite',\n",
    "                                #accumulation_tau=1,\n",
    "                                \n",
    "                                use_rgb_features=model_args['use_rgb_features'],\n",
    "                                use_gt_depth=model_args['use_gt_depth'],\n",
    "                                #use_inverse_depth=False,\n",
    "                                normalize_images=model_args['normalize_images'])\n",
    "model_args[\"model\"] = type(model).__name__\n",
    "\n",
    "print(\"Model configuration: {}\".format(model_args))\n",
    "\n",
    "#print(\"Architecture:\", model)\n",
    "print(\"Total number of paramaters:\", count_parameters(model))\n",
    "print(\"Parameters ENCODER:\", count_parameters(model.encoder))\n",
    "print(\"Parameters DEPTH:\", count_parameters(model.pts_regressor))\n",
    "print(\"Parameters DECODER:\", count_parameters(model.projector))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HeFz-j00u5LR"
   },
   "source": [
    "# Training Visualization\n",
    "\n",
    "Start Tensorboard for visualization of the upcoming training / validation / test steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "c-6Mnrc-TPU9",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Start tensorboard. Might need to make sure, that the correct runs directory is chosen here.\n",
    "#%load_ext tensorboard\n",
    "#%tensorboard --logdir \"../runs\"\n",
    "#!tensorboard --logdir ../runs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-zet9gPxvM2i"
   },
   "source": [
    "# Training\n",
    "\n",
    "Start training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mvDPM-hKJGlt"
   },
   "outputs": [],
   "source": [
    "# This flag decides with solver gets used and where the logs will be logged into (into which directory)\n",
    "train_with_discriminator = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OTnq2RYJy4Dn"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log_dir: ../runs/Full_No_GAN/2020-Jun-30_11-56-16_f0c3f6aa-bab7-11ea-9e4e-bd4659db248c\n"
     ]
    }
   ],
   "source": [
    "# Create unique ID for this training process for saving to disk.\n",
    "\n",
    "from datetime import datetime\n",
    "import uuid\n",
    "now = datetime.now() # current date and time\n",
    "id = str(uuid.uuid1())\n",
    "id_suffix = now.strftime(\"%Y-%b-%d_%H-%M-%S\") + \"_\" + id\n",
    "\n",
    "if train_with_discriminator:\n",
    "    log_dir_name = \"Full_GAN\"\n",
    "else:\n",
    "    log_dir_name = \"Full_No_GAN\"\n",
    "\n",
    "log_dir = \"../runs/\" + log_dir_name + \"/\" + id_suffix # Might need to make sure, that the correct runs directory is chosen here.\n",
    "print(\"log_dir:\", log_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QY38vRjuAyBc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metric names: PSNR SSIM\n",
      "Hyperparameters of this solver: {'loss_function': 'SynthesisLoss', 'optimizer': 'Adam', 'learning_rate': 0.01, 'weight_decay': 0.0, 'imageSize': '128', 'use_gt_depth': 'False', 'normalize_images': 'False', 'use_rgb_features': 'False', 'num_depth_filters': '16', 'enc_dims': '[3, 8, 8, 16, 16, 32, 32, 64, 64, 64, 128, 128, 256, 256]', 'enc_blk_types': \"['id', 'id', 'id', 'id', 'id', 'id', 'id', 'id', 'id', 'id', 'id', 'id', 'id']\", 'enc_noisy_bn': 'False', 'enc_spectral_norm': 'True', 'dec_activation_func': 'Sigmoid()', 'dec_dims': '[256, 256, 128, 128, 64, 64, 64, 32, 32, 16, 16, 8, 8, 3]', 'dec_blk_types': \"['id', 'id', 'id', 'id', 'id', 'id', 'id', 'id', 'id', 'id', 'id', 'id', 'id']\", 'dec_noisy_bn': 'False', 'dec_spectral_norm': 'True', 'projection_mode': 'icl', 'l1_loss': '1.0_l1', 'content_loss': '0.0_content', 'model': 'NovelViewSynthesisModel', 'batch_size': '8', 'num_workers': '1', 'random_seed': '42', 'shuffle_dataset': 'True', 'mode': 'icl', 'image_size': '128', 'use_dynamics': 'True', 'path': '/mnt/raid/teampc/ICL-NUIM/custom/seq0001', 'sampleOutput': 'True', 'inverse_depth': 'False', 'cacheItems': 'False', 'icl_nuim_output_size': '128', 'icl_dynamic_output_from_other_view': 'False', 'validation_percentage': '0.2', 'train_len': '7', 'val_len': '2', 'num_D': '3', 'size_D': '64', 'loss_D': 'original', 'no_feature_loss': 'False', 'init_weights': 'True', 'lr_step': '10', 'lr_gamma': '0.3'}\n"
     ]
    }
   ],
   "source": [
    "# Configure solver\n",
    "extra_args = {\n",
    "    **model_args,\n",
    "    **dataset_args,\n",
    "    'num_D': 3, # number of discriminators, each downsamples by 2\n",
    "    'size_D': 64, # number of channels each conv in the discriminator has\n",
    "    'loss_D': 'original', # discriminator loss, options are original(cross-entropy), ls (MSE), hinge, w\n",
    "    'no_feature_loss': False, # if discriminator should not use feature loss\n",
    "    'init_weights': True,\n",
    "    'lr_step': 10, #number of epochs after which the learning rate is mulitplied with gamma\n",
    "    'lr_gamma': 0.3\n",
    "}\n",
    "\n",
    "if train_with_discriminator:\n",
    "    solver = GAN_Wrapper_Solver(optim_d=torch.optim.Adam,\n",
    "                                optim_d_args={\"lr\": 1e-3,\n",
    "                                              \"betas\": (0.9, 0.999),\n",
    "                                              \"eps\": 1e-8,\n",
    "                                              \"weight_decay\": 0.0},# is the l2 regularization parameter, see: https://pytorch.org/docs/stable/optim.html\n",
    "                                optim_g=torch.optim.Adam,\n",
    "                                optim_g_args={\"lr\": 1e-5,\n",
    "                                              \"betas\": (0.9, 0.999),\n",
    "                                              \"eps\": 1e-8,\n",
    "                                              \"weight_decay\": 0.0}, # is the l2 regularization parameter, see: https://pytorch.org/docs/stable/optim.html\n",
    "                                g_loss_func=nvs_loss,\n",
    "                                extra_args=extra_args,\n",
    "                                log_dir=log_dir,\n",
    "                                num_D=extra_args['num_D'],\n",
    "                                size_D=extra_args['size_D'],\n",
    "                                loss_D=extra_args['loss_D'],\n",
    "                                no_gan_feature_loss=extra_args['no_feature_loss'],\n",
    "                                init_discriminator_weights=extra_args['init_weights'],\n",
    "                                lr_step=extra_args['lr_step'],\n",
    "                                lr_gamma=extra_args['lr_gamma'])\n",
    "else:\n",
    "    solver = NVS_Solver(optim=torch.optim.Adam,\n",
    "                        optim_args={\"lr\": 1e-3,\n",
    "                                    \"betas\": (0.9, 0.999),\n",
    "                                    \"eps\": 1e-8,\n",
    "                                    \"weight_decay\": 0.0}, # is the l2 regularization parameter, see: https://pytorch.org/docs/stable/optim.html,\n",
    "                        loss_func=nvs_loss,\n",
    "                        extra_args=extra_args,\n",
    "                        tensorboard_writer=None, # let solver create a new instance\n",
    "                        log_dir=log_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1IhNe6ynzXox"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "START TRAIN on device: cuda:0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4bf7357fd55a42a2b677bf66eed816f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[EPOCH 1/100] TRAIN mean acc/loss: 0.36623328924179077/0.23115961253643036\n",
      "[EPOCH 1/100] VAL mean acc/loss: 0.4342935383319855/0.22014491260051727\n",
      "[EPOCH 2/100] TRAIN mean acc/loss: 0.37504562735557556/0.2159462422132492\n",
      "[EPOCH 2/100] VAL mean acc/loss: 0.41031405329704285/0.21959055960178375\n",
      "[EPOCH 3/100] TRAIN mean acc/loss: 0.38483813405036926/0.20236821472644806\n",
      "[EPOCH 3/100] VAL mean acc/loss: 0.4013960063457489/0.21427950263023376\n",
      "[EPOCH 4/100] TRAIN mean acc/loss: 0.4000573754310608/0.17544342577457428\n",
      "[EPOCH 4/100] VAL mean acc/loss: 0.41485533118247986/0.22618597745895386\n",
      "[EPOCH 5/100] TRAIN mean acc/loss: 0.500309944152832/0.16466565430164337\n",
      "[EPOCH 5/100] VAL mean acc/loss: 0.45054006576538086/0.20534653961658478\n",
      "[EPOCH 6/100] TRAIN mean acc/loss: 0.5534108877182007/0.15748049318790436\n",
      "[EPOCH 6/100] VAL mean acc/loss: 0.5170996189117432/0.183480903506279\n",
      "[EPOCH 7/100] TRAIN mean acc/loss: 0.5597397685050964/0.15571358799934387\n",
      "[EPOCH 7/100] VAL mean acc/loss: 0.5408817529678345/0.17848683893680573\n",
      "[EPOCH 8/100] TRAIN mean acc/loss: 0.5674611330032349/0.1497344821691513\n",
      "[EPOCH 8/100] VAL mean acc/loss: 0.5276266932487488/0.20843923091888428\n",
      "[EPOCH 9/100] TRAIN mean acc/loss: 0.5612769722938538/0.15089337527751923\n",
      "[EPOCH 9/100] VAL mean acc/loss: 0.5250264406204224/0.19716477394104004\n",
      "[EPOCH 10/100] TRAIN mean acc/loss: 0.5553049445152283/0.15549826622009277\n",
      "[EPOCH 10/100] VAL mean acc/loss: 0.5534594058990479/0.17159803211688995\n",
      "[EPOCH 11/100] TRAIN mean acc/loss: 0.5713015198707581/0.15223075449466705\n",
      "[EPOCH 11/100] VAL mean acc/loss: 0.5509932041168213/0.1741621345281601\n",
      "[EPOCH 12/100] TRAIN mean acc/loss: 0.5748865008354187/0.15137381851673126\n",
      "[EPOCH 12/100] VAL mean acc/loss: 0.5641399621963501/0.16546136140823364\n",
      "[EPOCH 13/100] TRAIN mean acc/loss: 0.5774896740913391/0.14492258429527283\n",
      "[EPOCH 13/100] VAL mean acc/loss: 0.5957520008087158/0.15067991614341736\n",
      "[EPOCH 14/100] TRAIN mean acc/loss: 0.5807557106018066/0.1418246477842331\n",
      "[EPOCH 14/100] VAL mean acc/loss: 0.5883617401123047/0.15102291107177734\n",
      "[EPOCH 15/100] TRAIN mean acc/loss: 0.5887377858161926/0.14141038060188293\n",
      "[EPOCH 15/100] VAL mean acc/loss: 0.5546122789382935/0.16558128595352173\n",
      "[EPOCH 16/100] TRAIN mean acc/loss: 0.5511574745178223/0.14843542873859406\n",
      "[EPOCH 16/100] VAL mean acc/loss: 0.531916618347168/0.16494928300380707\n",
      "[EPOCH 17/100] TRAIN mean acc/loss: 0.5308367609977722/0.15512172877788544\n",
      "[EPOCH 17/100] VAL mean acc/loss: 0.5468649864196777/0.16376742720603943\n",
      "[EPOCH 18/100] TRAIN mean acc/loss: 0.5594940781593323/0.15473711490631104\n",
      "[EPOCH 18/100] VAL mean acc/loss: 0.5193313360214233/0.17227119207382202\n",
      "[EPOCH 19/100] TRAIN mean acc/loss: 0.5591548085212708/0.15050861239433289\n",
      "[EPOCH 19/100] VAL mean acc/loss: 0.5265467166900635/0.1711486279964447\n",
      "[EPOCH 20/100] TRAIN mean acc/loss: 0.5602068305015564/0.147962287068367\n",
      "[EPOCH 20/100] VAL mean acc/loss: 0.5528868436813354/0.1596091389656067\n",
      "[EPOCH 21/100] TRAIN mean acc/loss: 0.5631967186927795/0.14083893597126007\n",
      "[EPOCH 21/100] VAL mean acc/loss: 0.5238080024719238/0.16750916838645935\n",
      "[EPOCH 22/100] TRAIN mean acc/loss: 0.5476460456848145/0.14272351562976837\n",
      "[EPOCH 22/100] VAL mean acc/loss: 0.47493699193000793/0.15979504585266113\n",
      "[EPOCH 23/100] TRAIN mean acc/loss: 0.5279619693756104/0.13645093142986298\n",
      "[EPOCH 23/100] VAL mean acc/loss: 0.51279616355896/0.15091732144355774\n",
      "[EPOCH 24/100] TRAIN mean acc/loss: 0.5583122372627258/0.13390648365020752\n",
      "[EPOCH 24/100] VAL mean acc/loss: 0.5423474311828613/0.148474782705307\n",
      "[EPOCH 25/100] TRAIN mean acc/loss: 0.5638378858566284/0.13360418379306793\n",
      "[EPOCH 25/100] VAL mean acc/loss: 0.5342416763305664/0.156471848487854\n",
      "[EPOCH 26/100] TRAIN mean acc/loss: 0.5627714991569519/0.133987158536911\n",
      "[EPOCH 26/100] VAL mean acc/loss: 0.5253157615661621/0.17578135430812836\n",
      "[EPOCH 27/100] TRAIN mean acc/loss: 0.5179776549339294/0.1590280681848526\n",
      "[EPOCH 27/100] VAL mean acc/loss: 0.480694979429245/0.20381712913513184\n",
      "[EPOCH 28/100] TRAIN mean acc/loss: 0.5137200951576233/0.15652522444725037\n",
      "[EPOCH 28/100] VAL mean acc/loss: 0.4798648953437805/0.18253234028816223\n",
      "[EPOCH 29/100] TRAIN mean acc/loss: 0.4769257605075836/0.1560741513967514\n",
      "[EPOCH 29/100] VAL mean acc/loss: 0.41419336199760437/0.21776701509952545\n",
      "[EPOCH 30/100] TRAIN mean acc/loss: 0.5109100937843323/0.1508243829011917\n",
      "[EPOCH 30/100] VAL mean acc/loss: 0.4346000850200653/0.22421325743198395\n",
      "[EPOCH 31/100] TRAIN mean acc/loss: 0.5128071904182434/0.14823590219020844\n",
      "[EPOCH 31/100] VAL mean acc/loss: 0.4260808825492859/0.26375511288642883\n",
      "[EPOCH 32/100] TRAIN mean acc/loss: 0.4921105206012726/0.1490093618631363\n",
      "[EPOCH 32/100] VAL mean acc/loss: 0.4590051472187042/0.20072609186172485\n",
      "[EPOCH 33/100] TRAIN mean acc/loss: 0.5145231485366821/0.14239893853664398\n",
      "[EPOCH 33/100] VAL mean acc/loss: 0.5107113122940063/0.18962356448173523\n",
      "[EPOCH 34/100] TRAIN mean acc/loss: 0.5375819802284241/0.14109860360622406\n",
      "[EPOCH 34/100] VAL mean acc/loss: 0.4831963777542114/0.1720457673072815\n",
      "[EPOCH 35/100] TRAIN mean acc/loss: 0.5484325885772705/0.13809522986412048\n",
      "[EPOCH 35/100] VAL mean acc/loss: 0.5398369431495667/0.15235833823680878\n",
      "[EPOCH 36/100] TRAIN mean acc/loss: 0.5514668226242065/0.13309553265571594\n",
      "[EPOCH 36/100] VAL mean acc/loss: 0.5234465003013611/0.15177679061889648\n",
      "[EPOCH 37/100] TRAIN mean acc/loss: 0.5554478764533997/0.13330671191215515\n",
      "[EPOCH 37/100] VAL mean acc/loss: 0.5472538471221924/0.15102997422218323\n",
      "[EPOCH 38/100] TRAIN mean acc/loss: 0.5610538125038147/0.1316719651222229\n",
      "[EPOCH 38/100] VAL mean acc/loss: 0.5509240031242371/0.15887516736984253\n",
      "[EPOCH 39/100] TRAIN mean acc/loss: 0.56076979637146/0.13400833308696747\n",
      "[EPOCH 39/100] VAL mean acc/loss: 0.5352257490158081/0.16134831309318542\n",
      "[EPOCH 40/100] TRAIN mean acc/loss: 0.5591170191764832/0.13060329854488373\n",
      "[EPOCH 40/100] VAL mean acc/loss: 0.5024824738502502/0.14406466484069824\n",
      "[EPOCH 41/100] TRAIN mean acc/loss: 0.556791365146637/0.12877336144447327\n",
      "[EPOCH 41/100] VAL mean acc/loss: 0.4771980047225952/0.18177837133407593\n",
      "[EPOCH 42/100] TRAIN mean acc/loss: 0.5123531222343445/0.1423603743314743\n",
      "[EPOCH 42/100] VAL mean acc/loss: 0.5327126979827881/0.15331271290779114\n",
      "[EPOCH 43/100] TRAIN mean acc/loss: 0.5464224815368652/0.1334560215473175\n",
      "[EPOCH 43/100] VAL mean acc/loss: 0.506659984588623/0.1739080250263214\n",
      "[EPOCH 44/100] TRAIN mean acc/loss: 0.555686891078949/0.13230235874652863\n",
      "[EPOCH 44/100] VAL mean acc/loss: 0.5434372425079346/0.15431217849254608\n",
      "[EPOCH 45/100] TRAIN mean acc/loss: 0.5613551735877991/0.13199011981487274\n",
      "[EPOCH 45/100] VAL mean acc/loss: 0.5421192646026611/0.15156984329223633\n",
      "[EPOCH 46/100] TRAIN mean acc/loss: 0.5513707399368286/0.13461561501026154\n",
      "[EPOCH 46/100] VAL mean acc/loss: 0.47162920236587524/0.19378706812858582\n",
      "[EPOCH 47/100] TRAIN mean acc/loss: 0.5353837013244629/0.1430353820323944\n",
      "[EPOCH 47/100] VAL mean acc/loss: 0.49179065227508545/0.18259119987487793\n",
      "[EPOCH 48/100] TRAIN mean acc/loss: 0.5280946493148804/0.14200559258460999\n",
      "[EPOCH 48/100] VAL mean acc/loss: 0.48207032680511475/0.1795954406261444\n",
      "[EPOCH 49/100] TRAIN mean acc/loss: 0.5372686982154846/0.1399388164281845\n",
      "[EPOCH 49/100] VAL mean acc/loss: 0.488045871257782/0.1803794503211975\n",
      "[EPOCH 50/100] TRAIN mean acc/loss: 0.5183016061782837/0.14725160598754883\n",
      "[EPOCH 50/100] VAL mean acc/loss: 0.48248517513275146/0.181599959731102\n",
      "[EPOCH 51/100] TRAIN mean acc/loss: 0.5170731544494629/0.14905956387519836\n",
      "[EPOCH 51/100] VAL mean acc/loss: 0.5402140617370605/0.18586453795433044\n",
      "[EPOCH 52/100] TRAIN mean acc/loss: 0.5399056077003479/0.14654378592967987\n",
      "[EPOCH 52/100] VAL mean acc/loss: 0.537067711353302/0.15247254073619843\n",
      "[EPOCH 53/100] TRAIN mean acc/loss: 0.5526400208473206/0.13584168255329132\n",
      "[EPOCH 53/100] VAL mean acc/loss: 0.5482710003852844/0.14699304103851318\n",
      "[EPOCH 54/100] TRAIN mean acc/loss: 0.5546216368675232/0.1344056874513626\n",
      "[EPOCH 54/100] VAL mean acc/loss: 0.5455837249755859/0.1473608762025833\n",
      "[EPOCH 55/100] TRAIN mean acc/loss: 0.5516499876976013/0.13275185227394104\n",
      "[EPOCH 55/100] VAL mean acc/loss: 0.5422965288162231/0.14808517694473267\n",
      "[EPOCH 56/100] TRAIN mean acc/loss: 0.550856351852417/0.13584046065807343\n",
      "[EPOCH 56/100] VAL mean acc/loss: 0.48566269874572754/0.17432527244091034\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[EPOCH 57/100] TRAIN mean acc/loss: 0.5393199324607849/0.1317393183708191\n",
      "[EPOCH 57/100] VAL mean acc/loss: 0.5399240255355835/0.151801198720932\n",
      "[EPOCH 58/100] TRAIN mean acc/loss: 0.5021136999130249/0.15104171633720398\n",
      "[EPOCH 58/100] VAL mean acc/loss: 0.47603511810302734/0.21641778945922852\n",
      "[EPOCH 59/100] TRAIN mean acc/loss: 0.4151744842529297/0.20627012848854065\n",
      "[EPOCH 59/100] VAL mean acc/loss: 0.38205549120903015/0.24563388526439667\n",
      "[EPOCH 60/100] TRAIN mean acc/loss: 0.4024325907230377/0.21574430167675018\n",
      "[EPOCH 60/100] VAL mean acc/loss: 0.3901605010032654/0.22088642418384552\n",
      "[EPOCH 61/100] TRAIN mean acc/loss: 0.43008431792259216/0.2079407274723053\n",
      "[EPOCH 61/100] VAL mean acc/loss: 0.3876503109931946/0.22629782557487488\n",
      "[EPOCH 62/100] TRAIN mean acc/loss: 0.4227430820465088/0.198365718126297\n",
      "[EPOCH 62/100] VAL mean acc/loss: 0.3567717671394348/0.23463742434978485\n",
      "[EPOCH 63/100] TRAIN mean acc/loss: 0.4451512396335602/0.17879749834537506\n",
      "[EPOCH 63/100] VAL mean acc/loss: 0.471881240606308/0.19383567571640015\n",
      "[EPOCH 64/100] TRAIN mean acc/loss: 0.44183650612831116/0.1811632364988327\n",
      "[EPOCH 64/100] VAL mean acc/loss: 0.3916207253932953/0.20305262506008148\n",
      "[EPOCH 65/100] TRAIN mean acc/loss: 0.4539194107055664/0.1825556606054306\n",
      "[EPOCH 65/100] VAL mean acc/loss: 0.48661181330680847/0.1890045702457428\n",
      "[EPOCH 66/100] TRAIN mean acc/loss: 0.5196834802627563/0.17302305996418\n",
      "[EPOCH 66/100] VAL mean acc/loss: 0.5099233388900757/0.18534846603870392\n",
      "[EPOCH 67/100] TRAIN mean acc/loss: 0.5239970088005066/0.17140726745128632\n",
      "[EPOCH 67/100] VAL mean acc/loss: 0.4953523278236389/0.2037421315908432\n",
      "[EPOCH 68/100] TRAIN mean acc/loss: 0.5064781308174133/0.18004350364208221\n",
      "[EPOCH 68/100] VAL mean acc/loss: 0.4958377480506897/0.19971102476119995\n",
      "[EPOCH 69/100] TRAIN mean acc/loss: 0.5045289397239685/0.17684324085712433\n",
      "[EPOCH 69/100] VAL mean acc/loss: 0.4012349545955658/0.2392643541097641\n",
      "[EPOCH 70/100] TRAIN mean acc/loss: 0.5352928042411804/0.16415153443813324\n",
      "[EPOCH 70/100] VAL mean acc/loss: 0.47601431608200073/0.2269231528043747\n",
      "[EPOCH 71/100] TRAIN mean acc/loss: 0.5500165820121765/0.1579662710428238\n",
      "[EPOCH 71/100] VAL mean acc/loss: 0.49027466773986816/0.2146715223789215\n",
      "[EPOCH 72/100] TRAIN mean acc/loss: 0.5278400182723999/0.15612439811229706\n",
      "[EPOCH 72/100] VAL mean acc/loss: 0.5354101061820984/0.16721898317337036\n",
      "[EPOCH 73/100] TRAIN mean acc/loss: 0.5513298511505127/0.15085938572883606\n",
      "[EPOCH 73/100] VAL mean acc/loss: 0.49929383397102356/0.18171754479408264\n",
      "[EPOCH 74/100] TRAIN mean acc/loss: 0.5391653776168823/0.1482841670513153\n",
      "[EPOCH 74/100] VAL mean acc/loss: 0.4934702515602112/0.16490527987480164\n",
      "[EPOCH 75/100] TRAIN mean acc/loss: 0.5465618371963501/0.17020143568515778\n",
      "[EPOCH 75/100] VAL mean acc/loss: 0.4896417260169983/0.20366428792476654\n",
      "[EPOCH 76/100] TRAIN mean acc/loss: 0.5451019406318665/0.14299844205379486\n",
      "[EPOCH 76/100] VAL mean acc/loss: 0.46417105197906494/0.18816480040550232\n",
      "[EPOCH 77/100] TRAIN mean acc/loss: 0.5419182181358337/0.13527880609035492\n",
      "[EPOCH 77/100] VAL mean acc/loss: 0.43283236026763916/0.1768644154071808\n",
      "[EPOCH 78/100] TRAIN mean acc/loss: 0.5516637563705444/0.12697169184684753\n",
      "[EPOCH 78/100] VAL mean acc/loss: 0.4424682557582855/0.17511755228042603\n",
      "[EPOCH 79/100] TRAIN mean acc/loss: 0.5446664094924927/0.129930779337883\n",
      "[EPOCH 79/100] VAL mean acc/loss: 0.519286036491394/0.15396764874458313\n",
      "[EPOCH 80/100] TRAIN mean acc/loss: 0.558678388595581/0.12689104676246643\n",
      "[EPOCH 80/100] VAL mean acc/loss: 0.5160605907440186/0.16936452686786652\n",
      "[EPOCH 81/100] TRAIN mean acc/loss: 0.5653975605964661/0.12546445429325104\n",
      "[EPOCH 81/100] VAL mean acc/loss: 0.5282517075538635/0.14207229018211365\n",
      "[EPOCH 82/100] TRAIN mean acc/loss: 0.5701321959495544/0.1264464557170868\n",
      "[EPOCH 82/100] VAL mean acc/loss: 0.5522565245628357/0.13732454180717468\n",
      "[EPOCH 83/100] TRAIN mean acc/loss: 0.5679982304573059/0.12479472160339355\n",
      "[EPOCH 83/100] VAL mean acc/loss: 0.544610857963562/0.14459671080112457\n",
      "[EPOCH 84/100] TRAIN mean acc/loss: 0.5580848455429077/0.12769488990306854\n",
      "[EPOCH 84/100] VAL mean acc/loss: 0.4555990397930145/0.1523894965648651\n",
      "[EPOCH 85/100] TRAIN mean acc/loss: 0.5557911992073059/0.12886865437030792\n",
      "[EPOCH 85/100] VAL mean acc/loss: 0.4657551348209381/0.18692557513713837\n",
      "[EPOCH 86/100] TRAIN mean acc/loss: 0.5538933873176575/0.1284778118133545\n",
      "[EPOCH 86/100] VAL mean acc/loss: 0.4641073942184448/0.22244112193584442\n",
      "[EPOCH 87/100] TRAIN mean acc/loss: 0.5578007102012634/0.1365610659122467\n",
      "[EPOCH 87/100] VAL mean acc/loss: 0.5294938087463379/0.1826574206352234\n",
      "[EPOCH 88/100] TRAIN mean acc/loss: 0.563782274723053/0.13583166897296906\n",
      "[EPOCH 88/100] VAL mean acc/loss: 0.5306278467178345/0.148337721824646\n",
      "[EPOCH 89/100] TRAIN mean acc/loss: 0.5668334364891052/0.13000665605068207\n",
      "[EPOCH 89/100] VAL mean acc/loss: 0.5393854975700378/0.15174643695354462\n",
      "[EPOCH 90/100] TRAIN mean acc/loss: 0.5718815922737122/0.12851662933826447\n",
      "[EPOCH 90/100] VAL mean acc/loss: 0.5545223951339722/0.1497201919555664\n",
      "[EPOCH 91/100] TRAIN mean acc/loss: 0.5725131034851074/0.12910479307174683\n",
      "[EPOCH 91/100] VAL mean acc/loss: 0.5538555383682251/0.14824144542217255\n",
      "[EPOCH 92/100] TRAIN mean acc/loss: 0.5760228037834167/0.1267465054988861\n",
      "[EPOCH 92/100] VAL mean acc/loss: 0.5287531614303589/0.1458783745765686\n",
      "[EPOCH 93/100] TRAIN mean acc/loss: 0.566398024559021/0.12614865601062775\n",
      "[EPOCH 93/100] VAL mean acc/loss: 0.4920087456703186/0.17067690193653107\n",
      "[EPOCH 94/100] TRAIN mean acc/loss: 0.5771386027336121/0.12625963985919952\n",
      "[EPOCH 94/100] VAL mean acc/loss: 0.49548301100730896/0.14153140783309937\n",
      "[EPOCH 95/100] TRAIN mean acc/loss: 0.5818316340446472/0.12415891140699387\n",
      "[EPOCH 95/100] VAL mean acc/loss: 0.5570158362388611/0.15226981043815613\n",
      "[EPOCH 96/100] TRAIN mean acc/loss: 0.5706278085708618/0.12446757405996323\n",
      "[EPOCH 96/100] VAL mean acc/loss: 0.5416487455368042/0.13882625102996826\n",
      "[EPOCH 97/100] TRAIN mean acc/loss: 0.5715479850769043/0.13003002107143402\n",
      "[EPOCH 97/100] VAL mean acc/loss: 0.5404040813446045/0.16036000847816467\n",
      "[EPOCH 98/100] TRAIN mean acc/loss: 0.5727141499519348/0.13012337684631348\n",
      "[EPOCH 98/100] VAL mean acc/loss: 0.5474114418029785/0.14820417761802673\n",
      "[EPOCH 99/100] TRAIN mean acc/loss: 0.5755382776260376/0.12459207326173782\n",
      "[EPOCH 99/100] VAL mean acc/loss: 0.47368815541267395/0.16071628034114838\n",
      "[EPOCH 100/100] TRAIN mean acc/loss: 0.566575288772583/0.13019007444381714\n",
      "[EPOCH 100/100] VAL mean acc/loss: 0.524253249168396/0.179427832365036\n",
      "\n",
      "FINISH.\n"
     ]
    }
   ],
   "source": [
    "# Start training\n",
    "\n",
    "num_epochs=100\n",
    "log_nth_iter=10\n",
    "log_nth_epoch=1\n",
    "tqdm_mode='total'\n",
    "'''\n",
    "tqdm_mode:\n",
    "    'total': tqdm log how long all epochs will take,\n",
    "    'epoch': tqdm for each epoch how long it will take,\n",
    "    anything else, e.g. None: do not use tqdm\n",
    "'''\n",
    "\n",
    "# TODO: Add parameters to extra_args dict?\n",
    "if train_with_discriminator:\n",
    "    steps = 1 # how many steps of training for discriminator/generator before switching to generator/discriminator\n",
    "    solver.train(model,\n",
    "                 train_loader, \n",
    "                 validation_loader,\n",
    "                 num_epochs=num_epochs,\n",
    "                 log_nth_iter=log_nth_iter,\n",
    "                 log_nth_epoch=log_nth_epoch,\n",
    "                 tqdm_mode=tqdm_mode,\n",
    "                 steps=steps)\n",
    "else:\n",
    "    solver.train(model,\n",
    "                 train_loader,\n",
    "                 validation_loader,\n",
    "                 num_epochs=num_epochs,\n",
    "                 log_nth_iter=log_nth_iter,\n",
    "                 log_nth_epoch=log_nth_epoch,\n",
    "                 tqdm_mode=tqdm_mode,\n",
    "                 verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tVWNikT4PvGj"
   },
   "outputs": [],
   "source": [
    "# To download tensorboard runs from Colab\n",
    "\n",
    "# TODO: Make sure that only new ones are copied --> for tensorboard runs on colab, do not use git repository as \"runs\" directory?\n",
    "# TODO: Instead of downloading, directly move it to the git repository that is currently checked out and push changes?\n",
    "if is_on_colab:\n",
    "  from google.colab import files\n",
    "  !zip -r /content/runs.zip /content/runs\n",
    "  files.download(\"/content/runs.zip\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "P2VDewrSvags"
   },
   "source": [
    "# Test\n",
    "\n",
    "Test with test dataset.\n",
    "Will load the data and start the training.\n",
    "\n",
    "Visualizations can be seen in Tensorboard above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "d8S9-1x0zbHZ"
   },
   "outputs": [],
   "source": [
    "# Load test data\n",
    "# TODO: Find real test split, for now we load the SAME dataset as for train/val (just that this notebook is complete...)\n",
    "# !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "if is_on_zerus:\n",
    "    test_path = \"/mnt/raid/teampc/ICL-NUIM/office_room_traj2_loop\"\n",
    "\n",
    "if is_on_colab:\n",
    "    test_path = \"/content/drive/My Drive/Novel_View_Synthesis/ICL-NUIM/living_room_traj2_loop\"\n",
    "\n",
    "test_dataset = ICLNUIMDataset(test_path, transform=transform) # TODO also use rest of parameters...\n",
    "\n",
    "test_indices = list(range(len(test_dataset)))\n",
    "np.random.shuffle(test_indices)\n",
    "\n",
    "test_sampler = SubsetRandomSampler(test_indices[:len(test_indices)//10])\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=dataset_args[\"batch_size\"], \n",
    "                                          sampler=test_sampler,\n",
    "                                          num_workers=4)\n",
    "\n",
    "print(\"Length of test set: {}\".format(len(test_loader)))\n",
    "print(\"Loaded test set: {}\".format(test_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cl2sFO4Kynp6"
   },
   "outputs": [],
   "source": [
    "# Start testing\n",
    "\n",
    "solver.test(model, test_loader, test_prefix=\"icl_test\", log_nth=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WlzKn0dWTNoU"
   },
   "source": [
    "## Generating a Test Time Trajectory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bKHYULewTNoV"
   },
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "from ipywidgets import interact, interactive, fixed, interact_manual\n",
    "import ipywidgets as widgets\n",
    "import math\n",
    "from util.nvs_solver import to_cuda, default_batch_loader\n",
    "%matplotlib inline\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# Load the model if needed by using the last cell\n",
    "\n",
    "model.eval().cuda()\n",
    "\n",
    "# Pick an image for the first frame and extract items related to it\n",
    "test_item_idx = 0\n",
    "test_item = test_dataset.__getitem__(test_item_idx)\n",
    "input_img, K, K_inv, input_RT, input_RT_inv, output_RT, output_RT_inv, gt_img, depth_img = to_cuda(default_batch_loader(test_item))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7DkK_yJpTNoX"
   },
   "source": [
    "### Rotation & Translation\n",
    "Use the sliders to jointly modify every axis and the rotation around them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AxmWFW1rTNoX"
   },
   "outputs": [],
   "source": [
    "# Keep modified RT2 matrices\n",
    "traj = []\n",
    "\n",
    "# A function to generate translational trajectory, modifies output_RT, output_RT_inv & gt_img\n",
    "def modify_frame(x=0,y=0,z=0, rx=0, ry=0, rz=0):\n",
    "    global traj, input_img, K, K_inv, input_RT, input_RT_inv, output_RT, output_RT_inv, gt_img, depth_img\n",
    "    R_X = torch.Tensor([\n",
    "      [ 1.0,  0.0, 0.0, x*255],\n",
    "      [ 0.0,  math.cos(rx*3.1415/180), -math.sin(rx*3.1415/180)/255, y*255],\n",
    "      [ 0.0,  math.sin(rx*3.1415/180)/255, math.cos(rx*3.1415/180), z],\n",
    "      [ 0.0,  0.0, 0.0, 1.0]]).cuda()\n",
    "    \n",
    "    R_Y = torch.Tensor([\n",
    "      [ math.cos(ry*3.1415/180),  0.0, math.sin(ry*3.1415/180)/255, 0.0],\n",
    "      [ 0.0,  1.0, 0.0, 0.0],\n",
    "      [ -math.sin(ry*3.1415/180)/255,  0.0, math.cos(ry*3.1415/180), 0.0],\n",
    "      [ 0.0,  0.0, 0.0, 1.0]]).cuda()\n",
    "    \n",
    "    R_Z = torch.Tensor([\n",
    "      [ math.cos(rz*3.1415/180),  -math.sin(rz*3.1415/180), 0.0, 0.0],\n",
    "      [ math.sin(rz*3.1415/180),  math.cos(rz*3.1415/180), 0.0, 0.0],\n",
    "      [ 0.0,  0.0, 1.0, 0.0],\n",
    "      [ 0.0,  0.0, 0.0, 1.0]]).cuda()\n",
    "    \n",
    "    # Translate input_RT by given x,y,z\n",
    "    output_RT_inv = (R_X@R_Y@R_Z).mm(input_RT_inv)\n",
    "\n",
    "    # Perform projection to obtain a pseudo GT for the manipulation\n",
    "    gt_img = model.pts_transformer.forward_justpts(\n",
    "        input_img.unsqueeze(0),\n",
    "        depth_img.unsqueeze(0),\n",
    "        K.unsqueeze(0),\n",
    "        K_inv.unsqueeze(0),\n",
    "        input_RT.unsqueeze(0),\n",
    "        input_RT_inv.unsqueeze(0),\n",
    "        output_RT.unsqueeze(0),\n",
    "        output_RT_inv.unsqueeze(0),\n",
    "    )\n",
    "    print(\"Projection with new RT:\")\n",
    "    gt_img_np = gt_img.squeeze(0).cpu().detach().numpy()\n",
    "    plt.imshow(np.moveaxis(gt_img_np, 0, -1))\n",
    "    plt.show()\n",
    "    \n",
    "    # Store matrices for the new view\n",
    "    traj.append((output_RT, output_RT_inv, gt_img))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wfRU_kyhTNoZ"
   },
   "outputs": [],
   "source": [
    "matrix = interact(modify_frame, \n",
    "                  x=(-1.0,1.0),\n",
    "                  y=(-1.0,1.0), \n",
    "                  z=(-1.0,1.0), \n",
    "                  rx=(-10.0,10.0, 1), \n",
    "                  ry=(-10.0,10.0, 1),\n",
    "                  rz=(-10.0,10.0, 1));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yyUp6f9bTNoc",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pprint(test_item[\"cam\"][\"RT1\"])\n",
    "# traj.pop(0) # Interactive slider sometimes first item has the same RT matrix as the input view (RT1), discard it\n",
    "pprint(traj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "M2Vsd4A3TNoe"
   },
   "outputs": [],
   "source": [
    "# It is important to pass this image to loader to apply same transforms that was applied during training.\n",
    "# We have to make sure that test time images get the same transforms as train time to have meaningful results.\n",
    "test_sampler = SubsetRandomSampler([test_item_idx])\n",
    "test_loader = torch.utils.data.DataLoader(dataset, batch_size=1,\n",
    "                                          sampler=test_sampler, num_workers=0)\n",
    "\n",
    "with torch.no_grad():\n",
    "    # Get the frame (triggers get_item and transforms)\n",
    "    test_item = next(iter(test_loader))\n",
    "    test_item = to_cuda(default_batch_loader(test_item)) # List of contents in dict\n",
    "    # For each different RT matrix perform a forward pass using GT depth\n",
    "    for output_RT, output_RT_inv, gt_img in traj:\n",
    "        out = model(test_item[0], # input_img\n",
    "                    test_item[1], # K\n",
    "                    test_item[2], # K_inv\n",
    "                    test_item[3], # input_RT\n",
    "                    test_item[4], # input_RT_inv\n",
    "                    output_RT.unsqueeze(0), \n",
    "                    output_RT_inv.unsqueeze(0), \n",
    "                    gt_img,                     # Not used\n",
    "                    test_item[-1]\n",
    "                   )              # GT depth\n",
    "        # Visualize prediction by network\n",
    "        pred = out[\"PredImg\"]\n",
    "        pred_np = pred.squeeze().cpu().detach().numpy()\n",
    "        plt.imshow(np.moveaxis(pred_np, 0, -1))\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pTLUJGpnBpud"
   },
   "source": [
    "# Save the model\n",
    "\n",
    "Save network with its weights to disk.\n",
    "\n",
    "See torch.save function: https://pytorch.org/docs/stable/notes/serialization.html#recommend-saving-models \n",
    "\n",
    "Load again with `the_model = TheModelClass(*args, **kwargs) the_model.load_state_dict(torch.load(PATH))`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7_zjMVB7Bpue"
   },
   "outputs": [],
   "source": [
    "def save_model(modelname, model):\n",
    "    from pathlib import Path\n",
    "    Path(\"../saved_models\").mkdir(parents=True, exist_ok=True)\n",
    "    # Might need to make sure, that the correct saved_results directory is chosen here.\n",
    "    filepath = \"../saved_models/\" + modelname + \".pt\"\n",
    "    torch.save(model.state_dict(), filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2JfoL3IHBpuv"
   },
   "outputs": [],
   "source": [
    "nvs_modelname = \"nvs_\" + id_suffix\n",
    "save_model(nvs_modelname, model)\n",
    "\n",
    "if train_with_discriminator:\n",
    "    # Also save the discriminator - currently this can only be accessed through the solver (change it!)\n",
    "    gan_modelname = \"gan_\" + id_suffix\n",
    "    save_model(gan_modelname, solver.netD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BwbC3OFOEmLX"
   },
   "outputs": [],
   "source": [
    "# LOAD MODEL AGAIN for verification purposes\n",
    "# Should print: <All keys matched successfully> per each model if it works\n",
    "\n",
    "new_model=False\n",
    "# add a different model name to be loaded here\n",
    "if new_model:\n",
    "    nvs_modelname=\"nvs_2020-May-29_18-44-55_b9c02778-a1cb-11ea-82a9-5542432396e9\"\n",
    "    gan_modelname=\"gan_2020-May-29_18-44-55_b9c02778-a1cb-11ea-82a9-5542432396e9\"\n",
    "    \n",
    "nvs_filepath = \"../saved_models/\" + nvs_modelname + \".pt\"\n",
    "print(\"NVS_Model loading: \", model.load_state_dict(torch.load(nvs_filepath)))\n",
    "\n",
    "if train_with_discriminator:\n",
    "    gan_filepath = \"../saved_models/\" + gan_modelname + \".pt\"\n",
    "    print(\"Discriminator loading: \", solver.netD.load_state_dict(torch.load(gan_filepath)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Jy91cvNeJGmc"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "entire_network_notebook.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
