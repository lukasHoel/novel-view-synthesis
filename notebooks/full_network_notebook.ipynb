{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4hmm4vcJti8k"
   },
   "source": [
    "# Basic Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PEG28Bp3aIAs"
   },
   "outputs": [],
   "source": [
    "# Check device running the notebook automatically\n",
    "import sys\n",
    "is_on_colab = 'google.colab' in sys.modules\n",
    "print(\"Is on colab: \", is_on_colab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yVHnBLJjvr-N"
   },
   "source": [
    "## Setup for Colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "B1BOhxh7BEYm"
   },
   "outputs": [],
   "source": [
    "if is_on_colab:\n",
    "    # Google Colab setup\n",
    "    \n",
    "    # Mount drive\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "\n",
    "    # Retrieve repository and cd into root folder\n",
    "    from getpass import getpass\n",
    "    import urllib\n",
    "    import os\n",
    "    user = input('Github user name: ')\n",
    "    password = getpass('Github password: ')\n",
    "    password = urllib.parse.quote(password) # your password is converted into url format\n",
    "    branch = \"\" # \"-b \" + \"branch_name\"\n",
    "    cmd_string = 'git clone {0} https://{1}:{2}@github.com/lukasHoel/novel-view-synthesis.git'.format(branch, user, password)\n",
    "    os.system(cmd_string)\n",
    "    os.chdir(\"novel-view-synthesis\")\n",
    "\n",
    "    # Install PyTorch3D libraries (required for pointcloud computations.)\n",
    "    !pip install 'git+https://github.com/facebookresearch/pytorch3d.git'\n",
    "    !pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VF0QazUmv5lY"
   },
   "source": [
    "## Setup for Local Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VTsC8dapAyAD"
   },
   "outputs": [],
   "source": [
    "# ONLY NECESSARY FOR LOCAL EXECUTION (WORKS WITHOUT THIS CELL IN GOOGLE COLAB)\n",
    "# Setup that is necessary for jupyter notebook to find sibling-directories\n",
    "# see: https://stackoverflow.com/questions/34478398/import-local-function-from-a-module-housed-in-another-directory-with-relative-im\n",
    "\n",
    "\n",
    "if not is_on_colab:\n",
    "    \n",
    "    import os\n",
    "    import sys\n",
    "    module_path = os.path.abspath(os.path.join('..'))\n",
    "    if module_path not in sys.path:\n",
    "        sys.path.append(module_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Gd7dyzCGwCZo"
   },
   "source": [
    "## General Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EGfclJp_AyA7"
   },
   "outputs": [],
   "source": [
    "# Imports for this notebook\n",
    "\n",
    "from models.nvs_model import NovelViewSynthesisModel\n",
    "from models.synthesis.synt_loss_metric import SynthesisLoss\n",
    "from util.nvs_solver import NVS_Solver\n",
    "from util.gan_wrapper_solver import GAN_Wrapper_Solver\n",
    "from data.nuim_dataloader import ICLNUIMDataset\n",
    "\n",
    "from torch.utils import data\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "import torchvision.transforms\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "B7R9GHGlhsMQ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training is on GPU with CUDA: True\n",
      "Device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "# Check training on GPU?\n",
    "\n",
    "cuda = torch.cuda.is_available()\n",
    "\n",
    "print(\"Training is on GPU with CUDA: {}\".format(cuda))\n",
    "\n",
    "device = \"cuda:0\" if cuda else \"cpu\"\n",
    "\n",
    "print(\"Device: {}\".format(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_parameters(model):\n",
    "    \"\"\"Given a model return total number of parameters\"\"\"\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qKgCXiSPZgY1"
   },
   "source": [
    "# Model & Loss Init\n",
    "\n",
    "Instantiate and initialize NovelViewSynthesisModel and a selected flavor of SynthesisLoss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hzBJYgAlZgY2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss names: ('l1', 'content')\n",
      "Weight of each loss: ('1.0', '1.0')\n",
      "Model configuration: {'imageSize': 64, 'use_gt_depth': True, 'normalize_images': False, 'use_rgb_features': False, 'enc_dims': [3, 8, 8, 16, 16, 32, 32, 64, 64, 64, 64, 64], 'enc_blk_types': ['id', 'id', 'id', 'id', 'id', 'id', 'id', 'id', 'id', 'id', 'id'], 'enc_noisy_bn': False, 'enc_spectral_norm': False, 'dec_activation_func': Sigmoid(), 'dec_dims': [64, 128, 64, 64, 64, 32, 32, 16, 16, 8, 8, 3], 'dec_blk_types': ['id', 'id', 'id', 'id', 'id', 'id', 'id', 'id', 'id', 'id', 'id'], 'dec_noisy_bn': False, 'dec_spectral_norm': False, 'l1_loss': '1.0_l1', 'content_loss': '1.0_content', 'model': 'NovelViewSynthesisModel'}\n",
      "Architecture: NovelViewSynthesisModel(\n",
      "  (dec_activation_func): Sigmoid()\n",
      "  (encoder): FeatureNet(\n",
      "    (res_blocks): Sequential(\n",
      "      (0): ResidualBlock(\n",
      "        (left_branch): Sequential(\n",
      "          (0): Conv2d(3, 8, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (1): Identity()\n",
      "        )\n",
      "        (right_branch): Sequential(\n",
      "          (0): LinearNoiseLayer(\n",
      "            (gain): Linear(in_features=20, out_features=3, bias=False)\n",
      "            (bias): Linear(in_features=20, out_features=3, bias=False)\n",
      "            (bn): bn()\n",
      "          )\n",
      "          (1): ReLU()\n",
      "          (2): Conv2d(3, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (3): LinearNoiseLayer(\n",
      "            (gain): Linear(in_features=20, out_features=8, bias=False)\n",
      "            (bias): Linear(in_features=20, out_features=8, bias=False)\n",
      "            (bn): bn()\n",
      "          )\n",
      "          (4): ReLU()\n",
      "          (5): Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (6): Identity()\n",
      "        )\n",
      "      )\n",
      "      (1): ResidualBlock(\n",
      "        (left_branch): Sequential(\n",
      "          (0): Conv2d(8, 8, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (1): Identity()\n",
      "        )\n",
      "        (right_branch): Sequential(\n",
      "          (0): LinearNoiseLayer(\n",
      "            (gain): Linear(in_features=20, out_features=8, bias=False)\n",
      "            (bias): Linear(in_features=20, out_features=8, bias=False)\n",
      "            (bn): bn()\n",
      "          )\n",
      "          (1): ReLU()\n",
      "          (2): Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (3): LinearNoiseLayer(\n",
      "            (gain): Linear(in_features=20, out_features=8, bias=False)\n",
      "            (bias): Linear(in_features=20, out_features=8, bias=False)\n",
      "            (bn): bn()\n",
      "          )\n",
      "          (4): ReLU()\n",
      "          (5): Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (6): Identity()\n",
      "        )\n",
      "      )\n",
      "      (2): ResidualBlock(\n",
      "        (left_branch): Sequential(\n",
      "          (0): Conv2d(8, 16, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (1): Identity()\n",
      "        )\n",
      "        (right_branch): Sequential(\n",
      "          (0): LinearNoiseLayer(\n",
      "            (gain): Linear(in_features=20, out_features=8, bias=False)\n",
      "            (bias): Linear(in_features=20, out_features=8, bias=False)\n",
      "            (bn): bn()\n",
      "          )\n",
      "          (1): ReLU()\n",
      "          (2): Conv2d(8, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (3): LinearNoiseLayer(\n",
      "            (gain): Linear(in_features=20, out_features=16, bias=False)\n",
      "            (bias): Linear(in_features=20, out_features=16, bias=False)\n",
      "            (bn): bn()\n",
      "          )\n",
      "          (4): ReLU()\n",
      "          (5): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (6): Identity()\n",
      "        )\n",
      "      )\n",
      "      (3): ResidualBlock(\n",
      "        (left_branch): Sequential(\n",
      "          (0): Conv2d(16, 16, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (1): Identity()\n",
      "        )\n",
      "        (right_branch): Sequential(\n",
      "          (0): LinearNoiseLayer(\n",
      "            (gain): Linear(in_features=20, out_features=16, bias=False)\n",
      "            (bias): Linear(in_features=20, out_features=16, bias=False)\n",
      "            (bn): bn()\n",
      "          )\n",
      "          (1): ReLU()\n",
      "          (2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (3): LinearNoiseLayer(\n",
      "            (gain): Linear(in_features=20, out_features=16, bias=False)\n",
      "            (bias): Linear(in_features=20, out_features=16, bias=False)\n",
      "            (bn): bn()\n",
      "          )\n",
      "          (4): ReLU()\n",
      "          (5): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (6): Identity()\n",
      "        )\n",
      "      )\n",
      "      (4): ResidualBlock(\n",
      "        (left_branch): Sequential(\n",
      "          (0): Conv2d(16, 32, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (1): Identity()\n",
      "        )\n",
      "        (right_branch): Sequential(\n",
      "          (0): LinearNoiseLayer(\n",
      "            (gain): Linear(in_features=20, out_features=16, bias=False)\n",
      "            (bias): Linear(in_features=20, out_features=16, bias=False)\n",
      "            (bn): bn()\n",
      "          )\n",
      "          (1): ReLU()\n",
      "          (2): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (3): LinearNoiseLayer(\n",
      "            (gain): Linear(in_features=20, out_features=32, bias=False)\n",
      "            (bias): Linear(in_features=20, out_features=32, bias=False)\n",
      "            (bn): bn()\n",
      "          )\n",
      "          (4): ReLU()\n",
      "          (5): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (6): Identity()\n",
      "        )\n",
      "      )\n",
      "      (5): ResidualBlock(\n",
      "        (left_branch): Sequential(\n",
      "          (0): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (1): Identity()\n",
      "        )\n",
      "        (right_branch): Sequential(\n",
      "          (0): LinearNoiseLayer(\n",
      "            (gain): Linear(in_features=20, out_features=32, bias=False)\n",
      "            (bias): Linear(in_features=20, out_features=32, bias=False)\n",
      "            (bn): bn()\n",
      "          )\n",
      "          (1): ReLU()\n",
      "          (2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (3): LinearNoiseLayer(\n",
      "            (gain): Linear(in_features=20, out_features=32, bias=False)\n",
      "            (bias): Linear(in_features=20, out_features=32, bias=False)\n",
      "            (bn): bn()\n",
      "          )\n",
      "          (4): ReLU()\n",
      "          (5): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (6): Identity()\n",
      "        )\n",
      "      )\n",
      "      (6): ResidualBlock(\n",
      "        (left_branch): Sequential(\n",
      "          (0): Conv2d(32, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (1): Identity()\n",
      "        )\n",
      "        (right_branch): Sequential(\n",
      "          (0): LinearNoiseLayer(\n",
      "            (gain): Linear(in_features=20, out_features=32, bias=False)\n",
      "            (bias): Linear(in_features=20, out_features=32, bias=False)\n",
      "            (bn): bn()\n",
      "          )\n",
      "          (1): ReLU()\n",
      "          (2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (3): LinearNoiseLayer(\n",
      "            (gain): Linear(in_features=20, out_features=64, bias=False)\n",
      "            (bias): Linear(in_features=20, out_features=64, bias=False)\n",
      "            (bn): bn()\n",
      "          )\n",
      "          (4): ReLU()\n",
      "          (5): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (6): Identity()\n",
      "        )\n",
      "      )\n",
      "      (7): ResidualBlock(\n",
      "        (left_branch): Sequential(\n",
      "          (0): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (1): Identity()\n",
      "        )\n",
      "        (right_branch): Sequential(\n",
      "          (0): LinearNoiseLayer(\n",
      "            (gain): Linear(in_features=20, out_features=64, bias=False)\n",
      "            (bias): Linear(in_features=20, out_features=64, bias=False)\n",
      "            (bn): bn()\n",
      "          )\n",
      "          (1): ReLU()\n",
      "          (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (3): LinearNoiseLayer(\n",
      "            (gain): Linear(in_features=20, out_features=64, bias=False)\n",
      "            (bias): Linear(in_features=20, out_features=64, bias=False)\n",
      "            (bn): bn()\n",
      "          )\n",
      "          (4): ReLU()\n",
      "          (5): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (6): Identity()\n",
      "        )\n",
      "      )\n",
      "      (8): ResidualBlock(\n",
      "        (left_branch): Sequential(\n",
      "          (0): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (1): Identity()\n",
      "        )\n",
      "        (right_branch): Sequential(\n",
      "          (0): LinearNoiseLayer(\n",
      "            (gain): Linear(in_features=20, out_features=64, bias=False)\n",
      "            (bias): Linear(in_features=20, out_features=64, bias=False)\n",
      "            (bn): bn()\n",
      "          )\n",
      "          (1): ReLU()\n",
      "          (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (3): LinearNoiseLayer(\n",
      "            (gain): Linear(in_features=20, out_features=64, bias=False)\n",
      "            (bias): Linear(in_features=20, out_features=64, bias=False)\n",
      "            (bn): bn()\n",
      "          )\n",
      "          (4): ReLU()\n",
      "          (5): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (6): Identity()\n",
      "        )\n",
      "      )\n",
      "      (9): ResidualBlock(\n",
      "        (left_branch): Sequential(\n",
      "          (0): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (1): Identity()\n",
      "        )\n",
      "        (right_branch): Sequential(\n",
      "          (0): LinearNoiseLayer(\n",
      "            (gain): Linear(in_features=20, out_features=64, bias=False)\n",
      "            (bias): Linear(in_features=20, out_features=64, bias=False)\n",
      "            (bn): bn()\n",
      "          )\n",
      "          (1): ReLU()\n",
      "          (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (3): LinearNoiseLayer(\n",
      "            (gain): Linear(in_features=20, out_features=64, bias=False)\n",
      "            (bias): Linear(in_features=20, out_features=64, bias=False)\n",
      "            (bn): bn()\n",
      "          )\n",
      "          (4): ReLU()\n",
      "          (5): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (6): Identity()\n",
      "        )\n",
      "      )\n",
      "      (10): ResidualBlock(\n",
      "        (left_branch): Sequential(\n",
      "          (0): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (1): Identity()\n",
      "        )\n",
      "        (right_branch): Sequential(\n",
      "          (0): LinearNoiseLayer(\n",
      "            (gain): Linear(in_features=20, out_features=64, bias=False)\n",
      "            (bias): Linear(in_features=20, out_features=64, bias=False)\n",
      "            (bn): bn()\n",
      "          )\n",
      "          (1): ReLU()\n",
      "          (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (3): LinearNoiseLayer(\n",
      "            (gain): Linear(in_features=20, out_features=64, bias=False)\n",
      "            (bias): Linear(in_features=20, out_features=64, bias=False)\n",
      "            (bn): bn()\n",
      "          )\n",
      "          (4): ReLU()\n",
      "          (5): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (6): Identity()\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (pts_regressor): Unet(\n",
      "    (conv1): Conv2d(3, 4, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "    (conv2): Conv2d(4, 8, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "    (conv3): Conv2d(8, 16, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "    (conv4): Conv2d(16, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "    (conv5): Conv2d(32, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "    (conv6): Conv2d(32, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "    (conv7): Conv2d(32, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "    (conv8): Conv2d(32, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "    (up): Upsample(scale_factor=2.0, mode=bilinear)\n",
      "    (dconv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (dconv2): Conv2d(64, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (dconv3): Conv2d(64, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (dconv4): Conv2d(64, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (dconv5): Conv2d(64, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (dconv6): Conv2d(32, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (dconv7): Conv2d(16, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (dconv8): Conv2d(8, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (batch_norm): BatchNorm2d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (batch_norm2_0): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (batch_norm2_1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (batch_norm4_0): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (batch_norm4_1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (batch_norm8_0): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (batch_norm8_1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (batch_norm8_2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (batch_norm8_3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (batch_norm8_4): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (batch_norm8_5): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (batch_norm8_6): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (batch_norm8_7): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (leaky_relu): LeakyReLU(negative_slope=0.2)\n",
      "    (relu): ReLU()\n",
      "    (tanh): Tanh()\n",
      "  )\n",
      "  (pts_transformer): PtsManipulator(\n",
      "    (splatter): RasterizePointsXYsBlending()\n",
      "  )\n",
      "  (projector): RefineNet(\n",
      "    (res_blocks): Sequential(\n",
      "      (0): ResidualBlock(\n",
      "        (left_branch): Sequential(\n",
      "          (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (1): Identity()\n",
      "        )\n",
      "        (right_branch): Sequential(\n",
      "          (0): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (1): ReLU()\n",
      "          (2): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (4): ReLU()\n",
      "          (5): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (6): Identity()\n",
      "        )\n",
      "      )\n",
      "      (1): ResidualBlock(\n",
      "        (left_branch): Sequential(\n",
      "          (0): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (1): Identity()\n",
      "        )\n",
      "        (right_branch): Sequential(\n",
      "          (0): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (1): ReLU()\n",
      "          (2): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (4): ReLU()\n",
      "          (5): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (6): Identity()\n",
      "        )\n",
      "      )\n",
      "      (2): ResidualBlock(\n",
      "        (left_branch): Sequential(\n",
      "          (0): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (1): Identity()\n",
      "        )\n",
      "        (right_branch): Sequential(\n",
      "          (0): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (1): ReLU()\n",
      "          (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (4): ReLU()\n",
      "          (5): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (6): Identity()\n",
      "        )\n",
      "      )\n",
      "      (3): ResidualBlock(\n",
      "        (left_branch): Sequential(\n",
      "          (0): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (1): Identity()\n",
      "        )\n",
      "        (right_branch): Sequential(\n",
      "          (0): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (1): ReLU()\n",
      "          (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (4): ReLU()\n",
      "          (5): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (6): Identity()\n",
      "        )\n",
      "      )\n",
      "      (4): ResidualBlock(\n",
      "        (left_branch): Sequential(\n",
      "          (0): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (1): Identity()\n",
      "        )\n",
      "        (right_branch): Sequential(\n",
      "          (0): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (1): ReLU()\n",
      "          (2): Conv2d(64, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (4): ReLU()\n",
      "          (5): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (6): Identity()\n",
      "        )\n",
      "      )\n",
      "      (5): ResidualBlock(\n",
      "        (left_branch): Sequential(\n",
      "          (0): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (1): Identity()\n",
      "        )\n",
      "        (right_branch): Sequential(\n",
      "          (0): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (1): ReLU()\n",
      "          (2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (4): ReLU()\n",
      "          (5): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (6): Identity()\n",
      "        )\n",
      "      )\n",
      "      (6): ResidualBlock(\n",
      "        (left_branch): Sequential(\n",
      "          (0): Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (1): Identity()\n",
      "        )\n",
      "        (right_branch): Sequential(\n",
      "          (0): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (1): ReLU()\n",
      "          (2): Conv2d(32, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (3): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (4): ReLU()\n",
      "          (5): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (6): Identity()\n",
      "        )\n",
      "      )\n",
      "      (7): ResidualBlock(\n",
      "        (left_branch): Sequential(\n",
      "          (0): Conv2d(16, 16, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (1): Identity()\n",
      "        )\n",
      "        (right_branch): Sequential(\n",
      "          (0): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (1): ReLU()\n",
      "          (2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (3): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (4): ReLU()\n",
      "          (5): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (6): Identity()\n",
      "        )\n",
      "      )\n",
      "      (8): ResidualBlock(\n",
      "        (left_branch): Sequential(\n",
      "          (0): Conv2d(16, 8, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (1): Identity()\n",
      "        )\n",
      "        (right_branch): Sequential(\n",
      "          (0): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (1): ReLU()\n",
      "          (2): Conv2d(16, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (3): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (4): ReLU()\n",
      "          (5): Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (6): Identity()\n",
      "        )\n",
      "      )\n",
      "      (9): ResidualBlock(\n",
      "        (left_branch): Sequential(\n",
      "          (0): Conv2d(8, 8, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (1): Identity()\n",
      "        )\n",
      "        (right_branch): Sequential(\n",
      "          (0): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (1): ReLU()\n",
      "          (2): Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (3): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (4): ReLU()\n",
      "          (5): Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (6): Identity()\n",
      "        )\n",
      "      )\n",
      "      (10): ResidualBlock(\n",
      "        (left_branch): Sequential(\n",
      "          (0): Conv2d(8, 3, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (1): Identity()\n",
      "        )\n",
      "        (right_branch): Sequential(\n",
      "          (0): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (1): ReLU()\n",
      "          (2): Conv2d(8, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (3): BatchNorm2d(3, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (4): ReLU()\n",
      "          (5): Conv2d(3, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (6): Identity()\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (activate_out): Sigmoid()\n",
      "  )\n",
      ")\n",
      "Total number of paramaters: 1172321\n"
     ]
    }
   ],
   "source": [
    "# TODO: Define more parameters in the dict according to availalbe ones in the model, as soon as they are needed.\n",
    "# Right now we just use the default parameters for the rest (see outcommented list or the .py file)\n",
    "    \n",
    "model_args={\n",
    "    'imageSize': 64,\n",
    "    \n",
    "    'use_gt_depth': True,\n",
    "    'normalize_images': False,\n",
    "    'use_rgb_features': False,\n",
    "    \n",
    "    'enc_dims': [3, 8, 8, 16, 16, 32, 32, 64, 64, 64, 64, 64],\n",
    "    'enc_blk_types': [\"id\", \"id\", \"id\", \"id\", \"id\", \"id\", \"id\", \"id\", \"id\", \"id\", \"id\"],\n",
    "    #'enc_dims': [3, 8, 8, 16, 16, 32, 32, 64, 64, 64],\n",
    "    #'enc_blk_types': [\"id\", \"id\", \"id\", \"id\", \"id\", \"id\", \"id\", \"id\", \"id\"],\n",
    "    #'enc_dims': [3, 8, 8],\n",
    "    #'enc_blk_types': [\"id\", \"id\"],\n",
    "    'enc_noisy_bn': False,\n",
    "    'enc_spectral_norm': False,\n",
    "    \n",
    "    'dec_activation_func': nn.Sigmoid(),\n",
    "    #'dec_dims': [64, 64, 32, 32, 32, 16, 16, 8, 8, 3],\n",
    "    #'dec_blk_types': [\"id\", \"id\", \"id\", \"id\", \"id\", \"id\", \"id\", \"id\", \"id\"],\n",
    "    'dec_dims': [64, 128, 64, 64, 64, 32, 32, 16, 16, 8, 8, 3],\n",
    "    'dec_blk_types': [\"id\", \"id\", \"id\", \"id\", \"id\", \"id\", \"id\", \"id\", \"id\", \"id\", \"id\"],\n",
    "    #'dec_dims': [3, 8, 8, 16, 16, 32, 32, 64, 64, 32, 32, 16, 16, 8, 8, 3],\n",
    "    #'dec_blk_types': [\"id\", \"id\", \"id\", \"id\", \"id\", \"id\", \"id\", \"id\", \"id\", \"id\", \"id\", \"id\", \"id\", \"id\", \"id\"],\n",
    "    'dec_noisy_bn': False,\n",
    "    'dec_spectral_norm': False,\n",
    "    \n",
    "    # from here attributes for the loss of the nvs_model\n",
    "    'l1_loss': '1.0_l1',\n",
    "    'content_loss': '1.0_content', # synsin default: 10.0\n",
    "}\n",
    "\n",
    "# keep this loss object constant and modify usage of losses by e.g. setting one coefficient to 0\n",
    "nvs_loss = SynthesisLoss(losses=[\n",
    "    model_args['l1_loss'],\n",
    "    model_args['content_loss']\n",
    "])\n",
    "\n",
    "model = NovelViewSynthesisModel(imageSize=model_args['imageSize'],\n",
    "                                \n",
    "                                #max_z=0,\n",
    "                                #min_z=0,\n",
    "                                \n",
    "                                enc_dims=model_args['enc_dims'],\n",
    "                                enc_blk_types=model_args['enc_blk_types'],\n",
    "                                enc_noisy_bn=model_args['enc_noisy_bn'],\n",
    "                                enc_spectral_norm=model_args['enc_spectral_norm'],\n",
    "                                \n",
    "                                dec_dims=model_args['dec_dims'],\n",
    "                                dec_blk_types=model_args['dec_blk_types'],\n",
    "                                dec_activation_func=model_args['dec_activation_func'],\n",
    "                                dec_noisy_bn=model_args['dec_noisy_bn'],\n",
    "                                dec_spectral_norm=model_args['dec_spectral_norm'],\n",
    "                                \n",
    "                                #points_per_pixel=8,\n",
    "                                #learn_feature=True,\n",
    "                                #radius=1.5,\n",
    "                                #rad_pow=2,\n",
    "                                #accumulation='alphacomposite',\n",
    "                                #accumulation_tau=1,\n",
    "                                \n",
    "                                use_rgb_features=model_args['use_rgb_features'],\n",
    "                                use_gt_depth=model_args['use_gt_depth'],\n",
    "                                #use_inverse_depth=False,\n",
    "                                normalize_images=model_args['normalize_images'])\n",
    "model_args[\"model\"] = type(model).__name__\n",
    "\n",
    "print(\"Model configuration: {}\".format(model_args))\n",
    "\n",
    "print(\"Architecture:\", model)\n",
    "print(\"Total number of paramaters:\", count_parameters(model))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QkO15Cr3t3pA"
   },
   "source": [
    "# Load Data\n",
    "Load ICL-NUIM dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TcKlv4vsAyBE"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded following data: /home/lukas/Desktop/datasets/ICL-NUIM/prerendered_data/living_room_traj2_loop (samples: 882) with configuration: {'path': '/home/lukas/Desktop/datasets/ICL-NUIM/prerendered_data/living_room_traj2_loop', 'depth_to_image_plane': True, 'use_real_intrinsics': False, 'sampleOutput': True, 'RTrelativeToOutput': False, 'inverse_depth': False, 'cacheItems': False}\n"
     ]
    }
   ],
   "source": [
    "# Load dataset from drive or local\n",
    "\n",
    "if is_on_colab:\n",
    "    path = \"/content/drive/My Drive/Novel_View_Synthesis/ICL-NUIM/living_room_traj2_loop\"\n",
    "else:\n",
    "    path = \"/home/lukas/Desktop/datasets/ICL-NUIM/prerendered_data/living_room_traj2_loop\"\n",
    "\n",
    "transform = torchvision.transforms.Compose([\n",
    "    #torchvision.transforms.ToPILImage(), # no longer needed: new dataloader now returns PIL Images\n",
    "    torchvision.transforms.Resize((model_args['imageSize'], model_args['imageSize'])),\n",
    "    torchvision.transforms.ToTensor(), \n",
    "])\n",
    "    \n",
    "data_dict = {\n",
    "    \"path\": path,\n",
    "    \"depth_to_image_plane\": True,\n",
    "    \"use_real_intrinsics\": False,\n",
    "    \"sampleOutput\": True,\n",
    "    \"RTrelativeToOutput\": False,\n",
    "    \"inverse_depth\": False,\n",
    "    \"cacheItems\": False, # Caching will work only if num_workers = 0. Decide what you like more!\n",
    "}\n",
    "    \n",
    "dataset = ICLNUIMDataset(path,\n",
    "                         transform=transform,\n",
    "                         depth_to_image_plane=data_dict[\"depth_to_image_plane\"],\n",
    "                         use_real_intrinsics=data_dict[\"use_real_intrinsics\"],\n",
    "                         sampleOutput=data_dict[\"sampleOutput\"],\n",
    "                         RTrelativeToOutput=data_dict[\"RTrelativeToOutput\"],\n",
    "                         inverse_depth=data_dict[\"inverse_depth\"],\n",
    "                         cacheItems=data_dict[\"cacheItems\"])\n",
    "\n",
    "print(\"Loaded following data: {} (samples: {}) with configuration: {}\".format(data_dict[\"path\"], len(dataset), data_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aJLyrjl2AyBK"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset parameters: {'batch_size': 4, 'validation_percentage': 0.2, 'shuffle_dataset': True, 'path': '/home/lukas/Desktop/datasets/ICL-NUIM/prerendered_data/living_room_traj2_loop', 'depth_to_image_plane': True, 'use_real_intrinsics': False, 'sampleOutput': True, 'RTrelativeToOutput': False, 'inverse_depth': False, 'cacheItems': False, 'train_len': 177, 'val_len': 44}\n"
     ]
    }
   ],
   "source": [
    "# Create Train and Val dataset with 80% train and 20% val.\n",
    "# from: https://stackoverflow.com/questions/50544730/how-do-i-split-a-custom-dataset-into-training-and-test-datasets\n",
    "\n",
    "dataset_args = {\n",
    "    \"batch_size\": 4,\n",
    "    \"validation_percentage\": 0.2,\n",
    "    \"shuffle_dataset\": True,\n",
    "    **data_dict\n",
    "}\n",
    "\n",
    "num_workers = 0 # Dataset Caching will work only if num_workers = 0. Decide what you like more!\n",
    "random_seed = 42 # seed random generation for shuffeling indices to always get same images in train/val\n",
    "\n",
    "# Creating data indices for training and validation splits:\n",
    "dataset_size = len(dataset)\n",
    "indices = list(range(dataset_size))\n",
    "split = int(np.floor(dataset_args[\"validation_percentage\"] * dataset_size))\n",
    "if dataset_args[\"shuffle_dataset\"]:\n",
    "    #np.random.seed(random_seed) # DO NOT SEED\n",
    "    np.random.shuffle(indices)\n",
    "train_indices, val_indices = indices[split:], indices[:split]\n",
    "\n",
    "# OVERFITTING CASE:\n",
    "'''\n",
    "train_indices = train_indices[:4] # train_indices[0:4] # [train_indices[0]]\n",
    "val_indices = val_indices[:2]\n",
    "\n",
    "overfit_item = dataset.__getitem__(train_indices[0])\n",
    "print(\"OVERFITTING Input Image: {}, Output Image: {}\".format(\n",
    "    train_indices[0],\n",
    "    overfit_item[\"output\"][\"idx\"]))\n",
    "\n",
    "input_img = overfit_item[\"image\"].cpu().detach().numpy()\n",
    "output_img = overfit_item[\"output\"][\"image\"].cpu().detach().numpy()\n",
    "\n",
    "print(torch.min(overfit_item[\"output\"][\"image\"]))\n",
    "print(torch.max(overfit_item[\"output\"][\"image\"]))\n",
    "print(overfit_item[\"cam\"])\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(\"OVERFIT TRAIN INPUT IMAGE\")\n",
    "plt.imshow(np.moveaxis(input_img, 0, -1))\n",
    "plt.show()\n",
    "\n",
    "print(\"OVERFIT TRAIN OUTPUT IMAGE\")\n",
    "plt.imshow(np.moveaxis(output_img, 0, -1))\n",
    "plt.show()\n",
    "'''\n",
    "# END OVERFITTING CASE\n",
    "\n",
    "\n",
    "# Creating PT data samplers and loaders:\n",
    "train_sampler = SubsetRandomSampler(train_indices)\n",
    "valid_sampler = SubsetRandomSampler(val_indices)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset, batch_size=dataset_args[\"batch_size\"], \n",
    "                                           sampler=train_sampler, num_workers=num_workers)\n",
    "validation_loader = torch.utils.data.DataLoader(dataset, batch_size=dataset_args[\"batch_size\"],\n",
    "                                                sampler=valid_sampler, num_workers=num_workers)\n",
    "\n",
    "dataset_args[\"train_len\"] = len(train_loader)\n",
    "dataset_args[\"val_len\"] = len(validation_loader)\n",
    "\n",
    "print(\"Dataset parameters: {}\".format(dataset_args))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HeFz-j00u5LR"
   },
   "source": [
    "# Training Visualization\n",
    "\n",
    "Start Tensorboard for visualization of the upcoming training / validation / test steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "c-6Mnrc-TPU9",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Start tensorboard. Might need to make sure, that the correct runs directory is chosen here.\n",
    "#%load_ext tensorboard\n",
    "#%tensorboard --logdir ../runs\n",
    "#!tensorboard --logdir ../runs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-zet9gPxvM2i"
   },
   "source": [
    "# Training\n",
    "\n",
    "Start training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This flag decides with solver gets used and where the logs will be logged into (into which directory)\n",
    "train_with_discriminator = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OTnq2RYJy4Dn"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log_dir: ../runs/Full_No_GAN/2020-May-09_12-49-02_b2335208-91e2-11ea-9343-edf6c16aa970\n"
     ]
    }
   ],
   "source": [
    "# Create unique ID for this training process for saving to disk.\n",
    "\n",
    "from datetime import datetime\n",
    "import uuid\n",
    "now = datetime.now() # current date and time\n",
    "id = str(uuid.uuid1())\n",
    "id_suffix = now.strftime(\"%Y-%b-%d_%H-%M-%S\") + \"_\" + id\n",
    "\n",
    "if train_with_discriminator:\n",
    "    log_dir_name = \"Full_GAN\"\n",
    "else:\n",
    "    log_dir_name = \"Full_No_GAN\"\n",
    "\n",
    "log_dir = \"../runs/\" + log_dir_name + \"/\" + id_suffix # Might need to make sure, that the correct runs directory is chosen here.\n",
    "print(\"log_dir:\", log_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QY38vRjuAyBc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metric names: PSNR SSIM\n",
      "Hyperparameters of this solver: {'loss_function': 'SynthesisLoss', 'optimizer': 'Adam', 'learning_rate': 0.004, 'weight_decay': 0.0, 'imageSize': '64', 'use_gt_depth': 'True', 'normalize_images': 'False', 'use_rgb_features': 'False', 'enc_dims': '[3, 8, 8, 16, 16, 32, 32, 64, 64, 64, 64, 64]', 'enc_blk_types': \"['id', 'id', 'id', 'id', 'id', 'id', 'id', 'id', 'id', 'id', 'id']\", 'enc_noisy_bn': 'False', 'enc_spectral_norm': 'False', 'dec_activation_func': 'Sigmoid()', 'dec_dims': '[64, 128, 64, 64, 64, 32, 32, 16, 16, 8, 8, 3]', 'dec_blk_types': \"['id', 'id', 'id', 'id', 'id', 'id', 'id', 'id', 'id', 'id', 'id']\", 'dec_noisy_bn': 'False', 'dec_spectral_norm': 'False', 'l1_loss': '1.0_l1', 'content_loss': '1.0_content', 'model': 'NovelViewSynthesisModel', 'batch_size': '4', 'validation_percentage': '0.2', 'shuffle_dataset': 'True', 'path': '/home/lukas/Desktop/datasets/ICL-NUIM/prerendered_data/living_room_traj2_loop', 'depth_to_image_plane': 'True', 'use_real_intrinsics': 'False', 'sampleOutput': 'True', 'RTrelativeToOutput': 'False', 'inverse_depth': 'False', 'cacheItems': 'False', 'train_len': '177', 'val_len': '44'}\n"
     ]
    }
   ],
   "source": [
    "# Configure solver\n",
    "extra_args = {\n",
    "    **model_args,\n",
    "    **dataset_args\n",
    "}\n",
    "\n",
    "if train_with_discriminator:\n",
    "    solver = GAN_Wrapper_Solver(optim_d=torch.optim.Adam,\n",
    "                                optim_d_args={\"lr\": 1e-2,\n",
    "                                              \"betas\": (0.9, 0.999),\n",
    "                                              \"eps\": 1e-8,\n",
    "                                              \"weight_decay\": 0.0},# is the l2 regularization parameter, see: https://pytorch.org/docs/stable/optim.html\n",
    "                                optim_g=torch.optim.Adam,\n",
    "                                optim_g_args={\"lr\": 1e-4,\n",
    "                                              \"betas\": (0.9, 0.999),\n",
    "                                              \"eps\": 1e-8,\n",
    "                                              \"weight_decay\": 0.0}, # is the l2 regularization parameter, see: https://pytorch.org/docs/stable/optim.html\n",
    "                                g_loss_func=nvs_loss,\n",
    "                                extra_args=extra_args,\n",
    "                                log_dir=log_dir,\n",
    "                                init_discriminator_weights=True)\n",
    "else:\n",
    "    solver = NVS_Solver(optim=torch.optim.Adam,\n",
    "                        optim_args={\"lr\": 4e-3,\n",
    "                                    \"betas\": (0.9, 0.999),\n",
    "                                    \"eps\": 1e-8,\n",
    "                                    \"weight_decay\": 0.0}, # is the l2 regularization parameter, see: https://pytorch.org/docs/stable/optim.html,\n",
    "                        loss_func=nvs_loss,\n",
    "                        extra_args=extra_args,\n",
    "                        tensorboard_writer=None, # let solver create a new instance\n",
    "                        log_dir=log_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1IhNe6ynzXox",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "START TRAIN on device: cuda:0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "690c4b0ec63b4eb3abc64cf8776e0a1d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Iteration 1/177] TRAIN loss: 1.4746381044387817\n",
      "[Iteration 101/177] TRAIN loss: 1.1100744009017944\n",
      "[EPOCH 1/100] TRAIN mean acc/loss: 0.4982454180717468/1.2126473188400269\n",
      "[Iteration 1/44] Val loss: 1.216338038444519\n",
      "[EPOCH 1/100] VAL mean acc/loss: 0.5103715658187866/1.2282030582427979\n",
      "[Iteration 1/177] TRAIN loss: 1.4165648221969604\n",
      "[Iteration 101/177] TRAIN loss: 1.1398279666900635\n",
      "[Iteration 1/44] Val loss: 1.200762391090393\n",
      "[Iteration 1/177] TRAIN loss: 1.0565669536590576\n",
      "[Iteration 101/177] TRAIN loss: 0.9038547873497009\n",
      "[Iteration 1/44] Val loss: 1.0313262939453125\n",
      "[Iteration 1/177] TRAIN loss: 0.892941415309906\n",
      "[Iteration 101/177] TRAIN loss: 0.9050414562225342\n",
      "[Iteration 1/44] Val loss: 1.0014495849609375\n",
      "[Iteration 1/177] TRAIN loss: 0.8218023777008057\n",
      "[Iteration 101/177] TRAIN loss: 0.7676307559013367\n",
      "[Iteration 1/44] Val loss: 0.8054351806640625\n",
      "[Iteration 1/177] TRAIN loss: 0.7853171825408936\n",
      "[Iteration 101/177] TRAIN loss: 0.7307523488998413\n",
      "[Iteration 1/44] Val loss: 0.7637450695037842\n",
      "[Iteration 1/177] TRAIN loss: 0.7960258722305298\n",
      "[Iteration 101/177] TRAIN loss: 0.6626992225646973\n",
      "[Iteration 1/44] Val loss: 0.5700755715370178\n",
      "[Iteration 1/177] TRAIN loss: 0.9096964597702026\n",
      "[Iteration 101/177] TRAIN loss: 0.628560483455658\n",
      "[Iteration 1/44] Val loss: 0.8312481045722961\n",
      "[Iteration 1/177] TRAIN loss: 0.6465891003608704\n",
      "[Iteration 101/177] TRAIN loss: 0.7656784057617188\n",
      "[Iteration 1/44] Val loss: 0.9688923954963684\n",
      "[Iteration 1/177] TRAIN loss: 0.965559720993042\n",
      "[Iteration 101/177] TRAIN loss: 0.769031822681427\n",
      "[Iteration 1/44] Val loss: 0.6252833008766174\n",
      "[Iteration 1/177] TRAIN loss: 0.6068033576011658\n",
      "[Iteration 101/177] TRAIN loss: 0.6613640785217285\n",
      "[EPOCH 11/100] TRAIN mean acc/loss: 0.6130952835083008/0.7170633673667908\n",
      "[Iteration 1/44] Val loss: 0.8292073011398315\n",
      "[EPOCH 11/100] VAL mean acc/loss: 0.6321057677268982/0.7047215104103088\n",
      "[Iteration 1/177] TRAIN loss: 0.8797260522842407\n",
      "[Iteration 101/177] TRAIN loss: 0.5843203663825989\n",
      "[Iteration 1/44] Val loss: 0.69882732629776\n",
      "[Iteration 1/177] TRAIN loss: 0.7163916826248169\n",
      "[Iteration 101/177] TRAIN loss: 0.6360533833503723\n",
      "[Iteration 1/44] Val loss: 0.6576330065727234\n",
      "[Iteration 1/177] TRAIN loss: 0.8196319341659546\n",
      "[Iteration 101/177] TRAIN loss: 0.8899105191230774\n",
      "[Iteration 1/44] Val loss: 0.5464491844177246\n",
      "[Iteration 1/177] TRAIN loss: 0.7978554368019104\n",
      "[Iteration 101/177] TRAIN loss: 0.5247625708580017\n",
      "[Iteration 1/44] Val loss: 0.556223452091217\n",
      "[Iteration 1/177] TRAIN loss: 0.5603681206703186\n",
      "[Iteration 101/177] TRAIN loss: 0.8288611173629761\n",
      "[Iteration 1/44] Val loss: 0.6113119721412659\n",
      "[Iteration 1/177] TRAIN loss: 0.7362209558486938\n",
      "[Iteration 101/177] TRAIN loss: 0.4979704022407532\n",
      "[Iteration 1/44] Val loss: 0.52723628282547\n",
      "[Iteration 1/177] TRAIN loss: 0.71092289686203\n",
      "[Iteration 101/177] TRAIN loss: 0.8295198082923889\n",
      "[Iteration 1/44] Val loss: 0.6112515926361084\n",
      "[Iteration 1/177] TRAIN loss: 0.6400840282440186\n",
      "[Iteration 101/177] TRAIN loss: 0.6297914385795593\n",
      "[Iteration 1/44] Val loss: 0.681675374507904\n",
      "[Iteration 1/177] TRAIN loss: 0.625270664691925\n",
      "[Iteration 101/177] TRAIN loss: 0.740447461605072\n",
      "[Iteration 1/44] Val loss: 0.863603949546814\n",
      "[Iteration 1/177] TRAIN loss: 0.7079999446868896\n",
      "[Iteration 101/177] TRAIN loss: 0.8063359260559082\n",
      "[EPOCH 21/100] TRAIN mean acc/loss: 0.6292837262153625/0.6592596173286438\n",
      "[Iteration 1/44] Val loss: 0.6899713277816772\n",
      "[EPOCH 21/100] VAL mean acc/loss: 0.6434780955314636/0.6566253304481506\n",
      "[Iteration 1/177] TRAIN loss: 0.5679469108581543\n",
      "[Iteration 101/177] TRAIN loss: 0.6826224327087402\n",
      "[Iteration 1/44] Val loss: 0.41516780853271484\n",
      "[Iteration 1/177] TRAIN loss: 0.6544632315635681\n",
      "[Iteration 101/177] TRAIN loss: 0.6106783151626587\n",
      "[Iteration 1/44] Val loss: 0.8241540193557739\n",
      "[Iteration 1/177] TRAIN loss: 0.5867221355438232\n",
      "[Iteration 101/177] TRAIN loss: 0.760518491268158\n",
      "[Iteration 1/44] Val loss: 0.6524873971939087\n",
      "[Iteration 1/177] TRAIN loss: 0.5737990736961365\n",
      "[Iteration 101/177] TRAIN loss: 0.4485340118408203\n",
      "[Iteration 1/44] Val loss: 0.7293311953544617\n",
      "[Iteration 1/177] TRAIN loss: 0.5655725598335266\n",
      "[Iteration 101/177] TRAIN loss: 0.6592167615890503\n",
      "[Iteration 1/44] Val loss: 0.540765643119812\n",
      "[Iteration 1/177] TRAIN loss: 0.6668000817298889\n",
      "[Iteration 101/177] TRAIN loss: 0.8125565052032471\n",
      "[Iteration 1/44] Val loss: 0.43746885657310486\n",
      "[Iteration 1/177] TRAIN loss: 0.735996663570404\n",
      "[Iteration 101/177] TRAIN loss: 0.7720867395401001\n",
      "[Iteration 1/44] Val loss: 0.4320518970489502\n",
      "[Iteration 1/177] TRAIN loss: 0.5773035883903503\n",
      "[Iteration 101/177] TRAIN loss: 0.5662504434585571\n",
      "[Iteration 1/44] Val loss: 0.5373070240020752\n",
      "[Iteration 1/177] TRAIN loss: 0.4755645990371704\n",
      "[Iteration 101/177] TRAIN loss: 0.7324846386909485\n",
      "[Iteration 1/44] Val loss: 0.6326214671134949\n",
      "[Iteration 1/177] TRAIN loss: 0.8042852878570557\n",
      "[Iteration 101/177] TRAIN loss: 0.59493088722229\n",
      "[EPOCH 31/100] TRAIN mean acc/loss: 0.6379919052124023/0.6248465776443481\n",
      "[Iteration 1/44] Val loss: 0.42623579502105713\n",
      "[EPOCH 31/100] VAL mean acc/loss: 0.6529937386512756/0.6130557060241699\n",
      "[Iteration 1/177] TRAIN loss: 0.43921107053756714\n",
      "[Iteration 101/177] TRAIN loss: 0.4464185833930969\n",
      "[Iteration 1/44] Val loss: 0.6889300346374512\n",
      "[Iteration 1/177] TRAIN loss: 0.44006091356277466\n",
      "[Iteration 101/177] TRAIN loss: 0.6106125712394714\n",
      "[Iteration 1/44] Val loss: 0.5754712820053101\n",
      "[Iteration 1/177] TRAIN loss: 0.5715558528900146\n",
      "[Iteration 101/177] TRAIN loss: 0.698167085647583\n",
      "[Iteration 1/44] Val loss: 0.5319225788116455\n",
      "[Iteration 1/177] TRAIN loss: 0.48067936301231384\n",
      "[Iteration 101/177] TRAIN loss: 0.5238505601882935\n",
      "[Iteration 1/44] Val loss: 0.5062006711959839\n",
      "[Iteration 1/177] TRAIN loss: 0.6464483141899109\n",
      "[Iteration 101/177] TRAIN loss: 0.6135676503181458\n",
      "[Iteration 1/44] Val loss: 0.5689061284065247\n",
      "[Iteration 1/177] TRAIN loss: 0.6875313520431519\n",
      "[Iteration 101/177] TRAIN loss: 0.5955413579940796\n",
      "[Iteration 1/44] Val loss: 0.5913142561912537\n",
      "[Iteration 1/177] TRAIN loss: 0.7677627205848694\n",
      "[Iteration 101/177] TRAIN loss: 0.5461950302124023\n",
      "[Iteration 1/44] Val loss: 0.6119934916496277\n",
      "[Iteration 1/177] TRAIN loss: 0.6504192352294922\n",
      "[Iteration 101/177] TRAIN loss: 0.5168182849884033\n",
      "[Iteration 1/44] Val loss: 0.7185484170913696\n",
      "[Iteration 1/177] TRAIN loss: 0.5278461575508118\n",
      "[Iteration 101/177] TRAIN loss: 0.7286016941070557\n",
      "[Iteration 1/44] Val loss: 0.7936531901359558\n",
      "[Iteration 1/177] TRAIN loss: 0.6323379278182983\n",
      "[Iteration 101/177] TRAIN loss: 0.5478519201278687\n",
      "[EPOCH 41/100] TRAIN mean acc/loss: 0.6421483755111694/0.6061772704124451\n",
      "[Iteration 1/44] Val loss: 0.7242545485496521\n",
      "[EPOCH 41/100] VAL mean acc/loss: 0.6631571650505066/0.6055887341499329\n",
      "[Iteration 1/177] TRAIN loss: 0.5651739239692688\n",
      "[Iteration 101/177] TRAIN loss: 0.4983755648136139\n",
      "[Iteration 1/44] Val loss: 0.7082767486572266\n",
      "[Iteration 1/177] TRAIN loss: 0.7793768644332886\n",
      "[Iteration 101/177] TRAIN loss: 0.6087964773178101\n",
      "[Iteration 1/44] Val loss: 0.7826114892959595\n",
      "[Iteration 1/177] TRAIN loss: 0.5953865051269531\n",
      "[Iteration 101/177] TRAIN loss: 0.4865944981575012\n",
      "[Iteration 1/44] Val loss: 0.5421289801597595\n",
      "[Iteration 1/177] TRAIN loss: 0.44607120752334595\n",
      "[Iteration 101/177] TRAIN loss: 0.5021701455116272\n"
     ]
    }
   ],
   "source": [
    "# Start training\n",
    "\n",
    "num_epochs=100\n",
    "log_nth_iter=100\n",
    "log_nth_epoch=10\n",
    "tqdm_mode='total'\n",
    "'''\n",
    "tqdm_mode:\n",
    "    'total': tqdm log how long all epochs will take,\n",
    "    'epoch': tqdm for each epoch how long it will take,\n",
    "    anything else, e.g. None: do not use tqdm\n",
    "'''\n",
    "\n",
    "# TODO: Add parameters to extra_args dict?\n",
    "if train_with_discriminator:\n",
    "    steps = 1 # how many steps of training for discriminator/generator before switching to generator/discriminator\n",
    "    solver.train(model,\n",
    "                 train_loader, \n",
    "                 validation_loader,\n",
    "                 num_epochs=num_epochs,\n",
    "                 log_nth_iter=log_nth_iter,\n",
    "                 log_nth_epoch=log_nth_epoch,\n",
    "                 tqdm_mode=tqdm_mode,\n",
    "                 steps=steps)\n",
    "else:\n",
    "    solver.train(model,\n",
    "                 train_loader,\n",
    "                 validation_loader,\n",
    "                 num_epochs=num_epochs,\n",
    "                 log_nth_iter=log_nth_iter,\n",
    "                 log_nth_epoch=log_nth_epoch,\n",
    "                 tqdm_mode=tqdm_mode,\n",
    "                 verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tVWNikT4PvGj"
   },
   "outputs": [],
   "source": [
    "# To download tensorboard runs from Colab\n",
    "\n",
    "# TODO: Make sure that only new ones are copied --> for tensorboard runs on colab, do not use git repository as \"runs\" directory?\n",
    "# TODO: Instead of downloading, directly move it to the git repository that is currently checked out and push changes?\n",
    "if is_on_colab:\n",
    "  from google.colab import files\n",
    "  !zip -r /content/runs.zip /content/runs\n",
    "  files.download(\"/content/runs.zip\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "P2VDewrSvags"
   },
   "source": [
    "# Test\n",
    "\n",
    "Test with test dataset.\n",
    "Will load the data and start the training.\n",
    "\n",
    "Visualizations can be seen in Tensorboard above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "d8S9-1x0zbHZ"
   },
   "outputs": [],
   "source": [
    "# Load test data\n",
    "# TODO: Find real test split, for now we load the SAME dataset as for train/val (just that this notebook is complete...)\n",
    "# !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "\n",
    "test_path = path # CHANGE HERE TO REAL PATH TO TEST SET\n",
    "\n",
    "test_dataset = dataset = ICLNUIMDataset(test_path, transform=transform) # TODO also use rest of parameters...\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=dataset_args[\"batch_size\"], \n",
    "                                               shuffle=True,\n",
    "                                               num_workers=4)\n",
    "\n",
    "print(\"Length of test set: {}\".format(len(test_dataset)))\n",
    "print(\"Loaded test set: {}\".format(test_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cl2sFO4Kynp6"
   },
   "outputs": [],
   "source": [
    "# Start testing\n",
    "\n",
    "#solver.test(model, test_loader, test_prefix=\"DUMMY_TEST_WITH_NO_REAL_TEST_SET\", log_nth=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pTLUJGpnBpud"
   },
   "source": [
    "# Save the model\n",
    "\n",
    "Save network with its weights to disk.\n",
    "\n",
    "See torch.save function: https://pytorch.org/docs/stable/notes/serialization.html#recommend-saving-models \n",
    "\n",
    "Load again with `the_model = TheModelClass(*args, **kwargs) the_model.load_state_dict(torch.load(PATH))`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7_zjMVB7Bpue"
   },
   "outputs": [],
   "source": [
    "def save_model(modelname, model):\n",
    "    # Might need to make sure, that the correct saved_results directory is chosen here.\n",
    "    filepath = \"../saved_models/\" + modelname + \".pt\"\n",
    "    torch.save(model.state_dict(), filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2JfoL3IHBpuv"
   },
   "outputs": [],
   "source": [
    "nvs_modelname = \"nvs_\" + id_suffix\n",
    "save_model(nvs_modelname, model)\n",
    "\n",
    "if train_with_discriminator:\n",
    "    # Also save the discriminator - currently this can only be accessed through the solver (change it!)\n",
    "    gan_modelname = \"gan_\" + id_suffix\n",
    "    save_model(gan_modelname, solver.netD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BwbC3OFOEmLX"
   },
   "outputs": [],
   "source": [
    "# LOAD MODEL AGAIN for verification purposes\n",
    "# Should print: <All keys matched successfully> per each model if it works\n",
    "\n",
    "nvs_filepath = \"../saved_models/\" + nvs_modelname + \".pt\"\n",
    "print(\"NVS_Model loading: \", model.load_state_dict(torch.load(nvs_filepath)))\n",
    "\n",
    "if train_with_discriminator:\n",
    "    gan_filepath = \"../saved_models/\" + gan_modelname + \".pt\"\n",
    "    print(\"Discriminator loading: \", solver.netD.load_state_dict(torch.load(gan_filepath)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "entire_network_notebook.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "nvs",
   "language": "python",
   "name": "nvs"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
