{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4hmm4vcJti8k"
   },
   "source": [
    "# Basic Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PEG28Bp3aIAs"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is on colab:  False\n",
      "Is on zerus: False\n"
     ]
    }
   ],
   "source": [
    "# Check device running the notebook automatically\n",
    "import sys\n",
    "is_on_colab = 'google.colab' in sys.modules\n",
    "is_on_zerus = 'teampc' in sys.argv[0]\n",
    "print(\"Is on colab: \", is_on_colab)\n",
    "print(\"Is on zerus:\", is_on_zerus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yVHnBLJjvr-N"
   },
   "source": [
    "## Setup for Colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "B1BOhxh7BEYm"
   },
   "outputs": [],
   "source": [
    "if is_on_colab:\n",
    "    # Google Colab setup\n",
    "    \n",
    "    # Mount drive\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive', force_remount=True)\n",
    "\n",
    "    # Retrieve repository and cd into root folder\n",
    "    from getpass import getpass\n",
    "    import urllib\n",
    "    import os\n",
    "    os.chdir(\"/content\")\n",
    "    user = input('Github user name: ')\n",
    "    password = getpass('Github password: ')\n",
    "    password = urllib.parse.quote(password) # your password is converted into url format\n",
    "    branch = \"\" # \"-b \" + \"branch_name\"\n",
    "    cmd_string = 'git clone {0} https://{1}:{2}@github.com/lukasHoel/novel-view-synthesis.git'.format(branch, user, password)\n",
    "    os.system(cmd_string)\n",
    "    os.chdir(\"novel-view-synthesis\")\n",
    "\n",
    "    # Install PyTorch3D libraries (required for pointcloud computations.)\n",
    "    !pip install 'git+https://github.com/facebookresearch/pytorch3d.git'\n",
    "    !pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VF0QazUmv5lY"
   },
   "source": [
    "## Setup for Local Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VTsC8dapAyAD"
   },
   "outputs": [],
   "source": [
    "# ONLY NECESSARY FOR LOCAL EXECUTION (WORKS WITHOUT THIS CELL IN GOOGLE COLAB)\n",
    "# Setup that is necessary for jupyter notebook to find sibling-directories\n",
    "# see: https://stackoverflow.com/questions/34478398/import-local-function-from-a-module-housed-in-another-directory-with-relative-im\n",
    "\n",
    "\n",
    "if not is_on_colab:\n",
    "    \n",
    "    import os\n",
    "    import sys\n",
    "    module_path = os.path.abspath(os.path.join('..'))\n",
    "    if module_path not in sys.path:\n",
    "        sys.path.append(module_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Gd7dyzCGwCZo"
   },
   "source": [
    "## General Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EGfclJp_AyA7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# Imports for this notebook\n",
    "from models.nvs_model import NovelViewSynthesisModel\n",
    "from models.synthesis.synt_loss_metric import SynthesisLoss\n",
    "from util.nvs_solver import NVS_Solver\n",
    "from util.gan_wrapper_solver import GAN_Wrapper_Solver\n",
    "from data.nuim_dataloader import ICLNUIMDataset\n",
    "from data.nuim_dynamics_dataloader import ICLNUIM_Dynamic_Dataset\n",
    "from data.mp3d_dataloader import MP3D_Habitat_Offline_Dataset\n",
    "from projection.z_buffer_manipulator import PtsManipulator\n",
    "\n",
    "from torch.utils import data\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "import torchvision.transforms\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "B7R9GHGlhsMQ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training is on GPU with CUDA: True\n",
      "Device: cuda:0\n",
      "Thu Jun 25 20:03:12 2020       \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| NVIDIA-SMI 418.87.01    Driver Version: 418.87.01    CUDA Version: 10.1     |\r\n",
      "|-------------------------------+----------------------+----------------------+\r\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n",
      "|===============================+======================+======================|\r\n",
      "|   0  GeForce GTX 970     On   | 00000000:01:00.0 Off |                  N/A |\r\n",
      "|  0%   40C    P8    11W / 180W |     31MiB /  4041MiB |      0%      Default |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "                                                                               \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| Processes:                                                       GPU Memory |\r\n",
      "|  GPU       PID   Type   Process name                             Usage      |\r\n",
      "|=============================================================================|\r\n",
      "|    0      1600      G   /usr/lib/xorg/Xorg                             9MiB |\r\n",
      "|    0      1642      G   /usr/bin/gnome-shell                           8MiB |\r\n",
      "+-----------------------------------------------------------------------------+\r\n"
     ]
    }
   ],
   "source": [
    "# Check training on GPU?\n",
    "\n",
    "cuda = torch.cuda.is_available()\n",
    "\n",
    "print(\"Training is on GPU with CUDA: {}\".format(cuda))\n",
    "\n",
    "device = \"cuda:0\" if cuda else \"cpu\"\n",
    "\n",
    "print(\"Device: {}\".format(device))\n",
    "\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "u7mIvRRwJGlR"
   },
   "outputs": [],
   "source": [
    "def count_parameters(model):\n",
    "    \"\"\"Given a model return total number of parameters\"\"\"\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QkO15Cr3t3pA"
   },
   "source": [
    "# Load Data\n",
    "Load ICL-NUIM dataset or Matterport3D dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lQOjeo-oTNny"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using dataset: icl_dynamic\n",
      "Image Size: 128\n"
     ]
    }
   ],
   "source": [
    "#dataset_mode = PtsManipulator.matterport_mode\n",
    "#dataset_mode = PtsManipulator.icl_nuim_mode \n",
    "dataset_mode = PtsManipulator.icl_nuim_dynamic_mode\n",
    "\n",
    "image_size = 128\n",
    "\n",
    "print(\"Using dataset: \" + dataset_mode)\n",
    "print(\"Image Size: \" + str(image_size))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TcKlv4vsAyBE"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded following data: /home/lukas/Desktop/datasets/ICL-NUIM/custom/seq0001 (samples: 64) with configuration: {'mode': 'icl_dynamic', 'image_size': 128, 'path': '/home/lukas/Desktop/datasets/ICL-NUIM/custom/seq0001', 'sampleOutput': True, 'inverse_depth': False, 'cacheItems': False, 'icl_nuim_output_size': 128, 'icl_dynamic_output_from_other_view': False}\n"
     ]
    }
   ],
   "source": [
    "# Load dataset from drive or local\n",
    "\n",
    "if is_on_colab:\n",
    "    if dataset_mode == PtsManipulator.matterport_mode:\n",
    "        path = \"/content/drive/My Drive/Novel_View_Synthesis/matterport3d\"\n",
    "    elif dataset_mode == PtsManipulator.icl_nuim_mode:\n",
    "        path = \"/content/drive/My Drive/Novel_View_Synthesis/ICL-NUIM/living_room_traj2_loop\"\n",
    "        \n",
    "elif is_on_zerus:\n",
    "    if dataset_mode == PtsManipulator.matterport_mode:\n",
    "        raise ValueError(\"Path to mp3d on zerus not specified in this notebook!\")\n",
    "    elif dataset_mode == PtsManipulator.icl_nuim_mode:\n",
    "        path = \"/mnt/raid/teampc/ICL-NUIM/living_room_traj2_loop\"\n",
    "        \n",
    "else:\n",
    "    if dataset_mode == PtsManipulator.matterport_mode:\n",
    "        path = \"/home/lukas/Desktop/git/synsin/dataset\"\n",
    "    elif dataset_mode == PtsManipulator.icl_nuim_mode:\n",
    "        path = \"/home/lukas/Desktop/datasets/ICL-NUIM/prerendered_data/living_room_traj2_loop\"\n",
    "    elif dataset_mode == PtsManipulator.icl_nuim_dynamic_mode:\n",
    "        path = \"/home/lukas/Desktop/datasets/ICL-NUIM/custom/seq0001\"\n",
    "\n",
    "transform = torchvision.transforms.Compose([\n",
    "    #torchvision.transforms.ToPILImage(), # no longer needed: new dataloader now returns PIL Images\n",
    "    torchvision.transforms.Resize((image_size, image_size)),\n",
    "    torchvision.transforms.ToTensor()\n",
    "])\n",
    "    \n",
    "data_dict = {\n",
    "    \"mode\": dataset_mode,\n",
    "    \"image_size\": image_size,\n",
    "    \"path\": path,\n",
    "    \"sampleOutput\": True,\n",
    "    \"inverse_depth\": False,\n",
    "    \"cacheItems\": False, # Caching will work only if num_workers = 0. Decide what you like more!\n",
    "}\n",
    "    \n",
    "if dataset_mode == PtsManipulator.matterport_mode:\n",
    "    \n",
    "    # THIS IS THE HARDCODED IMAGE SIZE THAT WE SET IN THE HABITAT FRAMEWORK WHEN RENDERING MP3D IMAGES\n",
    "    # THIS DOES NOT CHANGE WHEN WE USE DIFFERENT IMAGE SIZES IN A TRANSFORM OBJECT\n",
    "    # WHEN CHANGING THE IMAGE SIZE IN TRANSFORM OBJECT, THIS GETS REFLECTED IN THE image_size ATTRIBUTE\n",
    "    data_dict['mp3d_image_input_size'] = 256\n",
    "    \n",
    "    data_dict['train_path'] = path + \"/train\"\n",
    "    data_dict['val_path'] = path + \"/val\"\n",
    "\n",
    "    train_dataset = MP3D_Habitat_Offline_Dataset(data_dict['train_path'],\n",
    "                                        in_size=data_dict['mp3d_image_input_size'],\n",
    "                                        transform=transform,\n",
    "                                        sampleOutput=data_dict[\"sampleOutput\"],\n",
    "                                        inverse_depth=data_dict[\"inverse_depth\"],\n",
    "                                        cacheItems=data_dict[\"cacheItems\"])\n",
    "    \n",
    "    print(\"Loaded following data: {} (samples: {}) with configuration: {}\\n\".format(data_dict[\"train_path\"], len(train_dataset), data_dict))\n",
    "    \n",
    "    val_dataset = MP3D_Habitat_Offline_Dataset(data_dict['val_path'],\n",
    "                                        in_size=data_dict['mp3d_image_input_size'],\n",
    "                                        transform=transform,\n",
    "                                        sampleOutput=data_dict[\"sampleOutput\"],\n",
    "                                        inverse_depth=data_dict[\"inverse_depth\"],\n",
    "                                        cacheItems=data_dict[\"cacheItems\"])\n",
    "    \n",
    "    print(\"Loaded following data: {} (samples: {}) with configuration: {}\\n\".format(data_dict[\"val_path\"], len(val_dataset), data_dict))\n",
    "        \n",
    "elif dataset_mode == PtsManipulator.icl_nuim_mode:\n",
    "\n",
    "    data_dict['icl_nuim_output_size'] = image_size\n",
    "    data_dict['path'] = path\n",
    "    \n",
    "    dataset = ICLNUIMDataset(data_dict['path'],\n",
    "                             transform=transform,\n",
    "                             sampleOutput=data_dict[\"sampleOutput\"],\n",
    "                             inverse_depth=data_dict[\"inverse_depth\"],\n",
    "                             cacheItems=data_dict[\"cacheItems\"], \n",
    "                             out_shape=(image_size, image_size))\n",
    "\n",
    "    print(\"Loaded following data: {} (samples: {}) with configuration: {}\".format(data_dict[\"path\"], len(dataset), data_dict))\n",
    "    \n",
    "elif dataset_mode == PtsManipulator.icl_nuim_dynamic_mode:\n",
    "    \n",
    "    data_dict['icl_nuim_output_size'] = image_size\n",
    "    data_dict['path'] = path\n",
    "    data_dict['icl_dynamic_output_from_other_view'] = False\n",
    "    \n",
    "    dataset = ICLNUIM_Dynamic_Dataset(data_dict['path'],\n",
    "                             sampleOutput=True,\n",
    "                             output_from_other_view=data_dict['icl_dynamic_output_from_other_view'], \n",
    "                             inverse_depth=False,\n",
    "                             cacheItems=False,\n",
    "                             transform=transform)\n",
    "    \n",
    "    print(\"Loaded following data: {} (samples: {}) with configuration: {}\".format(data_dict[\"path\"], len(dataset), data_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aJLyrjl2AyBK"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset parameters: {'batch_size': 8, 'num_workers': 1, 'random_seed': 42, 'shuffle_dataset': True, 'mode': 'icl_dynamic', 'image_size': 128, 'path': '/home/lukas/Desktop/datasets/ICL-NUIM/custom/seq0001', 'sampleOutput': True, 'inverse_depth': False, 'cacheItems': False, 'icl_nuim_output_size': 128, 'icl_dynamic_output_from_other_view': False, 'validation_percentage': 0.2, 'train_len': 7, 'val_len': 2}\n"
     ]
    }
   ],
   "source": [
    "dataset_args = {\n",
    "    \"batch_size\": 8,\n",
    "    \"num_workers\": 1, # Dataset Caching will work only if num_workers = 0. Decide what you like more!\n",
    "    \"random_seed\": 42, # seed random generation for shuffeling indices to always get same images in train/val\n",
    "    \"shuffle_dataset\": True,\n",
    "    **data_dict\n",
    "}\n",
    "\n",
    "if dataset_mode == PtsManipulator.matterport_mode:\n",
    "    # For mp3d we have separate train/val folders so we can just create different loaders out of the different datasets\n",
    "\n",
    "    train_len = len(train_dataset)\n",
    "    train_sampler = SubsetRandomSampler(list(range(train_len)))\n",
    "\n",
    "    val_len = len(val_dataset)\n",
    "    val_sampler = SubsetRandomSampler(list(range(val_len)))\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(train_dataset,\n",
    "                                               batch_size=dataset_args[\"batch_size\"], \n",
    "                                               #shuffle=dataset_args[\"shuffle_dataset\"],\n",
    "                                               sampler=train_sampler,\n",
    "                                               num_workers=dataset_args[\"num_workers\"])\n",
    "    \n",
    "    validation_loader = torch.utils.data.DataLoader(val_dataset,\n",
    "                                            batch_size=dataset_args[\"batch_size\"], \n",
    "                                            #shuffle=dataset_args[\"shuffle_dataset\"],\n",
    "                                            sampler=val_sampler,\n",
    "                                            num_workers=dataset_args[\"num_workers\"])\n",
    "\n",
    "elif dataset_mode == PtsManipulator.icl_nuim_mode or dataset_mode == PtsManipulator.icl_nuim_dynamic_mode:\n",
    "    # Create Train and Val dataset with 80% train and 20% val.\n",
    "    # from: https://stackoverflow.com/questions/50544730/how-do-i-split-a-custom-dataset-into-training-and-test-datasets\n",
    "\n",
    "    # For ICL dataset we do not have train/val datasets so we split the existing dataset 80% to 20%\n",
    "    dataset_args[\"validation_percentage\"] = 0.2\n",
    "\n",
    "    # Creating data indices for training and validation splits:\n",
    "    dataset_size = len(dataset)\n",
    "    indices = list(range(dataset_size))\n",
    "    split = int(np.floor(dataset_args[\"validation_percentage\"] * dataset_size))\n",
    "    if dataset_args[\"shuffle_dataset\"]:\n",
    "        np.random.seed(dataset_args[\"random_seed\"])\n",
    "        np.random.shuffle(indices)\n",
    "    train_indices, val_indices = indices[split:], indices[:split]\n",
    "\n",
    "    # Creating PT data samplers and loaders:\n",
    "    train_sampler = SubsetRandomSampler(train_indices)\n",
    "    valid_sampler = SubsetRandomSampler(val_indices)\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(dataset,\n",
    "                                               batch_size=dataset_args[\"batch_size\"], \n",
    "                                               sampler=train_sampler,\n",
    "                                               num_workers=dataset_args[\"num_workers\"])\n",
    "    validation_loader = torch.utils.data.DataLoader(dataset,\n",
    "                                                    batch_size=dataset_args[\"batch_size\"],\n",
    "                                                    sampler=valid_sampler,\n",
    "                                                    num_workers=dataset_args[\"num_workers\"])\n",
    "\n",
    "    # ICL OVERFITTING CASE:\n",
    "    '''\n",
    "    train_indices = train_indices[:4] # train_indices[0:4] # [train_indices[0]]\n",
    "    val_indices = val_indices[:2]\n",
    "\n",
    "    overfit_item = dataset.__getitem__(train_indices[0])\n",
    "    print(\"OVERFITTING Input Image: {}, Output Image: {}\".format(\n",
    "        train_indices[0],\n",
    "        overfit_item[\"output\"][\"idx\"]))\n",
    "\n",
    "    input_img = overfit_item[\"image\"].cpu().detach().numpy()\n",
    "    output_img = overfit_item[\"output\"][\"image\"].cpu().detach().numpy()\n",
    "\n",
    "    print(torch.min(overfit_item[\"output\"][\"image\"]))\n",
    "    print(torch.max(overfit_item[\"output\"][\"image\"]))\n",
    "    print(overfit_item[\"cam\"])\n",
    "\n",
    "    %matplotlib inline\n",
    "\n",
    "    import matplotlib\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    print(\"OVERFIT TRAIN INPUT IMAGE\")\n",
    "    plt.imshow(np.moveaxis(input_img, 0, -1))\n",
    "    plt.show()\n",
    "\n",
    "    print(\"OVERFIT TRAIN OUTPUT IMAGE\")\n",
    "    plt.imshow(np.moveaxis(output_img, 0, -1))\n",
    "    plt.show()\n",
    "    '''\n",
    "    # END ICL OVERFITTING CASE\n",
    "\n",
    "dataset_args[\"train_len\"] = len(train_loader)\n",
    "dataset_args[\"val_len\"] = len(validation_loader)\n",
    "\n",
    "print(\"Dataset parameters: {}\".format(dataset_args))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qKgCXiSPZgY1"
   },
   "source": [
    "# Model & Loss Init\n",
    "\n",
    "Instantiate and initialize NovelViewSynthesisModel and a selected flavor of SynthesisLoss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hzBJYgAlZgY2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss names: ('l1', 'content')\n",
      "Weight of each loss: ('1.0', '10.0')\n",
      "Model configuration: {'imageSize': 128, 'use_gt_depth': True, 'normalize_images': False, 'use_rgb_features': False, 'num_depth_filters': 16, 'enc_dims': [3, 8, 8, 16, 16, 32, 32, 64], 'enc_blk_types': ['id', 'id', 'id', 'id', 'id', 'id', 'id'], 'enc_noisy_bn': False, 'enc_spectral_norm': True, 'dec_activation_func': Sigmoid(), 'dec_dims': [64, 32, 32, 16, 16, 8, 8, 3], 'dec_blk_types': ['id', 'id', 'id', 'id', 'id', 'id', 'id'], 'dec_noisy_bn': False, 'dec_spectral_norm': True, 'projection_mode': 'icl_dynamic', 'l1_loss': '1.0_l1', 'content_loss': '10.0_content', 'model': 'NovelViewSynthesisModel'}\n",
      "Total number of paramaters: 2619783\n",
      "Parameters ENCODER: 102726\n",
      "Parameters DEPTH: 2451233\n",
      "Parameters DECODER: 65760\n"
     ]
    }
   ],
   "source": [
    "# TODO: Define more parameters in the dict according to availalbe ones in the model, as soon as they are needed.\n",
    "# Right now we just use the default parameters for the rest (see outcommented list or the .py file)\n",
    "    \n",
    "model_args={\n",
    "    'imageSize': image_size, # change this now in the first dataloading cell from above!\n",
    "    \n",
    "    'use_gt_depth': True,\n",
    "    'normalize_images': False,\n",
    "    'use_rgb_features': False,\n",
    "\n",
    "    'num_depth_filters': 16,\n",
    "    \n",
    "    'enc_dims': [3, 8, 8, 16, 16, 32, 32, 64],\n",
    "    'enc_blk_types': [\"id\", \"id\", \"id\", \"id\", \"id\", \"id\", \"id\"],\n",
    "    #'enc_dims': [3, 8, 8, 16, 16, 32, 32, 64, 64, 64],\n",
    "    #'enc_blk_types': [\"id\", \"id\", \"id\", \"id\", \"id\", \"id\", \"id\", \"id\", \"id\"],\n",
    "    #'enc_dims': [3, 8, 8],\n",
    "    #'enc_blk_types': [\"id\", \"id\"],\n",
    "    'enc_noisy_bn': False,\n",
    "    'enc_spectral_norm': True,\n",
    "    \n",
    "    'dec_activation_func': nn.Sigmoid(),\n",
    "    #'dec_dims': [64, 64, 32, 32, 32, 16, 16, 8, 8, 3],\n",
    "    #'dec_blk_types': [\"id\", \"id\", \"id\", \"id\", \"id\", \"id\", \"id\", \"id\", \"id\"],\n",
    "    'dec_dims': [64, 32, 32, 16, 16, 8, 8, 3],\n",
    "    'dec_blk_types': [\"id\", \"id\", \"id\", \"id\", \"id\", \"id\", \"id\"],\n",
    "    #'dec_dims': [3, 8, 8, 16, 16, 32, 32, 64, 64, 32, 32, 16, 16, 8, 8, 3],\n",
    "    #'dec_blk_types': [\"id\", \"id\", \"id\", \"id\", \"id\", \"id\", \"id\", \"id\", \"id\", \"id\", \"id\", \"id\", \"id\", \"id\", \"id\"],\n",
    "    'dec_noisy_bn': False,\n",
    "    'dec_spectral_norm': True,\n",
    "                      \n",
    "    'projection_mode': dataset_mode,\n",
    "    \n",
    "    # from here attributes for the loss of the nvs_model\n",
    "    'l1_loss': '1.0_l1',\n",
    "    'content_loss': '10.0_content', # synsin default: 10.0\n",
    "}\n",
    "\n",
    "# keep this loss object constant and modify usage of losses by e.g. setting one coefficient to 0\n",
    "nvs_loss = SynthesisLoss(losses=[\n",
    "    model_args['l1_loss'],\n",
    "    model_args['content_loss']\n",
    "])\n",
    "\n",
    "model = NovelViewSynthesisModel(imageSize=model_args['imageSize'],\n",
    "                                \n",
    "                                max_z=10,\n",
    "                                min_z=0,\n",
    "                                num_filters=model_args['num_depth_filters'],\n",
    "                                \n",
    "                                enc_dims=model_args['enc_dims'],\n",
    "                                enc_blk_types=model_args['enc_blk_types'],\n",
    "                                enc_noisy_bn=model_args['enc_noisy_bn'],\n",
    "                                enc_spectral_norm=model_args['enc_spectral_norm'],\n",
    "                                \n",
    "                                dec_dims=model_args['dec_dims'],\n",
    "                                dec_blk_types=model_args['dec_blk_types'],\n",
    "                                dec_activation_func=model_args['dec_activation_func'],\n",
    "                                dec_noisy_bn=model_args['dec_noisy_bn'],\n",
    "                                dec_spectral_norm=model_args['dec_spectral_norm'],\n",
    "                                \n",
    "                                projection_mode=model_args['projection_mode'],\n",
    "                                #points_per_pixel=8,\n",
    "                                #learn_feature=True,\n",
    "                                #radius=3.0,\n",
    "                                #rad_pow=2,\n",
    "                                #accumulation='alphacomposite',\n",
    "                                #accumulation_tau=1,\n",
    "                                \n",
    "                                use_rgb_features=model_args['use_rgb_features'],\n",
    "                                use_gt_depth=model_args['use_gt_depth'],\n",
    "                                #use_inverse_depth=False,\n",
    "                                normalize_images=model_args['normalize_images'])\n",
    "model_args[\"model\"] = type(model).__name__\n",
    "\n",
    "print(\"Model configuration: {}\".format(model_args))\n",
    "\n",
    "#print(\"Architecture:\", model)\n",
    "print(\"Total number of paramaters:\", count_parameters(model))\n",
    "print(\"Parameters ENCODER:\", count_parameters(model.encoder))\n",
    "print(\"Parameters DEPTH:\", count_parameters(model.pts_regressor))\n",
    "print(\"Parameters DECODER:\", count_parameters(model.projector))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HeFz-j00u5LR"
   },
   "source": [
    "# Training Visualization\n",
    "\n",
    "Start Tensorboard for visualization of the upcoming training / validation / test steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "c-6Mnrc-TPU9",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Start tensorboard. Might need to make sure, that the correct runs directory is chosen here.\n",
    "%load_ext tensorboard\n",
    "%tensorboard --logdir \"../runs\"\n",
    "#!tensorboard --logdir ../runs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-zet9gPxvM2i"
   },
   "source": [
    "# Training\n",
    "\n",
    "Start training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mvDPM-hKJGlt"
   },
   "outputs": [],
   "source": [
    "# This flag decides with solver gets used and where the logs will be logged into (into which directory)\n",
    "train_with_discriminator = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OTnq2RYJy4Dn"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log_dir: ../runs/Full_No_GAN/2020-Jun-25_20-16-16_f5bd0076-b70f-11ea-9342-4bf2b94f1bae\n"
     ]
    }
   ],
   "source": [
    "# Create unique ID for this training process for saving to disk.\n",
    "\n",
    "from datetime import datetime\n",
    "import uuid\n",
    "now = datetime.now() # current date and time\n",
    "id = str(uuid.uuid1())\n",
    "id_suffix = now.strftime(\"%Y-%b-%d_%H-%M-%S\") + \"_\" + id\n",
    "\n",
    "if train_with_discriminator:\n",
    "    log_dir_name = \"Full_GAN\"\n",
    "else:\n",
    "    log_dir_name = \"Full_No_GAN\"\n",
    "\n",
    "log_dir = \"../runs/\" + log_dir_name + \"/\" + id_suffix # Might need to make sure, that the correct runs directory is chosen here.\n",
    "print(\"log_dir:\", log_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QY38vRjuAyBc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metric names: PSNR SSIM\n",
      "Hyperparameters of this solver: {'loss_function': 'SynthesisLoss', 'optimizer': 'Adam', 'learning_rate': 0.01, 'weight_decay': 0.0, 'imageSize': '128', 'use_gt_depth': 'True', 'normalize_images': 'False', 'use_rgb_features': 'False', 'num_depth_filters': '16', 'enc_dims': '[3, 8, 8, 16, 16, 32, 32, 64]', 'enc_blk_types': \"['id', 'id', 'id', 'id', 'id', 'id', 'id']\", 'enc_noisy_bn': 'False', 'enc_spectral_norm': 'True', 'dec_activation_func': 'Sigmoid()', 'dec_dims': '[64, 32, 32, 16, 16, 8, 8, 3]', 'dec_blk_types': \"['id', 'id', 'id', 'id', 'id', 'id', 'id']\", 'dec_noisy_bn': 'False', 'dec_spectral_norm': 'True', 'projection_mode': 'icl_dynamic', 'l1_loss': '1.0_l1', 'content_loss': '10.0_content', 'model': 'NovelViewSynthesisModel', 'batch_size': '8', 'num_workers': '1', 'random_seed': '42', 'shuffle_dataset': 'True', 'mode': 'icl_dynamic', 'image_size': '128', 'path': '/home/lukas/Desktop/datasets/ICL-NUIM/custom/seq0001', 'sampleOutput': 'True', 'inverse_depth': 'False', 'cacheItems': 'False', 'icl_nuim_output_size': '128', 'icl_dynamic_output_from_other_view': 'False', 'validation_percentage': '0.2', 'train_len': '7', 'val_len': '2', 'num_D': '3', 'size_D': '64', 'loss_D': 'original', 'no_feature_loss': 'False', 'init_weights': 'True', 'lr_step': '10', 'lr_gamma': '0.3'}\n"
     ]
    }
   ],
   "source": [
    "# Configure solver\n",
    "extra_args = {\n",
    "    **model_args,\n",
    "    **dataset_args,\n",
    "    'num_D': 3, # number of discriminators, each downsamples by 2\n",
    "    'size_D': 64, # number of channels each conv in the discriminator has\n",
    "    'loss_D': 'original', # discriminator loss, options are original(cross-entropy), ls (MSE), hinge, w\n",
    "    'no_feature_loss': False, # if discriminator should not use feature loss\n",
    "    'init_weights': True,\n",
    "    'lr_step': 10, #number of epochs after which the learning rate is mulitplied with gamma\n",
    "    'lr_gamma': 0.3\n",
    "}\n",
    "\n",
    "if train_with_discriminator:\n",
    "    solver = GAN_Wrapper_Solver(optim_d=torch.optim.Adam,\n",
    "                                optim_d_args={\"lr\": 1e-3,\n",
    "                                              \"betas\": (0.9, 0.999),\n",
    "                                              \"eps\": 1e-8,\n",
    "                                              \"weight_decay\": 0.0},# is the l2 regularization parameter, see: https://pytorch.org/docs/stable/optim.html\n",
    "                                optim_g=torch.optim.Adam,\n",
    "                                optim_g_args={\"lr\": 1e-5,\n",
    "                                              \"betas\": (0.9, 0.999),\n",
    "                                              \"eps\": 1e-8,\n",
    "                                              \"weight_decay\": 0.0}, # is the l2 regularization parameter, see: https://pytorch.org/docs/stable/optim.html\n",
    "                                g_loss_func=nvs_loss,\n",
    "                                extra_args=extra_args,\n",
    "                                log_dir=log_dir,\n",
    "                                num_D=extra_args['num_D'],\n",
    "                                size_D=extra_args['size_D'],\n",
    "                                loss_D=extra_args['loss_D'],\n",
    "                                no_gan_feature_loss=extra_args['no_feature_loss'],\n",
    "                                init_discriminator_weights=extra_args['init_weights'],\n",
    "                                lr_step=extra_args['lr_step'],\n",
    "                                lr_gamma=extra_args['lr_gamma'])\n",
    "else:\n",
    "    solver = NVS_Solver(optim=torch.optim.Adam,\n",
    "                        optim_args={\"lr\": 1e-2,\n",
    "                                    \"betas\": (0.9, 0.999),\n",
    "                                    \"eps\": 1e-8,\n",
    "                                    \"weight_decay\": 0.0}, # is the l2 regularization parameter, see: https://pytorch.org/docs/stable/optim.html,\n",
    "                        loss_func=nvs_loss,\n",
    "                        extra_args=extra_args,\n",
    "                        tensorboard_writer=None, # let solver create a new instance\n",
    "                        log_dir=log_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1IhNe6ynzXox"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "START TRAIN on device: cuda:0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "91809ff665f448948879cfa954e4eaef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[EPOCH 1/100] TRAIN mean acc/loss: 0.3116759657859802/9.724814414978027\n",
      "[EPOCH 1/100] VAL mean acc/loss: 0.34919318556785583/9.313089370727539\n",
      "[EPOCH 2/100] TRAIN mean acc/loss: 0.3570016920566559/8.732717514038086\n",
      "[EPOCH 2/100] VAL mean acc/loss: 0.35035020112991333/8.958216667175293\n",
      "[EPOCH 3/100] TRAIN mean acc/loss: 0.3685509264469147/8.320528030395508\n",
      "[EPOCH 3/100] VAL mean acc/loss: 0.4068100154399872/8.283292770385742\n",
      "[EPOCH 4/100] TRAIN mean acc/loss: 0.4041394889354706/7.887763500213623\n",
      "[EPOCH 4/100] VAL mean acc/loss: 0.3992113471031189/8.332878112792969\n",
      "[EPOCH 5/100] TRAIN mean acc/loss: 0.40922942757606506/7.755444049835205\n",
      "[EPOCH 5/100] VAL mean acc/loss: 0.32720276713371277/9.130121231079102\n",
      "[EPOCH 6/100] TRAIN mean acc/loss: 0.41893938183784485/7.715611934661865\n",
      "[EPOCH 6/100] VAL mean acc/loss: 0.3125711679458618/9.073844909667969\n",
      "[EPOCH 7/100] TRAIN mean acc/loss: 0.42992228269577026/7.585361003875732\n",
      "[EPOCH 7/100] VAL mean acc/loss: 0.42030590772628784/7.681224822998047\n",
      "[EPOCH 8/100] TRAIN mean acc/loss: 0.45347389578819275/7.240264892578125\n",
      "[EPOCH 8/100] VAL mean acc/loss: 0.46208134293556213/7.8530168533325195\n",
      "[EPOCH 9/100] TRAIN mean acc/loss: 0.46168285608291626/7.079098224639893\n",
      "[EPOCH 9/100] VAL mean acc/loss: 0.45847243070602417/6.981966972351074\n",
      "[EPOCH 10/100] TRAIN mean acc/loss: 0.4661831557750702/7.045088768005371\n",
      "[EPOCH 10/100] VAL mean acc/loss: 0.4812564253807068/7.025161266326904\n",
      "[EPOCH 11/100] TRAIN mean acc/loss: 0.48131677508354187/6.83521032333374\n",
      "[EPOCH 11/100] VAL mean acc/loss: 0.4170404374599457/7.172476768493652\n",
      "[EPOCH 12/100] TRAIN mean acc/loss: 0.45305198431015015/6.790186405181885\n",
      "[EPOCH 12/100] VAL mean acc/loss: 0.45187944173812866/6.782267093658447\n",
      "[EPOCH 13/100] TRAIN mean acc/loss: 0.4593827426433563/6.626471042633057\n",
      "[EPOCH 13/100] VAL mean acc/loss: 0.46005958318710327/6.811371803283691\n",
      "[EPOCH 14/100] TRAIN mean acc/loss: 0.47989875078201294/6.549126148223877\n",
      "[EPOCH 14/100] VAL mean acc/loss: 0.4851805567741394/6.515995979309082\n",
      "[EPOCH 15/100] TRAIN mean acc/loss: 0.4870058596134186/6.47556209564209\n",
      "[EPOCH 15/100] VAL mean acc/loss: 0.5051478743553162/6.52634859085083\n",
      "[EPOCH 16/100] TRAIN mean acc/loss: 0.4943530559539795/6.540119171142578\n",
      "[EPOCH 16/100] VAL mean acc/loss: 0.49718910455703735/7.203609466552734\n",
      "[EPOCH 17/100] TRAIN mean acc/loss: 0.5044272541999817/6.3854522705078125\n",
      "[EPOCH 17/100] VAL mean acc/loss: 0.5170236229896545/6.938211441040039\n",
      "[EPOCH 18/100] TRAIN mean acc/loss: 0.5250702500343323/6.106894016265869\n",
      "[EPOCH 18/100] VAL mean acc/loss: 0.5266275405883789/6.193704605102539\n",
      "[EPOCH 19/100] TRAIN mean acc/loss: 0.527938723564148/5.97313928604126\n",
      "[EPOCH 19/100] VAL mean acc/loss: 0.5239404439926147/6.158162593841553\n",
      "[EPOCH 20/100] TRAIN mean acc/loss: 0.5276911854743958/5.984499931335449\n",
      "[EPOCH 20/100] VAL mean acc/loss: 0.5298823714256287/6.138594627380371\n",
      "[EPOCH 21/100] TRAIN mean acc/loss: 0.5266322493553162/6.081474781036377\n",
      "[EPOCH 21/100] VAL mean acc/loss: 0.4818616509437561/6.952523231506348\n",
      "[EPOCH 22/100] TRAIN mean acc/loss: 0.5061503052711487/6.105010986328125\n",
      "[EPOCH 22/100] VAL mean acc/loss: 0.4878543019294739/6.426150798797607\n",
      "[EPOCH 23/100] TRAIN mean acc/loss: 0.5065441727638245/6.151162147521973\n",
      "[EPOCH 23/100] VAL mean acc/loss: 0.5258634090423584/6.282382011413574\n",
      "[EPOCH 24/100] TRAIN mean acc/loss: 0.5350057482719421/5.9653449058532715\n",
      "[EPOCH 24/100] VAL mean acc/loss: 0.5339779853820801/5.889216899871826\n",
      "[EPOCH 25/100] TRAIN mean acc/loss: 0.5375687479972839/5.770169734954834\n",
      "[EPOCH 25/100] VAL mean acc/loss: 0.5281615257263184/5.92172908782959\n",
      "[EPOCH 26/100] TRAIN mean acc/loss: 0.5234683156013489/5.761328220367432\n",
      "[EPOCH 26/100] VAL mean acc/loss: 0.5065605640411377/5.980801582336426\n",
      "[EPOCH 27/100] TRAIN mean acc/loss: 0.5214415192604065/5.833610534667969\n",
      "[EPOCH 27/100] VAL mean acc/loss: 0.519025981426239/5.885520935058594\n",
      "[EPOCH 28/100] TRAIN mean acc/loss: 0.5204237699508667/5.842263221740723\n",
      "[EPOCH 28/100] VAL mean acc/loss: 0.5175297260284424/6.120121955871582\n",
      "[EPOCH 29/100] TRAIN mean acc/loss: 0.5111871361732483/5.966477870941162\n",
      "[EPOCH 29/100] VAL mean acc/loss: 0.566813588142395/5.942966461181641\n",
      "[EPOCH 30/100] TRAIN mean acc/loss: 0.5290842652320862/5.744772911071777\n",
      "[EPOCH 30/100] VAL mean acc/loss: 0.528747022151947/5.745104789733887\n",
      "[EPOCH 31/100] TRAIN mean acc/loss: 0.5317336916923523/5.74325704574585\n",
      "[EPOCH 31/100] VAL mean acc/loss: 0.5377836227416992/5.774926662445068\n",
      "[EPOCH 32/100] TRAIN mean acc/loss: 0.5351220965385437/5.666633605957031\n",
      "[EPOCH 32/100] VAL mean acc/loss: 0.5477144122123718/5.613041877746582\n",
      "[EPOCH 33/100] TRAIN mean acc/loss: 0.5407177209854126/5.526839256286621\n",
      "[EPOCH 33/100] VAL mean acc/loss: 0.5330926179885864/5.658304214477539\n",
      "[EPOCH 34/100] TRAIN mean acc/loss: 0.5378639101982117/5.477011680603027\n",
      "[EPOCH 34/100] VAL mean acc/loss: 0.535366952419281/5.618118762969971\n",
      "[EPOCH 35/100] TRAIN mean acc/loss: 0.5296931266784668/5.722880840301514\n",
      "[EPOCH 35/100] VAL mean acc/loss: 0.5473489761352539/5.7447052001953125\n",
      "[EPOCH 36/100] TRAIN mean acc/loss: 0.5417875647544861/5.693055152893066\n",
      "[EPOCH 36/100] VAL mean acc/loss: 0.53916335105896/5.62190055847168\n",
      "[EPOCH 37/100] TRAIN mean acc/loss: 0.5363354682922363/5.582484245300293\n",
      "[EPOCH 37/100] VAL mean acc/loss: 0.5539902448654175/5.685396671295166\n",
      "[EPOCH 38/100] TRAIN mean acc/loss: 0.543150782585144/5.481014728546143\n",
      "[EPOCH 38/100] VAL mean acc/loss: 0.5452823638916016/5.602063179016113\n",
      "[EPOCH 39/100] TRAIN mean acc/loss: 0.5360137820243835/5.5529680252075195\n",
      "[EPOCH 39/100] VAL mean acc/loss: 0.5437193512916565/5.53512716293335\n",
      "[EPOCH 40/100] TRAIN mean acc/loss: 0.5440620183944702/5.448987007141113\n",
      "[EPOCH 40/100] VAL mean acc/loss: 0.5534319281578064/5.482461929321289\n",
      "[EPOCH 41/100] TRAIN mean acc/loss: 0.5490633845329285/5.399147987365723\n",
      "[EPOCH 41/100] VAL mean acc/loss: 0.5507308840751648/5.505025863647461\n",
      "[EPOCH 42/100] TRAIN mean acc/loss: 0.5489675998687744/5.3770751953125\n",
      "[EPOCH 42/100] VAL mean acc/loss: 0.5500169992446899/5.406009674072266\n",
      "[EPOCH 43/100] TRAIN mean acc/loss: 0.5481758713722229/5.3233137130737305\n",
      "[EPOCH 43/100] VAL mean acc/loss: 0.5523046851158142/5.347775459289551\n",
      "[EPOCH 44/100] TRAIN mean acc/loss: 0.5462402105331421/5.362892150878906\n",
      "[EPOCH 44/100] VAL mean acc/loss: 0.5575147867202759/5.470386028289795\n",
      "[EPOCH 45/100] TRAIN mean acc/loss: 0.5507999658584595/5.3397650718688965\n",
      "[EPOCH 45/100] VAL mean acc/loss: 0.5630892515182495/5.423148155212402\n",
      "[EPOCH 46/100] TRAIN mean acc/loss: 0.5507357716560364/5.332437992095947\n",
      "[EPOCH 46/100] VAL mean acc/loss: 0.5594886541366577/5.316000938415527\n",
      "[EPOCH 47/100] TRAIN mean acc/loss: 0.5535344481468201/5.2920823097229\n",
      "[EPOCH 47/100] VAL mean acc/loss: 0.5478943586349487/5.422957420349121\n",
      "[EPOCH 48/100] TRAIN mean acc/loss: 0.5488015413284302/5.412891387939453\n",
      "[EPOCH 48/100] VAL mean acc/loss: 0.5597478151321411/5.429311752319336\n",
      "[EPOCH 49/100] TRAIN mean acc/loss: 0.5627485513687134/5.408614158630371\n",
      "[EPOCH 49/100] VAL mean acc/loss: 0.5611281394958496/5.357080936431885\n",
      "[EPOCH 50/100] TRAIN mean acc/loss: 0.5547681450843811/5.300667762756348\n",
      "[EPOCH 50/100] VAL mean acc/loss: 0.5495738387107849/5.400008201599121\n",
      "[EPOCH 51/100] TRAIN mean acc/loss: 0.5591430068016052/5.242018699645996\n",
      "[EPOCH 51/100] VAL mean acc/loss: 0.5661797523498535/5.28222131729126\n",
      "[EPOCH 52/100] TRAIN mean acc/loss: 0.5632367730140686/5.222992420196533\n",
      "[EPOCH 52/100] VAL mean acc/loss: 0.5549346208572388/5.2964911460876465\n",
      "[EPOCH 53/100] TRAIN mean acc/loss: 0.5636096000671387/5.244422912597656\n",
      "[EPOCH 53/100] VAL mean acc/loss: 0.5510469079017639/5.384160041809082\n",
      "[EPOCH 54/100] TRAIN mean acc/loss: 0.561549186706543/5.2869439125061035\n",
      "[EPOCH 54/100] VAL mean acc/loss: 0.5509558916091919/5.244446754455566\n",
      "[EPOCH 55/100] TRAIN mean acc/loss: 0.5573664307594299/5.254627704620361\n",
      "[EPOCH 55/100] VAL mean acc/loss: 0.5576246380805969/5.3852643966674805\n",
      "[EPOCH 56/100] TRAIN mean acc/loss: 0.564776599407196/5.192642688751221\n",
      "[EPOCH 56/100] VAL mean acc/loss: 0.5497547388076782/5.324787139892578\n",
      "[EPOCH 57/100] TRAIN mean acc/loss: 0.5483769774436951/5.261033535003662\n",
      "[EPOCH 57/100] VAL mean acc/loss: 0.5592854022979736/5.328904628753662\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[EPOCH 58/100] TRAIN mean acc/loss: 0.5575830340385437/5.233142852783203\n",
      "[EPOCH 58/100] VAL mean acc/loss: 0.55208420753479/5.339027404785156\n",
      "[EPOCH 59/100] TRAIN mean acc/loss: 0.5594340562820435/5.186810493469238\n",
      "[EPOCH 59/100] VAL mean acc/loss: 0.5522112846374512/5.291464328765869\n",
      "[EPOCH 60/100] TRAIN mean acc/loss: 0.559779703617096/5.165469646453857\n",
      "[EPOCH 60/100] VAL mean acc/loss: 0.5524017214775085/5.264989376068115\n",
      "[EPOCH 61/100] TRAIN mean acc/loss: 0.5601096153259277/5.133998394012451\n",
      "[EPOCH 61/100] VAL mean acc/loss: 0.5700401663780212/5.156096458435059\n",
      "[EPOCH 62/100] TRAIN mean acc/loss: 0.5640848278999329/5.1423115730285645\n",
      "[EPOCH 62/100] VAL mean acc/loss: 0.5712646245956421/5.14431095123291\n",
      "[EPOCH 63/100] TRAIN mean acc/loss: 0.562294065952301/5.18170166015625\n",
      "[EPOCH 63/100] VAL mean acc/loss: 0.5419762134552002/5.352577209472656\n",
      "[EPOCH 64/100] TRAIN mean acc/loss: 0.5546364784240723/5.238419532775879\n",
      "[EPOCH 64/100] VAL mean acc/loss: 0.5638796091079712/5.31743049621582\n",
      "[EPOCH 65/100] TRAIN mean acc/loss: 0.5559906363487244/5.277931213378906\n",
      "[EPOCH 65/100] VAL mean acc/loss: 0.5560970306396484/5.5584917068481445\n",
      "[EPOCH 66/100] TRAIN mean acc/loss: 0.5648897886276245/5.201505184173584\n",
      "[EPOCH 66/100] VAL mean acc/loss: 0.5649070739746094/5.355788230895996\n",
      "[EPOCH 67/100] TRAIN mean acc/loss: 0.5661941170692444/5.118727684020996\n",
      "[EPOCH 67/100] VAL mean acc/loss: 0.5545119047164917/5.42996072769165\n",
      "[EPOCH 68/100] TRAIN mean acc/loss: 0.5648911595344543/5.191410541534424\n",
      "[EPOCH 68/100] VAL mean acc/loss: 0.5563331842422485/5.150106430053711\n",
      "[EPOCH 69/100] TRAIN mean acc/loss: 0.56819748878479/5.140864372253418\n",
      "[EPOCH 69/100] VAL mean acc/loss: 0.5666753649711609/5.209096908569336\n",
      "[EPOCH 70/100] TRAIN mean acc/loss: 0.5667180418968201/5.123720645904541\n",
      "[EPOCH 70/100] VAL mean acc/loss: 0.5744485855102539/5.116077423095703\n",
      "[EPOCH 71/100] TRAIN mean acc/loss: 0.5667017102241516/5.055410861968994\n",
      "[EPOCH 71/100] VAL mean acc/loss: 0.5590909719467163/5.2542266845703125\n",
      "[EPOCH 72/100] TRAIN mean acc/loss: 0.5667151212692261/5.039306640625\n",
      "[EPOCH 72/100] VAL mean acc/loss: 0.569683313369751/5.151397705078125\n",
      "[EPOCH 73/100] TRAIN mean acc/loss: 0.5663508176803589/5.1804728507995605\n",
      "[EPOCH 73/100] VAL mean acc/loss: 0.55353844165802/5.339611530303955\n",
      "[EPOCH 74/100] TRAIN mean acc/loss: 0.56639564037323/5.18229341506958\n",
      "[EPOCH 74/100] VAL mean acc/loss: 0.5755119323730469/5.278299331665039\n",
      "[EPOCH 75/100] TRAIN mean acc/loss: 0.5672321915626526/5.077616214752197\n",
      "[EPOCH 75/100] VAL mean acc/loss: 0.5821400284767151/5.124123573303223\n",
      "[EPOCH 76/100] TRAIN mean acc/loss: 0.5721769332885742/5.103365421295166\n",
      "[EPOCH 76/100] VAL mean acc/loss: 0.5710917711257935/5.208249092102051\n",
      "[EPOCH 77/100] TRAIN mean acc/loss: 0.577231764793396/5.087437152862549\n",
      "[EPOCH 77/100] VAL mean acc/loss: 0.5976077318191528/5.080465793609619\n",
      "[EPOCH 78/100] TRAIN mean acc/loss: 0.5816435217857361/5.069178581237793\n",
      "[EPOCH 78/100] VAL mean acc/loss: 0.5800922513008118/5.168817043304443\n",
      "[EPOCH 79/100] TRAIN mean acc/loss: 0.5756744146347046/5.035609245300293\n",
      "[EPOCH 79/100] VAL mean acc/loss: 0.5393583178520203/5.221748352050781\n",
      "[EPOCH 80/100] TRAIN mean acc/loss: 0.5774199366569519/5.026947975158691\n",
      "[EPOCH 80/100] VAL mean acc/loss: 0.5960593819618225/5.160573959350586\n",
      "[EPOCH 81/100] TRAIN mean acc/loss: 0.581155002117157/4.98409366607666\n",
      "[EPOCH 81/100] VAL mean acc/loss: 0.5629996061325073/5.139035224914551\n",
      "[EPOCH 82/100] TRAIN mean acc/loss: 0.5817441940307617/4.979653835296631\n",
      "[EPOCH 82/100] VAL mean acc/loss: 0.5780556797981262/4.951720237731934\n",
      "[EPOCH 83/100] TRAIN mean acc/loss: 0.582358717918396/4.983763217926025\n",
      "[EPOCH 83/100] VAL mean acc/loss: 0.5790632963180542/5.116146564483643\n",
      "[EPOCH 84/100] TRAIN mean acc/loss: 0.5778467059135437/4.975177764892578\n",
      "[EPOCH 84/100] VAL mean acc/loss: 0.5644419193267822/5.216444969177246\n",
      "[EPOCH 85/100] TRAIN mean acc/loss: 0.5800978541374207/4.961789131164551\n",
      "[EPOCH 85/100] VAL mean acc/loss: 0.5666281580924988/5.098862648010254\n",
      "[EPOCH 86/100] TRAIN mean acc/loss: 0.5658890008926392/5.270923137664795\n",
      "[EPOCH 86/100] VAL mean acc/loss: 0.5543203353881836/5.964966297149658\n",
      "[EPOCH 87/100] TRAIN mean acc/loss: 0.5486279129981995/5.527122974395752\n",
      "[EPOCH 87/100] VAL mean acc/loss: 0.5420643091201782/6.236785411834717\n",
      "[EPOCH 88/100] TRAIN mean acc/loss: 0.5497420430183411/5.343843936920166\n",
      "[EPOCH 88/100] VAL mean acc/loss: 0.5450055599212646/5.476044178009033\n",
      "[EPOCH 89/100] TRAIN mean acc/loss: 0.5621073246002197/5.279926300048828\n",
      "[EPOCH 89/100] VAL mean acc/loss: 0.5875635743141174/5.395264148712158\n",
      "[EPOCH 90/100] TRAIN mean acc/loss: 0.5684899091720581/5.255053520202637\n",
      "[EPOCH 90/100] VAL mean acc/loss: 0.5651295185089111/5.470736503601074\n",
      "[EPOCH 91/100] TRAIN mean acc/loss: 0.5715937614440918/5.176880836486816\n",
      "[EPOCH 91/100] VAL mean acc/loss: 0.5527744293212891/5.293844699859619\n",
      "[EPOCH 92/100] TRAIN mean acc/loss: 0.5775402188301086/5.156784534454346\n",
      "[EPOCH 92/100] VAL mean acc/loss: 0.5796054601669312/5.4502739906311035\n",
      "[EPOCH 93/100] TRAIN mean acc/loss: 0.5711398124694824/5.487707614898682\n",
      "[EPOCH 93/100] VAL mean acc/loss: 0.5263816118240356/5.787521839141846\n",
      "[EPOCH 94/100] TRAIN mean acc/loss: 0.5636764764785767/5.3525614738464355\n",
      "[EPOCH 94/100] VAL mean acc/loss: 0.5553033351898193/5.4354753494262695\n",
      "[EPOCH 95/100] TRAIN mean acc/loss: 0.5625510215759277/5.174381732940674\n",
      "[EPOCH 95/100] VAL mean acc/loss: 0.557308554649353/5.379533767700195\n",
      "[EPOCH 96/100] TRAIN mean acc/loss: 0.570952296257019/5.12046480178833\n",
      "[EPOCH 96/100] VAL mean acc/loss: 0.546272873878479/5.334687232971191\n",
      "[EPOCH 97/100] TRAIN mean acc/loss: 0.5749155879020691/5.048934459686279\n",
      "[EPOCH 97/100] VAL mean acc/loss: 0.5621687173843384/5.180370330810547\n",
      "[EPOCH 98/100] TRAIN mean acc/loss: 0.5817917585372925/4.990120887756348\n",
      "[EPOCH 98/100] VAL mean acc/loss: 0.5797388553619385/5.096711158752441\n",
      "[EPOCH 99/100] TRAIN mean acc/loss: 0.582464873790741/5.010778903961182\n",
      "[EPOCH 99/100] VAL mean acc/loss: 0.5814108848571777/5.173823356628418\n",
      "[EPOCH 100/100] TRAIN mean acc/loss: 0.582252562046051/4.964353084564209\n",
      "[EPOCH 100/100] VAL mean acc/loss: 0.5648623704910278/5.074463844299316\n",
      "\n",
      "FINISH.\n"
     ]
    }
   ],
   "source": [
    "# Start training\n",
    "\n",
    "num_epochs=100\n",
    "log_nth_iter=10\n",
    "log_nth_epoch=1\n",
    "tqdm_mode='total'\n",
    "'''\n",
    "tqdm_mode:\n",
    "    'total': tqdm log how long all epochs will take,\n",
    "    'epoch': tqdm for each epoch how long it will take,\n",
    "    anything else, e.g. None: do not use tqdm\n",
    "'''\n",
    "\n",
    "# TODO: Add parameters to extra_args dict?\n",
    "if train_with_discriminator:\n",
    "    steps = 1 # how many steps of training for discriminator/generator before switching to generator/discriminator\n",
    "    solver.train(model,\n",
    "                 train_loader, \n",
    "                 validation_loader,\n",
    "                 num_epochs=num_epochs,\n",
    "                 log_nth_iter=log_nth_iter,\n",
    "                 log_nth_epoch=log_nth_epoch,\n",
    "                 tqdm_mode=tqdm_mode,\n",
    "                 steps=steps)\n",
    "else:\n",
    "    solver.train(model,\n",
    "                 train_loader,\n",
    "                 validation_loader,\n",
    "                 num_epochs=num_epochs,\n",
    "                 log_nth_iter=log_nth_iter,\n",
    "                 log_nth_epoch=log_nth_epoch,\n",
    "                 tqdm_mode=tqdm_mode,\n",
    "                 verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tVWNikT4PvGj"
   },
   "outputs": [],
   "source": [
    "# To download tensorboard runs from Colab\n",
    "\n",
    "# TODO: Make sure that only new ones are copied --> for tensorboard runs on colab, do not use git repository as \"runs\" directory?\n",
    "# TODO: Instead of downloading, directly move it to the git repository that is currently checked out and push changes?\n",
    "if is_on_colab:\n",
    "  from google.colab import files\n",
    "  !zip -r /content/runs.zip /content/runs\n",
    "  files.download(\"/content/runs.zip\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "P2VDewrSvags"
   },
   "source": [
    "# Test\n",
    "\n",
    "Test with test dataset.\n",
    "Will load the data and start the training.\n",
    "\n",
    "Visualizations can be seen in Tensorboard above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "d8S9-1x0zbHZ"
   },
   "outputs": [],
   "source": [
    "# Load test data\n",
    "# TODO: Find real test split, for now we load the SAME dataset as for train/val (just that this notebook is complete...)\n",
    "# !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "if is_on_zerus:\n",
    "    test_path = \"/mnt/raid/teampc/ICL-NUIM/office_room_traj2_loop\"\n",
    "\n",
    "if is_on_colab:\n",
    "    test_path = \"/content/drive/My Drive/Novel_View_Synthesis/ICL-NUIM/living_room_traj2_loop\"\n",
    "\n",
    "test_dataset = ICLNUIMDataset(test_path, transform=transform) # TODO also use rest of parameters...\n",
    "\n",
    "test_indices = list(range(len(test_dataset)))\n",
    "np.random.shuffle(test_indices)\n",
    "\n",
    "test_sampler = SubsetRandomSampler(test_indices[:len(test_indices)//10])\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=dataset_args[\"batch_size\"], \n",
    "                                          sampler=test_sampler,\n",
    "                                          num_workers=4)\n",
    "\n",
    "print(\"Length of test set: {}\".format(len(test_loader)))\n",
    "print(\"Loaded test set: {}\".format(test_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cl2sFO4Kynp6"
   },
   "outputs": [],
   "source": [
    "# Start testing\n",
    "\n",
    "solver.test(model, test_loader, test_prefix=\"icl_test\", log_nth=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WlzKn0dWTNoU"
   },
   "source": [
    "## Generating a Test Time Trajectory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bKHYULewTNoV"
   },
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "from ipywidgets import interact, interactive, fixed, interact_manual\n",
    "import ipywidgets as widgets\n",
    "import math\n",
    "from util.nvs_solver import to_cuda, default_batch_loader\n",
    "%matplotlib inline\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# Load the model if needed by using the last cell\n",
    "\n",
    "model.eval().cuda()\n",
    "\n",
    "# Pick an image for the first frame and extract items related to it\n",
    "test_item_idx = 0\n",
    "test_item = test_dataset.__getitem__(test_item_idx)\n",
    "input_img, K, K_inv, input_RT, input_RT_inv, output_RT, output_RT_inv, gt_img, depth_img = to_cuda(default_batch_loader(test_item))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7DkK_yJpTNoX"
   },
   "source": [
    "### Rotation & Translation\n",
    "Use the sliders to jointly modify every axis and the rotation around them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AxmWFW1rTNoX"
   },
   "outputs": [],
   "source": [
    "# Keep modified RT2 matrices\n",
    "traj = []\n",
    "\n",
    "# A function to generate translational trajectory, modifies output_RT, output_RT_inv & gt_img\n",
    "def modify_frame(x=0,y=0,z=0, rx=0, ry=0, rz=0):\n",
    "    global traj, input_img, K, K_inv, input_RT, input_RT_inv, output_RT, output_RT_inv, gt_img, depth_img\n",
    "    R_X = torch.Tensor([\n",
    "      [ 1.0,  0.0, 0.0, x*255],\n",
    "      [ 0.0,  math.cos(rx*3.1415/180), -math.sin(rx*3.1415/180)/255, y*255],\n",
    "      [ 0.0,  math.sin(rx*3.1415/180)/255, math.cos(rx*3.1415/180), z],\n",
    "      [ 0.0,  0.0, 0.0, 1.0]]).cuda()\n",
    "    \n",
    "    R_Y = torch.Tensor([\n",
    "      [ math.cos(ry*3.1415/180),  0.0, math.sin(ry*3.1415/180)/255, 0.0],\n",
    "      [ 0.0,  1.0, 0.0, 0.0],\n",
    "      [ -math.sin(ry*3.1415/180)/255,  0.0, math.cos(ry*3.1415/180), 0.0],\n",
    "      [ 0.0,  0.0, 0.0, 1.0]]).cuda()\n",
    "    \n",
    "    R_Z = torch.Tensor([\n",
    "      [ math.cos(rz*3.1415/180),  -math.sin(rz*3.1415/180), 0.0, 0.0],\n",
    "      [ math.sin(rz*3.1415/180),  math.cos(rz*3.1415/180), 0.0, 0.0],\n",
    "      [ 0.0,  0.0, 1.0, 0.0],\n",
    "      [ 0.0,  0.0, 0.0, 1.0]]).cuda()\n",
    "    \n",
    "    # Translate input_RT by given x,y,z\n",
    "    output_RT_inv = (R_X@R_Y@R_Z).mm(input_RT_inv)\n",
    "\n",
    "    # Perform projection to obtain a pseudo GT for the manipulation\n",
    "    gt_img = model.pts_transformer.forward_justpts(\n",
    "        input_img.unsqueeze(0),\n",
    "        depth_img.unsqueeze(0),\n",
    "        K.unsqueeze(0),\n",
    "        K_inv.unsqueeze(0),\n",
    "        input_RT.unsqueeze(0),\n",
    "        input_RT_inv.unsqueeze(0),\n",
    "        output_RT.unsqueeze(0),\n",
    "        output_RT_inv.unsqueeze(0),\n",
    "    )\n",
    "    print(\"Projection with new RT:\")\n",
    "    gt_img_np = gt_img.squeeze(0).cpu().detach().numpy()\n",
    "    plt.imshow(np.moveaxis(gt_img_np, 0, -1))\n",
    "    plt.show()\n",
    "    \n",
    "    # Store matrices for the new view\n",
    "    traj.append((output_RT, output_RT_inv, gt_img))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wfRU_kyhTNoZ"
   },
   "outputs": [],
   "source": [
    "matrix = interact(modify_frame, \n",
    "                  x=(-1.0,1.0),\n",
    "                  y=(-1.0,1.0), \n",
    "                  z=(-1.0,1.0), \n",
    "                  rx=(-10.0,10.0, 1), \n",
    "                  ry=(-10.0,10.0, 1),\n",
    "                  rz=(-10.0,10.0, 1));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yyUp6f9bTNoc",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pprint(test_item[\"cam\"][\"RT1\"])\n",
    "# traj.pop(0) # Interactive slider sometimes first item has the same RT matrix as the input view (RT1), discard it\n",
    "pprint(traj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "M2Vsd4A3TNoe"
   },
   "outputs": [],
   "source": [
    "# It is important to pass this image to loader to apply same transforms that was applied during training.\n",
    "# We have to make sure that test time images get the same transforms as train time to have meaningful results.\n",
    "test_sampler = SubsetRandomSampler([test_item_idx])\n",
    "test_loader = torch.utils.data.DataLoader(dataset, batch_size=1,\n",
    "                                          sampler=test_sampler, num_workers=0)\n",
    "\n",
    "with torch.no_grad():\n",
    "    # Get the frame (triggers get_item and transforms)\n",
    "    test_item = next(iter(test_loader))\n",
    "    test_item = to_cuda(default_batch_loader(test_item)) # List of contents in dict\n",
    "    # For each different RT matrix perform a forward pass using GT depth\n",
    "    for output_RT, output_RT_inv, gt_img in traj:\n",
    "        out = model(test_item[0], # input_img\n",
    "                    test_item[1], # K\n",
    "                    test_item[2], # K_inv\n",
    "                    test_item[3], # input_RT\n",
    "                    test_item[4], # input_RT_inv\n",
    "                    output_RT.unsqueeze(0), \n",
    "                    output_RT_inv.unsqueeze(0), \n",
    "                    gt_img,                     # Not used\n",
    "                    test_item[-1]\n",
    "                   )              # GT depth\n",
    "        # Visualize prediction by network\n",
    "        pred = out[\"PredImg\"]\n",
    "        pred_np = pred.squeeze().cpu().detach().numpy()\n",
    "        plt.imshow(np.moveaxis(pred_np, 0, -1))\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pTLUJGpnBpud"
   },
   "source": [
    "# Save the model\n",
    "\n",
    "Save network with its weights to disk.\n",
    "\n",
    "See torch.save function: https://pytorch.org/docs/stable/notes/serialization.html#recommend-saving-models \n",
    "\n",
    "Load again with `the_model = TheModelClass(*args, **kwargs) the_model.load_state_dict(torch.load(PATH))`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7_zjMVB7Bpue"
   },
   "outputs": [],
   "source": [
    "def save_model(modelname, model):\n",
    "    from pathlib import Path\n",
    "    Path(\"../saved_models\").mkdir(parents=True, exist_ok=True)\n",
    "    # Might need to make sure, that the correct saved_results directory is chosen here.\n",
    "    filepath = \"../saved_models/\" + modelname + \".pt\"\n",
    "    torch.save(model.state_dict(), filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2JfoL3IHBpuv"
   },
   "outputs": [],
   "source": [
    "nvs_modelname = \"nvs_\" + id_suffix\n",
    "save_model(nvs_modelname, model)\n",
    "\n",
    "if train_with_discriminator:\n",
    "    # Also save the discriminator - currently this can only be accessed through the solver (change it!)\n",
    "    gan_modelname = \"gan_\" + id_suffix\n",
    "    save_model(gan_modelname, solver.netD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BwbC3OFOEmLX"
   },
   "outputs": [],
   "source": [
    "# LOAD MODEL AGAIN for verification purposes\n",
    "# Should print: <All keys matched successfully> per each model if it works\n",
    "\n",
    "new_model=False\n",
    "# add a different model name to be loaded here\n",
    "if new_model:\n",
    "    nvs_modelname=\"nvs_2020-May-29_18-44-55_b9c02778-a1cb-11ea-82a9-5542432396e9\"\n",
    "    gan_modelname=\"gan_2020-May-29_18-44-55_b9c02778-a1cb-11ea-82a9-5542432396e9\"\n",
    "    \n",
    "nvs_filepath = \"../saved_models/\" + nvs_modelname + \".pt\"\n",
    "print(\"NVS_Model loading: \", model.load_state_dict(torch.load(nvs_filepath)))\n",
    "\n",
    "if train_with_discriminator:\n",
    "    gan_filepath = \"../saved_models/\" + gan_modelname + \".pt\"\n",
    "    print(\"Discriminator loading: \", solver.netD.load_state_dict(torch.load(gan_filepath)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Jy91cvNeJGmc"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "entire_network_notebook.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
